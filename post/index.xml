<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Maxime Garcia</title>
    <link>http://typhomnt.github.io/post/</link>
      <atom:link href="http://typhomnt.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 26 Sep 2021 16:22:09 +0100</lastBuildDate>
    <image>
      <url>http://typhomnt.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_box_center_2.png</url>
      <title>Posts</title>
      <link>http://typhomnt.github.io/post/</link>
    </image>
    
    <item>
      <title>Making a NPR Shader in Blender</title>
      <link>http://typhomnt.github.io/post/blender_npr/</link>
      <pubDate>Sun, 26 Sep 2021 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/blender_npr/</guid>
      <description>&lt;p&gt;This article is the result of my first attempt at making an NPR shader inside Blender.&lt;/p&gt;
&lt;p&gt;I first tried to play around the shading node system as well as the post processing effects like Ambient Occlusion or Bloom and test various things. As I would like to focus on &amp;ldquo;real-time&amp;rdquo; rendering, I tested everything with the EEVEE rendering engine. I also looked at many videos and works that have been done inside Blender for NPR.&lt;/p&gt;
&lt;p&gt;If you are interested in the subject, you can take a look at (or even join) the 
&lt;a href=&#34;https://www.facebook.com/groups/BNPRandFreestyle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blender NPR community&lt;/a&gt; and also check out the Lightning Boy Studio 
&lt;a href=&#34;https://www.youtube.com/channel/UCd9i2MKimSaKezat1xkn8-A&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;youtube channel&lt;/a&gt;, there are many resources for that field.&lt;/p&gt;
&lt;p&gt;In this article, I am going to describe a simple version of a Blender shader NPR that I built. Of course, I took inspiration from what is available on the Internet and also tried to propose my own vision. It is very far from perfect but I hope it will give you a basic understanding in order to start exploring NPR on your own.&lt;/p&gt;
&lt;h2 id=&#34;shader-structure-&#34;&gt;Shader Structure&lt;/h2&gt;
&lt;p&gt;The shader is represented as a node group which itself is divided into several groups that correspond to light contributions.
More precisely, this NPR shader light contributions are divided into 5 components: Diffuse, Specular, Ambient occlusion, Sub-Surface Scattering (SSS) and Emission.
Furthermore, I also include Rim light and Outline contributions which are heavily present in NPR shaders.&lt;/p&gt;
&lt;p&gt;First let me tease the kind of results you can obtain with this shader.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Clare_2.png&#34; alt=&#34;NPR Example&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Clare_1.png&#34; alt=&#34;NPR Example&#34;&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: center;&#34;&gt; model: Clare by ruslik85 on Sketchfab &lt;/p&gt;
&lt;p&gt;As you can see the range of rendering styles can vary from very cartoonish shading to more realistic results.
Below, you can notice the difference compared to a more realistic look using the principled Shader (model: Ruby Rose by theStoff on Sketchfab) :&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Realistic&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Cartoon&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;video autoplay loop muted controls&gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/Realistic_Animated.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;video autoplay loop muted controls&gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/NPR_Animated.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The main tool to make such variations is to use color ramps to tweak the output of base shaders, like the Diffuse or Glossy (Specular) shaders.
There is more to it of course, and we will go into details in the next sections.&lt;/p&gt;
&lt;p&gt;As you can see, the NPR look is more contrasted and &amp;ldquo;flashier&amp;rdquo; than the realistic look. It also while masks details at the same time.
However, thanks to effects like Outline and Rim Light, we are also able to make relevant parts of the model (like the silhouettes) stand out.
This is especially useful to improve the perception of characters&amp;rsquo; poses and their motion.&lt;/p&gt;
&lt;p&gt;This simple Blender NPR shader is build as a node group that can be tweaked with numerous parameters:&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/NPR_Mat.PNG&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/NPR_Mat.PNG&#34; alt=&#34;NPR Material&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And here is an overview of what the inside node group looks like:&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/NPR_Group.PNG&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/NPR_Group.PNG&#34; alt=&#34;NPR Group&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This node group is itself divided into 7 sub-groups, each corresponding to different light contributions. We then blend them together by choosing an appropriate blending operation. Each of these contributions will be detailed in its own section as well as their blending.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s first dig into each component independently for a scene containing a plane, a sphere, Suzanne and a sun light.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; Before going further, it is important to switch the color management to Standard in the Render Properties panel as we want to control our color output precisely &lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Color_Managment.PNG&#34; alt=&#34;Color Managment&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;diffuse-component-&#34;&gt;Diffuse Component&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_Group.PNG&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_Group.PNG&#34; alt=&#34;Diffuse Group&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The diffuse component represents the amount of light re-emitted by the surface after receiving incoming light, here coming from lamps. This amount can be obtained using the Diffuse BRDF node which computes a physically based diffusion.&lt;/p&gt;
&lt;p&gt;As our objective is to produce stylized results, we would like to tweak this output and have some artistic control over it.
Here comes into play the basic node that everyone uses in the NPR community, namely the ShaderToRGB Node. Indeed, this magic node converts the shader output into a color (works only for EEVEE at the time I write this article) and lets us tweak this output color.&lt;/p&gt;
&lt;p&gt;One convenient way of tweaking this color is to use a Color Ramp which allows us to remap an input color into an interpolated color computed from parameterized colors spread into a $ [0:1] $ range. The control offered by color ramps is pretty intuitive and thus, it makes a good candidate for us to use in our stylization process.
To tweak the diffuse component we will proceed the following way: first set the albedo of the surface as pure white in the diffuse shader and plug the output color (after ShaderToRGB) as the input of a ColorRamp.
Then we can modify the color ramp content to stylize the diffuse contribution.
And finally, we can get back the color of the surface we actually want (diffuse albedo) by multiplying the output of the color ramp with the surface color.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; Note that I use a Separate HSV node right after ShaderToRGB to use the &lt;strong&gt;value&lt;/strong&gt; of the input color as it is the only attribute that interests us.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_1.png&#34; alt=&#34;Diffuse Example&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_1_Mat.png&#34; alt=&#34;Diffuse Example&#34;&gt;&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s a good start, but several questions need to be answered: how do we modify the color ramp to get a stylized result ? What about metallic materials ? How to get back the influence of the incoming light color which is discarded with our current setup ?&lt;/p&gt;
&lt;h3 id=&#34;playing-with-color-ramps-&#34;&gt;Playing with color ramps&lt;/h3&gt;
&lt;p&gt;We were able to get the light contribution of the Diffuse component inside a color ramp, it is now time to play with it in order to stylize this contribution.
To do it properly, we first need to understand what is happening inside the Diffuse node and think about how we can get closer to a stylized result like we can see in cartoons/anime.&lt;/p&gt;
&lt;p&gt;To put it simply, the Diffusion or the light re-emitted by the surface of an object hit by light, depends on the surface orientation, referred as the normal $N$, with respect to the incoming direction of the light $L$. If the surface faces the incoming light direction it will re-emit more light. This can be simply modeled using a dot product between L and N of the surface.&lt;/p&gt;
&lt;p&gt;$$ D_{iffuse} \approx max(-L \cdot N,0) $$
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_Fig.png&#34; alt=&#34;Diffuse Fig&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is the basic computation that occurs inside the Diffuse node (it is a little bit more complicated in practice but let&amp;rsquo;s keep it simple in this tutorial).
For smooth surfaces like humanoid characters, this Diffuse contribution also varies in a smooth fashion alongside the model, which is not the case when you look at cartoons/anime.
Usually you get one, two or three grading levels with abrupt changes.&lt;/p&gt;
&lt;p&gt;To get these results with the color ramp, we simply have to add levels to the ramp (press the &amp;lsquo;+&amp;rsquo; button inside the ramp) and change the interpolation mode to constant, restraining the number of light levels diffused by the model to a fixed number.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_2.png&#34; alt=&#34;Diffuse Example&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_2_Mat.PNG&#34; alt=&#34;Diffuse Example&#34;&gt;&lt;/p&gt;
&lt;p&gt;Et voila ! you get a cartoonish/anime shading. You are free to tweak the levels at your own convenience as well as to use a smoother interpolation and even mix the discrete and smooth Diffusion, like I suggest in the final version of the Diffuse shader.&lt;/p&gt;
&lt;h3 id=&#34;metallic-diffusion--&#34;&gt;Metallic Diffusion ?&lt;/h3&gt;
&lt;p&gt;As you probably guess there is something to take into account with metallic materials regarding Diffusion.
Indeed, something to know is that metallic materials do not diffuse light, only absorbing and reflecting it.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s why we took it into consideration by multiplying the result of the diffusion with $ 1 - metalness $.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_3.png&#34; alt=&#34;Diffuse Example&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_3_Mat.PNG&#34; alt=&#34;Diffuse Example&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;light-color-influence-&#34;&gt;Light Color Influence&lt;/h3&gt;
&lt;p&gt;Using our current node configuration, you can notice that changing the light color has no effect on the shading of our objects.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_Light_Influence_Objs.PNG&#34; alt=&#34;Light Influence Off&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_Light_Influence.PNG&#34; alt=&#34;Light Influence On&#34;&gt;&lt;/p&gt;
&lt;p&gt;And this is due to the fact that we use the output of the gray scaled color ramp as the Diffuse light contribution. And so, light color influence is something we lost from the Diffuse BSDF that takes this factor into account.
The question is how to get it back ?
The answer is quite simple. As we already recover the light intensity of our shading (&lt;strong&gt;value&lt;/strong&gt; of the HSV node) we just have to get back the &lt;strong&gt;hue&lt;/strong&gt; and &lt;strong&gt;saturation&lt;/strong&gt; resulting from  a multiplication between the object color and the light color.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_Light_Influence_Objs_On.PNG&#34; alt=&#34;Light Influence On&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_Light_Influence_Objs_On_Mat.PNG&#34; alt=&#34;Light Influence On Mat&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;smoothness-and-responsiveness-&#34;&gt;Smoothness and Responsiveness&lt;/h3&gt;
&lt;p&gt;It is also possible to give more artistic control to the Diffuse component, and this is something we will also use for the Specular contribution.&lt;/p&gt;
&lt;p&gt;First, it is very handy to add a smoothness control to your Diffuse shading.
And this can be done easily by adding a second color ramp with a linear interpolation this time, and blend it with the previous color ramp we built using a Mix Color node and a smoothness parameter.&lt;/p&gt;
&lt;p&gt;Secondly, it can also be useful to add a control that tweaks the amount of light re-emitted using a Map range node that maps $ [a : b] $ to $ [c : d] $. Here, we use an attenuation parameter that we plug as $b$ so that $ [0:b] $ gets mapped into $ [0:1] $, thus the higher $b$ is the less light will be re-emitted.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_4.png&#34; alt=&#34;Diffuse Example&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_4_Mat.PNG&#34; alt=&#34;Diffuse Example&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;specular-component-&#34;&gt;Specular Component&lt;/h2&gt;
&lt;p&gt;The Specular component represents the amount of incoming light that is reflected by the object surface.
One simple way of formulating its contribution is : from the camera point of view, how well does it see the light source reflected on the object ?&lt;/p&gt;
&lt;p&gt;The simplest model describing this contribution has been introduced by Phong, and simply look at the dot product between the reflected light vector $ R = reflect(L,N) $ and the view vector $ V $.&lt;/p&gt;
&lt;p&gt;$$ S_{pecular} \approx R \cdot V $$
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Specular_Fig.png&#34; alt=&#34;Specular Fig&#34;&gt;&lt;/p&gt;
&lt;p&gt;Inside Blender, computations are more complicated as it relies on physically-based algorithms and I invite you to take a look at my 
&lt;a href=&#34;../../teaching/ray_tracing/pbr_intro&#34;&gt;Physically-Based Rendering tutorial&lt;/a&gt; if you want to know more about it.&lt;/p&gt;
&lt;p&gt;The way we will implement this component is highly similar to what we did for the Diffuse component and thus I will not detail everything in this section.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Specular_Group.PNG&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Specular_Group.PNG&#34; alt=&#34;Specular Group&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The first thing to note is that I used the Principled BRDF node to get the specular component as the other nodes don&amp;rsquo;t provide support for metallic materials. As we are only interested in the Specular component here, I put a perfectly black color for the base color, so that we remove the Diffuse contribution provided by the Principled BRDF node. I also put the specular parameter at $1$ and plugged in the roughness parameter that plays a big role for that contribution. It is also important to notice that the metallness used is actually 1 minus the metalness and this is for stylization purposes.&lt;/p&gt;
&lt;p&gt;The rest of the Specular node group is exactly the same as the Diffuse component. We sample the intensity (or &lt;strong&gt;value&lt;/strong&gt;) of the Specular contribution using color ramps, possibly attenuating it beforehand, and mix the result with the specular color and the light color influence.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Specular.png&#34; alt=&#34;Specular Example&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; A small note for metallic materials. To enhance the look of metallic materials, you might use an environment map for the World material to get more reflections and thus a more believable look.
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/World_Mat_Env.PNG&#34; alt=&#34;World Mat&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;No Env Map&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Env Map on&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;video autoplay loop muted controls&gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/Magnemite_1.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;video autoplay loop muted controls&gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/Magnemite_2.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;ambient-occlusion-&#34;&gt;Ambient Occlusion&lt;/h2&gt;
&lt;p&gt;Ambient Occlusion (AO) is actually not a light contribution but more like a shadow contribution. It describes the area of the scene where light rays are less likely to hit.
This is especially the case of corners and creases and it is quite noticeable if you look at your room for instance.&lt;/p&gt;
&lt;p&gt;Ambient occlusion greatly enhances the realism of real-time renders and is something that artists are aware of when drawing and painting.
In our case, we would like to use it to give more contrast and color variation to our renders.&lt;/p&gt;
&lt;p&gt;The first thing to do when using EEVEE, is to enable Ambient Occlusion in the Render Properties tab.
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Ambient_Occlusion_Panel.PNG&#34; alt=&#34;Ambient Occlusion Panel&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are a couple of parameters like the Distance, Factor and Trace Precision as well as Bents Normals and Bounces Approximation. I won&amp;rsquo;t go into details on how Ambient Occlusion is computed but for our purpose let&amp;rsquo;s keep Factor and Trace Precision at 1 and enable Bents Normals and Bounces Approximation. The distance in this tab is not relevant and will be tweakable from the Ambient Occlusion node directly.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Ambient_Occlusion_Group.PNG&#34; alt=&#34;Ambient Group&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/ambient_occlusion.png&#34; alt=&#34;Ambient Example&#34;&gt;&lt;/p&gt;
&lt;p&gt;Like for Diffusion and Specular Reflexion, we use a color ramp to limit the number of grayscale levels AO can contribute to.
Notice that I limited AO to two levels for the cartoonish/anime shading as it is often more subtle than the previous components.&lt;/p&gt;
&lt;p&gt;You will also have to adjust the Distance parameter depending on the scale of your model, especially for small and large models. In the first case your model might be fully occluded while in the second case you might not notice any occlusion if you don&amp;rsquo;t tune this parameter carefully.&lt;br&gt;
I also added a Math power node to tweak the contrast of AO and give it more importance.&lt;/p&gt;
&lt;p&gt;Finally, to further increase the color contrast, we assign a color to the non occluded areas with a Multiply node.&lt;/p&gt;
&lt;h2 id=&#34;emission-component-&#34;&gt;Emission Component&lt;/h2&gt;
&lt;p&gt;The emissive contribution is the easiest to take into account as Blender already does all the work for us with the Emissive shader.&lt;/p&gt;
&lt;p&gt;However, it is important to note that when using EEVEE and real-time renderers, you have to enable Bloom to get a greater impact.
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Bloom_Panel.PNG&#34; alt=&#34;Bloom Panel&#34;&gt;&lt;/p&gt;
&lt;p&gt;It is a post-processing effect that highlights the brightest parts of your scene which is the case of emissive materials.
This is something widely used in video games to make fire, lasers, lamps and other special effects to stand out.
As you can see, there are also few parameters to tune the Bloom effect. You might want to tweak the Threshold parameter that determines when a given color intensity (&lt;strong&gt;value&lt;/strong&gt;) is considered as bright.&lt;/p&gt;
&lt;p&gt;Notice the difference when Bloom is enabled.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;No Bloom&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Bloom On&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Emission.png&#34; alt=&#34;Emission Example&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Emission_Bloom.png&#34; alt=&#34;Emission Example&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Emission_Group.PNG&#34; alt=&#34;Emission Group&#34;&gt;&lt;/p&gt;
&lt;p&gt;Currently it is quite costly to handle emissive materials as light sources in a real-time fashion.
Handling emissive materials like path tracers (Cycles in Blender) might change in the future with real time ray tracing being faster and more optimized.&lt;/p&gt;
&lt;h2 id=&#34;rim-light-and-outline-&#34;&gt;Rim Light and Outline&lt;/h2&gt;
&lt;p&gt;Rim lighting and Outline are tools used by artists to better depict the contours and the pose of characters.
They can also be used to set a specific mood and to enhance the perception of motion.&lt;/p&gt;
&lt;h3 id=&#34;extracting-the-contours-&#34;&gt;Extracting the contours&lt;/h3&gt;
&lt;p&gt;There are plenty of techniques to get the contours of a 3D model and draw art lines on top of it. There are pros and cons for each technique, but state-of-the-art methods tend to give a lot of freedom with computational costs that get shorter and shorter.

&lt;a href=&#34;https://hal.inria.fr/hal-02189483/document&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Benard et al.&lt;/a&gt; made a survey summarizing many existing methods to draw art lines on top of 3D models.&lt;/p&gt;
&lt;p&gt;In this tutorial, I described the simplest approach which is far from being perfect and you might want to use other methods like the 
&lt;a href=&#34;https://www.blendersecrets.org/secrets/inverted-hull-toon-outline-bnpr-blender-tutorial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;inverted hull&lt;/a&gt;, at least for the outline.&lt;/p&gt;
&lt;p&gt;The simplest way of getting the contours is to use the dot product between the view direction $ V $ and the normal $ N $.
Indeed, one way of defining parts of a 3D model that are near the silhouette is to look at where this dot product is near zero, or formulated differently, where the normal of the surface is orthogonal to the view direction.&lt;/p&gt;
&lt;h3 id=&#34;outline-&#34;&gt;Outline&lt;/h3&gt;
&lt;p&gt;There is an easy way to get this dot product in Blender within one node, namely the Fresnel node.
It takes into account an index of refraction which is a fundamental parameter to determine the amount of light that goes through the object, which also tells us the amount of light that gets reflected as its complementary.
This amount of light being reflected is directly linked to the dot product between the view direction $ V $ and  the normal $ N $.
And so, to get the Outline, we can use the output of the Fresnel node and threshold it with a color ramp like we did for the other components.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Outline_Group.PNG&#34; alt=&#34;Outline Group&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/outline.png&#34; alt=&#34;Outline Example&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;rim-light-&#34;&gt;Rim light&lt;/h3&gt;
&lt;p&gt;Rim Light is also referred as a backlight that highlights the full silhouette or only one part of it.&lt;/p&gt;
&lt;p&gt;To achieve this effect with 3D geometry, we can also compute the dot product between the normal $ N $ and the camera view $ V $ to get the silhouette and use color ramps to tweak its values.
This time I intentionally computed the dot product using Vector Math nodes.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Rim_Light_Group.PNG&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Rim_Light_Group.PNG&#34; alt=&#34;Rim Light Group&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We also need to limit the highlighted area to one side of the model (Side Masking). More precisely, we have to think in view space to correctly achieve this effect.
Therefore, we convert the normal into view space using the Transform node.
Basically what we want is to limit the silhouette lighting where the view space normal is aligned with a desired rim light direction.
Computing an alignment with a vector is equivalent to compute the dot product between the normal and the Rim light direction.&lt;/p&gt;
&lt;p&gt;And that&amp;rsquo;s it, using this trick we can highlight one side of the silhouette and mask the other side.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; Note that this Rim light direction is encoded as a 3D vector as Blender only supports them, but in practice we do not use the Z coordinate, hence the Vector Multiplication node.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a final touch, like precedently, we can use color ramps to sharpen or soften the Rim Light depending on the style you want to convey.
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/rim_light.png&#34; alt=&#34;Rim Light Example&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;sub-surface-scaterring-&#34;&gt;Sub-Surface Scaterring&lt;/h2&gt;
&lt;p&gt;Sub-Surface Scattering (SSS) is one of the most difficult contributions to handle as it is also a more complex phenomena.
It corresponds to light rays that completely traverse an opaque medium, like your body, and that reaches your eye. One common example given to behold this phenomena is to place your hand between you and the sun and look at the space between your fingers, you will notice a reddish color at the silhouette of your hand. This is the light going through a thin opaque medium that is your skin.&lt;/p&gt;
&lt;p&gt;Although it is a quite common phenomena, it is also very difficult to achieve in a real-time fashion and for this tutorial we will use the approximation proposed by 
&lt;a href=&#34;https://www.alanzucconi.com/2017/08/30/fast-subsurface-scattering-1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alan Zucconi&lt;/a&gt; (based on a GDC talk) to achieve our NPR SSS inside Blender. Note that we will not use the SSS shader provided by Blender as the method proposed by Alan Zucconi offers more expressiveness and  better artistic control.
We will also extend this approach by adding two additional contributions.&lt;/p&gt;
&lt;h3 id=&#34;approximated-sss-&#34;&gt;Approximated SSS&lt;/h3&gt;
&lt;p&gt;In this section, I will  mostly paraphrase the tutorial of 
&lt;a href=&#34;https://www.alanzucconi.com/2017/08/30/fast-subsurface-scattering-1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alan Zucconi&lt;/a&gt;, I will skip few details and explain how to implement it inside Blender.&lt;/p&gt;
&lt;p&gt;Like I mentioned above, SSS is a complex phenomenon where light rays go through the surface and are also deviated from their incoming direction.
If we look at the simple situation of a directional light casting rays to our object, we can notice that the non lighted region should be dark for an opaque material.
However, for translucent materials this is not the case and this area should be lighted somehow.&lt;br&gt;
The trick to take into account this phenomena is to create a fictive light contribution from the opposite direction of the incoming light.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s suppose that the incoming light from L is not deviated inside the material, that means that a viewer will best see light rays that go through the object if his gaze is aligned with L and on the contrary if it is orthogonal, he/she won&amp;rsquo;t see light. This means that the intensity of visible traversing light is proportional to the dot product of $ V  $ and $ L $.&lt;/p&gt;
&lt;p&gt;$$ I_{SSS} = clamp(V \cdot L ,0,1) $$&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;SSS Off&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;SSS On&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_No.png&#34; alt=&#34;SSS Fig&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_On.png&#34; alt=&#34;SSS On Fig&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;However, inside the object, the light undergoes some deviation and most noticeably at the object surface. So let&amp;rsquo;s take into account the deviation of the light rays that occurs when they leave the inside of an object.
This can be done using the normal $ N $ and the subsurface distortion $\delta$ that tells how much the normal will distort the light towards an halfway direction $ (N - L) $.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_Deviation.png&#34; alt=&#34;SSS Deviation Fig&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s also give more artistic control to this contribution, tweaking the contrast $ p $ and the scale $ S $with two parameters:&lt;/p&gt;
&lt;p&gt;$$ I_{SSS} = clamp(V \cdot - (\delta N - L),0,1)^pS $$&lt;/p&gt;
&lt;p&gt;Finally, we can also take into account the thickness of the object light rays are traversing. Indeed, the thicker the object is the more light will be absorbed and restrained inside the object.
Hence, the final formula of this approximated SSS taking into account the thickness:&lt;/p&gt;
&lt;p&gt;$$ I_{SSS} = clamp(V \cdot - (\delta N - L),0,1)^pS * (1 - thickness) $$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; Getting the true thickness, meaning the distance that a light ray traversed inside the object, can be costly as its computation is based on ray tracing. Instead, we can rely on a rough approximation using Ambient Occlusion like stated by Alan Zucconi. The trick is to compute AO on the inverted faces of the model, so that we compute the occlusion of the inside of the model and get information of the local thickness which can be sufficient for our case. Thankfully, the Ambient Occlusion node Blender provides this information by checking the &lt;strong&gt;Inside&lt;/strong&gt; option.
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_Thickness.png&#34; alt=&#34;SSS Group&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that I explained the theory behind this version of SSS, it is time to implement it inside Blender. One difficulty we encounter here with the node system, is the fact that we do not have access to the light direction $ L $.
Thankfully, there is a way to overcome this issue using Blender Drivers which are basically linking scene properties to variables inside the node editor.
However, it is important to note that SSS will only work for a unique light source and will not be taken into account for others, if you have any on your scene.&lt;/p&gt;
&lt;p&gt;To get $ L $ from our main directional light we will add a CombineXYZ node and right click on the x coordinate and choose &amp;lsquo;Add Driver&amp;rsquo;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Driver.png&#34; alt=&#34;Driver 1&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then, a panel will appear that will allow you to choose the scene property that will be linked to this coordinate. Select your light source in the object field and choose its X rotation as we deal with a directional light (could be the position for a point light).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Driver_Light.png&#34; alt=&#34;Driver 2&#34;&gt;&lt;/p&gt;
&lt;p&gt;Repeat this process for the y and z coordinates and you get the rotation of the light inside this node. Now let&amp;rsquo;s convert this rotation into a direction vector.
The Vector Rotation node Blender provides does exactly what we need, it applies a rotation to an input vector from the Euler angles (that we retrieve with the Drivers).
Notice that the z coordinate of the input vector is $ -1 $ so that we get $ -L $ directly from the rotation node.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Driver_Light_Dir.png&#34; alt=&#34;Driver 3&#34;&gt;&lt;/p&gt;
&lt;p&gt;We finally compute the dot product with the view Vector $ V $, apply the scale, thickness and power factor and get our approximated SSS contribution.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_Approx.png&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_Approx.png&#34; alt=&#34;SSS Group&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;video autoplay loop muted &gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/SSS_Approx.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/p&gt;
&lt;h3 id=&#34;inverse-diffuse-lighting-&#34;&gt;Inverse Diffuse Lighting&lt;/h3&gt;
&lt;p&gt;It is possible to go further in the stylization of SSS by adding an additional effect which is much more &amp;ldquo;artistic&amp;rdquo; than the approximation I just described.
The main idea is to add the complementary of the diffuse component as a contribution in order to make non lighted areas pop out a little bit.
The contribution is divided into two parts : front inverse diffuse and back inverse diffuse. The first adds the complementary of the incoming light while the later adds the complementary of the light coming from the opposite direction.&lt;/p&gt;
&lt;p&gt;Mathematically speaking :&lt;/p&gt;
&lt;p&gt;$$ F_{SSS} = 1 - max(L \cdot N,0)  $$
$$ B_{SSS} = 1 - max(-L \cdot N,0)  $$&lt;/p&gt;
&lt;p&gt;In the same manner as for the approximated SSS, we can add more control to these contributions :&lt;/p&gt;
&lt;p&gt;$$ F_{SSS} = (1 - max(L \cdot N,0))^p*\alpha_{front} $$
$$ B_{SSS} = (1 - max(-L \cdot N,0))^p*\alpha_{back}  $$&lt;/p&gt;
&lt;p&gt;And finally, the thickness of the object should also be taken into consideration:&lt;/p&gt;
&lt;p&gt;$$ I_{nverse Diffuse} = ((F_{SSS} + B_{SSS})*S)^{p_i}* (1 - thickness) $$&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_Front_Back.png&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_Front_Back.png&#34; alt=&#34;SSS Group&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;video autoplay loop muted &gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/SSS_Inverse_Diffuse.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/p&gt;
&lt;h3 id=&#34;final-sss-node-group-&#34;&gt;Final SSS Node Group&lt;/h3&gt;
&lt;p&gt;The final SSS contribution results in the sum of the approximated SSS and the inverse diffuse contribution.&lt;/p&gt;
&lt;p&gt;$$ SSS = (I_{SSS} + I_{nverse Diffuse})*(1 - metalness) $$&lt;/p&gt;
&lt;p&gt;As metallic materials cannot be translucent, we take into account the metalness of the material with a Multiply node and nullify the contribution of the SSS if the material is a metal.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_All.png&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_All.png&#34; alt=&#34;SSS Group&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Notice that eventually tweak the final result with color ramps to discretize or smooth the contribution.&lt;/p&gt;
&lt;p&gt;&lt;video autoplay loop muted &gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/SSS_Example.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/p&gt;
&lt;h2 id=&#34;blending-components-&#34;&gt;Blending Components&lt;/h2&gt;
&lt;p&gt;Now that we computed all the 7 different contributions, it is time to blend them together.
Diffuse, Specular and SSS components should be added together.
However, we first apply the Ambient Occlusion to the Diffuse component in Overlay mode in order to increase the contrast between the non occluded and occluded regions.
We then proceed to the SSS coloring using a simple multiplication with the SSS color. Note that, at this stage, we could decide to take into account the influence of the material color over the SSS output color.
We then successively add the SSS and the Specular component with an Add node.&lt;/p&gt;
&lt;p&gt;Finally we add the Rim lighting using a Mix node so that it goes on top of all the other contributions and re-iterate this operation for the Outline that also goes on top of the Rim light.&lt;/p&gt;
&lt;p&gt;Et voil√† ! We implemented our first NPR shader inside Blender !&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/NPR_Group.PNG&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/NPR_Group.PNG&#34; alt=&#34;Blending&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; This way of blending contributions together is one of the infinite possibilities you can test. There is no true way to blend these contributions together as we deal with NPR, so tweak these blend modes to your heart content !&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;limitations-&#34;&gt;Limitations&lt;/h2&gt;
&lt;p&gt;The main limitation of the shader is that it only supports one light for the main lighting and the SSS as we use drivers. If you want to add more light I recommend that you use point, spot and area lights even though the SSS won&amp;rsquo;t be computed for these additional lights.
Additionally, Inverse Diffuse Lighting can produce artifacts when it is not properly aligned with the Diffuse contribution, and that is because we compute it as the complementary of a fake Diffuse component.
This part should be fixed and take into account the output of the true Diffuse component instead.&lt;/p&gt;
&lt;p&gt;There is not too much room for stylizing metallic materials with this shader, it mainly depends on the environment map you use. This aspect can surely be improved a lot.&lt;/p&gt;
&lt;p&gt;Finally, we did not cover the artifacts that are due to the topology of the model that may not be adapted to cartoon/anime style, I&amp;rsquo;ll be definitely working on that part.&lt;/p&gt;
&lt;h2 id=&#34;going-further--&#34;&gt;Going further ?&lt;/h2&gt;
&lt;p&gt;From that point, I strongly encourage you to test this shader in more complex scenes like the one below. Do not hesitate to try variations, new ideas often come from testing !&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Sakura Tree 1&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Sakura Tree 2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;video autoplay loop muted controls &gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/Sakura_Tree.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;video autoplay loop muted controls &gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/Sakura_Tree_2.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This tutorial only covers the basics but you may want to go further and include texture effects like cross hatching, stippling and watercolor. These effects may rely on noises that will disturb contributions by a small amount. Of course, more advanced methods outside the scope of Blender exist like our post processing method we introduced in 
&lt;a href=&#34;../../research/coherent_splat_stylization&#34;&gt;our paper&lt;/a&gt;. As for the Watercolor and other painterly looks, you can definitely check the tools proposed by 
&lt;a href=&#34;https://artineering.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Artineering&lt;/a&gt; !&lt;/p&gt;
&lt;p&gt;You may also go further in shading control and tweak the normals of your model to get rid of unwanted artifacts. Indeed, if you want to go for anime style, unless you adapt the topology of your model to have sharp transitions and add/remove some shadows, you will have to tweak the normals. Inside Blender, you can edit your normals or you can use proxy objects to achieve a more consistent anime look. That is the topic of other tutorials you can find on the Internet but as a good start you can check the 
&lt;a href=&#34;https://www.youtube.com/watch?v=yhGjCzxJV3E&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Guilty Gear GDC conference&lt;/a&gt; that gives a good insight on how to achieve anime looking style in 3D.&lt;/p&gt;
&lt;p&gt;There is a lot more to be discovered in that field of course, never stop exploring !&lt;/p&gt;
&lt;h2 id=&#34;tutorial-files-&#34;&gt;Tutorial Files&lt;/h2&gt;
&lt;p&gt;You can find this tutorial NPR material inside this 
&lt;a href=&#34;http://typhomnt.github.io/Files/shading_npr.blend&#34;&gt;blend file&lt;/a&gt; as well as the file generating the teaser image 
&lt;a href=&#34;http://typhomnt.github.io/Files/shading_samples.blend&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Post Processing Effects</title>
      <link>http://typhomnt.github.io/post/post_proc_effects/</link>
      <pubDate>Fri, 20 Aug 2021 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/post_proc_effects/</guid>
      <description>&lt;p&gt;Here is a compilation of post processing effects implemented inside Skyengine. I implemented some of them inside Unity as well.&lt;/p&gt;
&lt;p&gt;Gundam model : Gundam GAT 105 modified by Rupen Kanwar on Sketchfab&lt;/p&gt;
&lt;h3 id=&#34;bloom-&#34;&gt;Bloom&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Post_Proc_Effects/Bloom_Sky_Gundam.PNG&#34; alt=&#34;Bloom&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;outline-&#34;&gt;Outline&lt;/h3&gt;
&lt;p&gt;Based on a Sobel filter on the Depth and Normals&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Post_Proc_Effects/Sobel_Sky_Gundam.PNG&#34; alt=&#34;Outline&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;contrast-&#34;&gt;Contrast&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Post_Proc_Effects/Contrast_Sky_Gundam.PNG&#34; alt=&#34;Contast&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;kuwahara-filter-&#34;&gt;Kuwahara Filter&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Post_Proc_Effects/Kuwahara_Sky_Gundam.PNG&#34; alt=&#34;Kuwahara&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Animation Target Constraint</title>
      <link>http://typhomnt.github.io/post/animation_target_ctr/</link>
      <pubDate>Mon, 16 Dec 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/animation_target_ctr/</guid>
      <description>&lt;p&gt;One common task game developers come across when using animated characters is to modify them in real-time in order to satisfy a given task.
More particularly, re-using the same grab/take animation or push/punch/kick animation to target a specific object is a feature every programmer want and may have to implement.&lt;/p&gt;
&lt;p&gt;When dealing with targets in animation, the first word that came into our mind is Inverse Kinematic (IK). And actually, that is a good start to tackle our problem. Indeed, for instance when a character has to grab an item, IK automatically find rotations and positions of the elbow and shoulder (can be more than that) so that the hand reach the target.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now define our problem more accurately. At a given frame of one animation, we want some body part of our character to reach a given target.
As I like to consider an animation as a time function instead of a function of frame, we define $t_t$ our target time and $p_t$ the position of the given target.&lt;/p&gt;
&lt;p&gt;For a given bone, let&amp;rsquo;s say the hand, we compute its trajectory $p(t)$ or ${p_i}$ (discretized) during the whole animation.
One could say that our problem is solved as we just have to call our IK solver at time $t_t$ so that $p(t_t) = p_t$.
However, this solution while satisfying our constraint, induce a high discontinuity in the animation. Moreover, the character&amp;rsquo;s hand reach the target at the given frame without actually taking the trajectory to reach it as the rest of the animation remains unchanged.
Our goal is then to change the overall hand motion so that it feels like the character aims to reach our target.
One formulation of this problem is to also impose that while satisfying the target constraint, our initial animation remains as consistent as possible with respect to the original.
This can be formulated as an as-rigid-as-possible (ARAP) deformation of the hand trajectory. While it was first used to 
&lt;a href=&#34;https://igl.ethz.ch/projects/ARAP/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deform 3D models&lt;/a&gt;, 
&lt;a href=&#34;https://dl.acm.org/doi/10.1145/1531326.1531385&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt; used the same principle but for bones trajectories by noticing each point of one trajectory could be expressed in a local frame of defined by its neighbours:&lt;/p&gt;
&lt;p&gt;$$
\forall i, p_i = p_{i-1} + \alpha_i \vec{A_i} + \beta_i \vec{B_i}
$$&lt;/p&gt;
&lt;p&gt;$$
\text{with } \vec{A_i} = \frac{p_{i+1} - p_{i-1}}{\left \lVert p_{i+1} - p_{i-1} \right \rVert}
$$&lt;/p&gt;





  











&lt;figure id=&#34;figure-expressing-a-point-curve-p_i-with-respect-to-its-two-neighbors-this-point-is-computed-in-its-previous-neighbor-local-frame-directed-by-the-p_i-1p_i1-vector&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Laplacian_formulation.png&#34; data-caption=&#34;Expressing a point curve $P_i$ with respect to its two neighbors. This point is computed in its previous neighbor local frame directed by the $P_{i-1}P_{i&amp;#43;1}$ vector.&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Laplacian_formulation.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Expressing a point curve $P_i$ with respect to its two neighbors. This point is computed in its previous neighbor local frame directed by the $P_{i-1}P_{i+1}$ vector.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This formulation translates the fact that $p_i$ can be expressed in a frame in which the frontal vector $\vec{A_i}$ and left vector $\vec{B_i}$ are contained in the $\hat{ p_{i-1}p_ip_{i+1}}$ plane, with $p_i$ being expressed with respect to its neighbors. Using simple trigonometry it is quite simple to compute $\alpha_i$ and $\beta_i$:&lt;/p&gt;
&lt;p&gt;$$
\alpha_i = dot(p_i - p_{i-1}\vec{A_i})
$$&lt;/p&gt;
&lt;p&gt;$$
\beta_i = \left \lVert cross(p_i - p_{i-1}\vec{A_i}) \right \rVert
$$&lt;/p&gt;
&lt;p&gt;Finally, we can deduce:&lt;/p&gt;
&lt;p&gt;$$
\vec{B_i} = \frac{p_{i} - p_{i-1} - \alpha_i\vec{A_i}}{\beta_i}
$$&lt;/p&gt;
&lt;p&gt;Using this characterization of each point of the bone&amp;rsquo;s trajectory with respect to its neighbors, we seek to conserve each original $\alpha_i$ and $\beta_i$ when deforming the trajectory to address the $ p(t_t) = p_t $ constraint. Let&amp;rsquo;s consider that $p(t_t) = p_k$ for $k \in [1:n]$. We then compute the new bone trajectory as an optimization process minimizing the cost function $L_i$:&lt;/p&gt;
&lt;p&gt;$$ L_i(p_0,&amp;hellip;,p_n) = \sum\limits_j {\left\lVert (\alpha^{init}_j,\beta^{init}_j) - (\alpha_j,\beta_j) \right\rVert}^2 + \gamma {\left\lVert p_k - p_t \right\rVert}^2 $$&lt;/p&gt;
&lt;p&gt;Using a gradient descent method we can update the ${p_i}$ trajectory at each step of the optimization process in which: $\forall j, p_j = p_j -\epsilon\nabla L_i(p_0,&amp;hellip;,p_n)_j$.&lt;/p&gt;
&lt;p&gt;




  











&lt;figure id=&#34;figure-default-slap-animation-which-do-not-reach-the-green-target&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Arlequin_Target_Fail.png&#34; data-caption=&#34;Default Slap animation which do not reach the green target.&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Arlequin_Target_Fail.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Default Slap animation which do not reach the green target.
  &lt;/figcaption&gt;


&lt;/figure&gt;






  











&lt;figure id=&#34;figure-deformed-slap-animation-using-our-optimization-algorithm-making-the-character-reach-the-green-target&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Arlequin_Target_Reach.png&#34; data-caption=&#34;Deformed Slap animation using our optimization algorithm making the character reach the green target.&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Arlequin_Target_Reach.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Deformed Slap animation using our optimization algorithm making the character reach the green target.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;video autoplay loop muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Animation/Animation_Target_CTR/Slap_Laplacian.mp4&#34; type=&#34;video/mp4&#34;&#34;&gt;
&lt;/video&gt;
</description>
    </item>
    
    <item>
      <title>Animation Transfer</title>
      <link>http://typhomnt.github.io/post/animation_transfer/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/animation_transfer/</guid>
      <description>&lt;p&gt;One main goal of my thesis is to find a way to animate 3D characters using physical props as an animation tool. The motivation is to create an animation sequence from a play with figurines, trying to reproduce what the player was imagining.&lt;/p&gt;
 &lt;!--- (I actually tried to tackle this task when I was still an ENSIMAG student during a 3 weeks project. At that time my first idea was to segment the curve using singular points and try to identify which action was performed by looking at the speed norm and direction. Basically, horizontal motions represented wlaking or running action while the other represented jumps and flying actions.
It is easy to identify the weaknesses of such an approach, it is limited to few actions and above all it is not really accurate, tweaking and thresholding is often needed for identifying singular points.
During my thesis I took inspiration from the paper untitled Motion Doodle where the authors were using 2D curves to created animation sequences . More particulary a given input curve, it was segmented into horizontal, vertical and oblic bins and animation were defined as a regular expression of curve bins. 
At that moment, I took a step back and told myself that they were creating a kind of motion language characterizing the curve by its different direction changes.) --&gt;
&lt;h3 id=&#34;space-time-doodle-transfer-examples--&#34;&gt;Space Time Doodle Transfer examples &lt;/h3&gt;
&lt;p&gt;Below, I show transfered space-time doodles into animation sequences. It is important to note that all actions were learnt for the two first examples as well as for the garden example.&lt;/p&gt;
&lt;video autoplay loop muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Animation/Animation_Transfer/PhD_Baseline.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;video autoplay loop muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Animation/Animation_Transfer/Max_Eg.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;video autoplay loop muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Animation/Animation_Transfer/Pierre_Eg.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;video autoplay loop muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Animation/Animation_Transfer/Remi_Eg.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
</description>
    </item>
    
    <item>
      <title>Base Mesh Creation</title>
      <link>http://typhomnt.github.io/post/bmesh/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/bmesh/</guid>
      <description>&lt;p&gt;This project is a second attemp to implement a fast base mesh creation method from 
&lt;a href=&#34;https://pdfs.semanticscholar.org/2009/3aea25b50e59c63998ba0377371c59bf007f.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt;.
The main idea is to create a animable mesh from a skeleton whose joints are represented by spheres with variable radius defining the distance from the mesh vertices.
This mesh generation algorithm is decomposed into 3 steps: First a simple init mesh is computed connecting each joint with successive quad extrusion and convex hull computation for T-junctions.
The resulting mesh is then refined through an iterative process, alternating subdivisions and evolutions. Finally, an edge fairing optimazation is performed whose puporse is to prevent quad deformation as much as possible without changing the geometry.&lt;/p&gt;
&lt;h3 id=&#34;bmesh-evolution-example--&#34;&gt;BMesh Evolution example &lt;/h3&gt;
&lt;video autoplay loop muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/BMesh/B_Mesh_0.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;video autoplay loop muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/BMesh/B_Mesh_1.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
</description>
    </item>
    
    <item>
      <title>Coherent Mark-based Stylization of 3D Scenes at the Compositing Stage</title>
      <link>http://typhomnt.github.io/post/npr_splatting/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/npr_splatting/</guid>
      <description>&lt;p&gt;This article describes the mark-based stylization technique that we developped at Inria with Romain Vergne, Mohamed-Amine Farhat Pierre Benard and Joelle Thollot.
This work will be presented as a full paper at the Eurographics conference and publshed in the Compter &amp;amp; Graphics Forum Journal. Our author version of the article can be found 
&lt;a href=&#34;&#34;&gt;here&lt;/a&gt;.
Our technique guarentees temporal coherence and operates at the compositing stage using G-Buffers as input, easing the integration into existing pipelines.
The source code is avaible 
&lt;a href=&#34;&#34;&gt;here&lt;/a&gt; and necessitate that you use Gratin, an open-source node-based compositing software that you can find 
&lt;a href=&#34;http://jcgt.org/published/0004/04/03/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
For windows users, we created an archive containing an executable that you can use right out of the box.&lt;/p&gt;
&lt;p&gt;This post is divided into three parts. First, I will describe the main principle behing our technique, then I will explain how to compile and use the application, then I will detail it works in the code.&lt;/p&gt;
&lt;h3 id=&#34;main-principle-&#34;&gt;Main principle&lt;/h3&gt;
&lt;p&gt;In the ocean of NPR techniques, we can distinguish two different families when it come to stylizing color regions: texture-based and mark-based approaches.
The first embeds marks in an image which is then mapped or pached either onto the 3D scene or over the entire screen. Here are some reference of previous research works falling into that category.
More concretly, this kind of technique is quite common when using 3D software like Maya or Blender.
The later consists in attaching 2D or 3D marks (brush strokes) to anchor points distributed onto the 3D scene.&lt;/p&gt;
&lt;p&gt;Our method belongs to the familly of mark-based methods but borrows ideas from texture-based methods.
Moreover, it consists in dynamically generating a motion-coherent 3D anchor point distribution that are drawn as billboard brush strokes.
In order to ensure temporal continuity as well as motion coherence at the compositing stage, the anchor point generation is based on implicit Voronoi noise generated from G-Buffers. This allows our technique to operate  regardless of the geometric representation or animation technique.&lt;/p&gt;
&lt;h4 id=&#34;anchor-point-generation-&#34;&gt;Anchor Point Generation&lt;/h4&gt;
&lt;p&gt;Our anchor point generation algorithm is two folds: a first process generates anchor points around the objects based on a 3D Worley noise which takes 3D positions stored in the G-Buffers as input. Then, we group together all pixels that generated the same anchor point and compute its final position in screen-space as the average of the screen-space position of these pixels. To ensure a quasi constant density of the points, especially when zooming in or out, we introduce a fractalization process which generate several layers of points. Moreover, at each pixel of the position G-Buffer, we generate several points using the same process as earlier but with a different frequency for each layer. For each layer its sampling frequency is related to the gradient of the position at the considered pixel such that when the gradient is low, meaning that the surface is flat, we increase the sampling frequency and increase it other wise. The intuition behind this process is to sample more points when the surface is flat because at a given frequency of the Worley noise, more points are generated when the position varies, the surface of the object intersecting more virtual 3D cells than when it is flat.&lt;/p&gt;
&lt;p&gt;Let me show you the difference of our anchor point genereation process with and without the fractalization :&lt;/p&gt;
&lt;p&gt;&lt;video autoplay loop muted playsinline&gt;]
&lt;source src=&#34;http://typhomnt.github.io/Videos/SplatNPR/Fractalization_Zoom.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;
&lt;p&gt;&lt;video autoplay loop muted playsinline&gt;]
&lt;source src=&#34;http://typhomnt.github.io/Videos/SplatNPR/Splat_Generation_Animation_Sub.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;
&lt;h4 id=&#34;improve-temporal-continuity-&#34;&gt;Improve Temporal Continuity&lt;/h4&gt;
&lt;p&gt;Until this stage we focused&lt;/p&gt;
&lt;h3 id=&#34;use-the-application-&#34;&gt;Use the application&lt;/h3&gt;
&lt;h3 id=&#34;understanding-the-code-&#34;&gt;Understanding the code&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Laban Effort Animation Transfer</title>
      <link>http://typhomnt.github.io/post/laban_transfer/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/laban_transfer/</guid>
      <description>&lt;p&gt;One important aspect of my thesis work is adding expressivity to an input &amp;ldquo;neutral&amp;rdquo; animation. In our work, we decided to take the Laban Effort space as the main representation of the expressivity.
This 4D space is divided into Space, Time, Weight and Flow axis. The Space axis describes how direct or indirect a movment is, Time describes if a movment is rather sudden or sustained while Weight differenciate Light from Strong motions. Finally Flow describes if a motion sequence is free or bound. In this article, I will only present how to transfer Time and Weight to a neutral animation.
To do so, I will first introduce three animation operators: scaling, retiming and shaping operators.&lt;/p&gt;
&lt;h3 id=&#34;scaling-&#34;&gt;Scaling&lt;/h3&gt;
&lt;h3 id=&#34;efforts-comparison--&#34;&gt;Efforts Comparison &lt;/h3&gt;
&lt;p&gt;Finally, I show below the comparison of the 4 efforts for several animations of the Mixamo database
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/kick_laban.gif&#34; alt=&#34;Kick Laban&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/punch_laban.gif&#34; alt=&#34;Punch Laban&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/stomp_laban.gif&#34; alt=&#34;Stomp Laban&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/throw_laban.gif&#34; alt=&#34;Throw Laban&#34;&gt;&lt;/p&gt;
&lt;p&gt;I personnaly think that those modifiers do the job as Laban Effort qualities are recognizable. That remains true as long as the animation phases are correctly defined. Still there are ways of improvment: first, foot sliding should be removed for light and strong modifiers; secondly, light and strong motions are sometimes too fast for some animations. This is due to the fact that some animations present long moments where there is no speed.&lt;/p&gt;
&lt;!---In furture experimentations, I will try to use the equi-affine speed instead of the speed for the retiming operator, taking into account animation breakdowns.--&gt;
&lt;p&gt;Comparisons:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Arm_Gesture_Light.mp4.gif&#34; alt=&#34;Raise Light&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Arm_Gesture_Strong.mp4.gif&#34; alt=&#34;Raise Strong&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Arm_Gesture_Sudden.mp4.gif&#34; alt=&#34;Raise Sudden&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Arm_Gesture_Sustained.mp4.gif&#34; alt=&#34;Raise Sustained&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Punch_Light.mp4.gif&#34; alt=&#34;Punch Light&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Punch_Strong.mp4.gif&#34; alt=&#34;Punch Strong&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Punch_Sudden.mp4.gif&#34; alt=&#34;Punch Sudden&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Punch_Sustained.mp4.gif&#34; alt=&#34;Punch Sustained&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Jump_Light.mp4.gif&#34; alt=&#34;Jump Light&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Jump_Strong.mp4.gif&#34; alt=&#34;Jump Strong&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Jump_Sudden.mp4.gif&#34; alt=&#34;Jump Sudden&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Jump_Sustained.mp4.gif&#34; alt=&#34;Jump Sustained&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Flip_Light.mp4.gif&#34; alt=&#34;Flip Light&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Flip_Strong.mp4.gif&#34; alt=&#34;Flip Strong&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Flip_Sudden.mp4.gif&#34; alt=&#34;Flip Sudden&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Flip_Sustained.mp4.gif&#34; alt=&#34;RaFlipise Sustained&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Stomp_Light.mp4.gif&#34; alt=&#34;Stomp Light&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Stomp_Strong.mp4.gif&#34; alt=&#34;Stomp Strong&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Stomp_Sudden.mp4.gif&#34; alt=&#34;Stomp Sudden&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Stomp_Sustained.mp4.gif&#34; alt=&#34;Stomp Sustained&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
