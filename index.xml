<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Maxime Garcia</title>
    <link>http://typhomnt.github.io/</link>
      <atom:link href="http://typhomnt.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Maxime Garcia</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 20 Feb 2019 16:29:59 +0100</lastBuildDate>
    <image>
      <url>http://typhomnt.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_box_center_2.png</url>
      <title>Maxime Garcia</title>
      <link>http://typhomnt.github.io/</link>
    </image>
    
    <item>
      <title>Oxyde ⌀  Redux edition</title>
      <link>http://typhomnt.github.io/project/oxyde/</link>
      <pubDate>Mon, 12 Apr 2021 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/project/oxyde/</guid>
      <description>&lt;!--&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/SkyEngine/SkyLogoMob.png&#34; alt=&#34;SkyLogo&#34; style=&#34;width:200px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;p&gt;Oxyde ⌀  Redux Edition is an arcade game I worked on with 5 teammates during the 
&lt;a href=&#34;https://www.grenoblegamelab.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scientific Game Jam&lt;/a&gt; 2020. This game jam focuses on creating games in 48 hours whose theme is linked with PhD students&amp;rsquo; research. And here it is, we created this game in a style between Super Hexagon and Touhou, where the player has to maintain the temperature of a cooling metalic glass at a medium while avoiding oxydation. The main principle is to catch heat particles but not too much while protecting the core with a controlable shield against both heat and oxygen particles. During the project I worked on the graphics of the game that are mainly built on Unity Shader Graphs.&lt;/p&gt;
&lt;p&gt;You can download and test the game on the 
&lt;a href=&#34;https://eikins.itch.io/oxyde-redux-edition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Itch page&lt;/a&gt; of the project.&lt;/p&gt;
&lt;p&gt;&lt;video autoplay loop muted playsinline&gt;]
&lt;source src=&#34;http://typhomnt.github.io/Videos/Oxyde/oxyde.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>3D Art</title>
      <link>http://typhomnt.github.io/art/3d_art/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/art/3d_art/</guid>
      <description>







  
  


&lt;div class=&#34;gallery&#34;&gt;

  
  
  
  
    
    
    
    
    
      
        
      
    
  &lt;a data-fancybox=&#34;gallery-3DC_Images&#34; href=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/3DC_Can.png&#34; data-caption=&#34;Can with procedural textures&#34;&gt;
  &lt;img data-src=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/3DC_Can_hufbc3b231a6e228efd72eb7d59fa8c922_2578240_0x190_resize_box_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;338&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
      
    
  &lt;a data-fancybox=&#34;gallery-3DC_Images&#34; href=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/Crystal_Table.png&#34; &gt;
  &lt;img data-src=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/Crystal_Table_hu4521d10d33b83edcf467a9c48d43873e_2842001_0x190_resize_box_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;338&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
      
    
  &lt;a data-fancybox=&#34;gallery-3DC_Images&#34; href=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/Sky_Lantern.png&#34; &gt;
  &lt;img data-src=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/Sky_Lantern_hue90aa73afc960b83ee08100c5c4c19a4_2859613_0x190_resize_box_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;338&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
      
    
  &lt;a data-fancybox=&#34;gallery-3DC_Images&#34; href=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/Throne_Nier.png&#34; &gt;
  &lt;img data-src=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/Throne_Nier_hu077fe2a572b0279ccb7185de22ffd95e_2184552_0x190_resize_box_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;338&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  

  
&lt;/div&gt;
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt;&lt;iframe width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/8917c0bb4e3944f2ae412a41bf33516c/embed&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; fullscreen; vr&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt;
    &lt;a href=&#34;https://sketchfab.com/3d-models/the-band-of-the-hawk-8917c0bb4e3944f2ae412a41bf33516c?utm_medium=embed&amp;utm_source=website&amp;utm_campaign=share-popup&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;The Band of the Hawk&lt;/a&gt;
    by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_source=website&amp;utm_campaign=share-popup&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;typhomnt&lt;/a&gt;
    on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_source=website&amp;utm_campaign=share-popup&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt;&lt;iframe width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/6c98855d813d4cdb8e9868262d7e0a89/embed&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; fullscreen; vr&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt;
    &lt;a href=&#34;https://sketchfab.com/3d-models/skyengine-bob-6c98855d813d4cdb8e9868262d7e0a89?utm_medium=embed&amp;utm_source=website&amp;utm_campaign=share-popup&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;SkyEngine Bob&lt;/a&gt;
    by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_source=website&amp;utm_campaign=share-popup&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;typhomnt&lt;/a&gt;
    on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_source=website&amp;utm_campaign=share-popup&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt;&lt;iframe width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/e0567743ac454070968d1005428ec3d6/embed&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; fullscreen; vr&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt;
    &lt;a href=&#34;https://sketchfab.com/3d-models/phantasy-star-online-2-ray-partizan-e0567743ac454070968d1005428ec3d6?utm_medium=embed&amp;utm_source=website&amp;utm_campaign=share-popup&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Phantasy Star Online 2 Ray Partizan&lt;/a&gt;
    by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_source=website&amp;utm_campaign=share-popup&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;typhomnt&lt;/a&gt;
    on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_source=website&amp;utm_campaign=share-popup&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt;&lt;iframe width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/df663648a1cc41928ba21dc09f7b4dba/embed&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; fullscreen; vr&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt;
    &lt;a href=&#34;https://sketchfab.com/3d-models/soul-calibur-fan-art-first-attempt-df663648a1cc41928ba21dc09f7b4dba?utm_medium=embed&amp;utm_source=website&amp;utm_campaign=share-popup&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Soul Calibur Fan Art First Attempt&lt;/a&gt;
    by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_source=website&amp;utm_campaign=share-popup&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;typhomnt&lt;/a&gt;
    on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_source=website&amp;utm_campaign=share-popup&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt;&lt;iframe width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/157800b903a647308a25627f134132b6/embed&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; fullscreen; vr&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt;
    &lt;a href=&#34;https://sketchfab.com/3d-models/soul-edge-fan-art-first-attempt-157800b903a647308a25627f134132b6?utm_medium=embed&amp;utm_source=website&amp;utm_campaign=share-popup&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Soul Edge Fan art First Attempt&lt;/a&gt;
    by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_source=website&amp;utm_campaign=share-popup&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;typhomnt&lt;/a&gt;
    on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_source=website&amp;utm_campaign=share-popup&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Animation Target Constraint</title>
      <link>http://typhomnt.github.io/post/animation_target_ctr/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/animation_target_ctr/</guid>
      <description>&lt;p&gt;One common problem game developers come across when using animated characters is to modify them in real-time in order to satisfy a given task.
More particularly, re-using the same grab/take animation or push/punch/kick animation to target a specific object is a feature every programmer want and may have to implement.&lt;/p&gt;
&lt;p&gt;When dealing with targets in animation, the first word that came into our mind is Inverse Kinematic (IK). And actually, that is a good start to tackle our problem. Indeed, for instance when a character has to grab an item, IK automatically find rotations and positions of the elbow and shoulder (can be more than that) so that the hand reach the target.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now define our problem more accurately. At a given frame of one animation, we want some body part of our character to reach a given target.
As I like to consider an animation as a time function instead of a function of frame, we define $t_t$ our target time and $p_t$ the position of the given target.&lt;/p&gt;
&lt;p&gt;For a given bone, let&amp;rsquo;s say the hand, we compute its trajectory $p(t)$ or ${p_i}$ (discretized) during the whole animation.
One could say that our problem is solved as we just have to call our IK solver at time $t_t$ so that $p(t_t) = p_t$.
However, this solution while satisfying our constraint, induce a high discontinuity in the animation. Moreover, the character&amp;rsquo;s hand reach the target at the given frame without actually taking the trajectory to reach it as the rest of the animation remains unchanged.
Our goal is then to change the overall hand motion so that it feels like the character aims to reach our target.
One formulation of this problem is to also impose that while satisfying the target constraint, our initial animation remains as consistent as possible with respect to the original.
This can be formulated as an as-rigid-as-possible (ARAP) deformation of the hand trajectory. While it was first used to 
&lt;a href=&#34;https://igl.ethz.ch/projects/ARAP/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deform 3D models&lt;/a&gt;, 
&lt;a href=&#34;https://dl.acm.org/doi/10.1145/1531326.1531385&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt; used the same principle but for bones trajectories by noticing each point of one trajectory could be expressed in a local frame of defined by its neighbours:&lt;/p&gt;
&lt;p&gt;$$
\forall i, p_i = p_{i-1} + \alpha_i \vec{A_i} + \beta_i \vec{B_i}
$$&lt;/p&gt;
&lt;p&gt;$$
\text{with } \vec{A_i} = \frac{p_{i+1} - p_{i-1}}{\left \lVert p_{i+1} - p_{i-1} \right \rVert}
$$&lt;/p&gt;





  











&lt;figure id=&#34;figure-expressing-a-point-curve-p_i-with-respect-to-its-two-neighbors-this-point-is-computed-in-its-previous-neighbor-local-frame-directed-by-the-p_i-1p_i1-vector&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Laplacian_formulation.png&#34; data-caption=&#34;Expressing a point curve $P_i$ with respect to its two neighbors. This point is computed in its previous neighbor local frame directed by the $P_{i-1}P_{i&amp;#43;1}$ vector.&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Laplacian_formulation.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Expressing a point curve $P_i$ with respect to its two neighbors. This point is computed in its previous neighbor local frame directed by the $P_{i-1}P_{i+1}$ vector.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This formulation translates the fact that $p_i$ can be expressed in a frame in which the frontal vector $\vec{A_i}$ and left vector $\vec{B_i}$ are contained in the $\hat{ p_{i-1}p_ip_{i+1}}$ plane, with $p_i$ being expressed with respect to its neighbors. Using simple trigonometry it is quite simple to compute $\alpha_i$ and $\beta_i$:&lt;/p&gt;
&lt;p&gt;$$
\alpha_i = dot(p_i - p_{i-1}\vec{A_i})
$$&lt;/p&gt;
&lt;p&gt;$$
\beta_i = \left \lVert cross(p_i - p_{i-1}\vec{A_i}) \right \rVert
$$&lt;/p&gt;
&lt;p&gt;Finally, we can deduce:&lt;/p&gt;
&lt;p&gt;$$
\vec{B_i} = \frac{p_{i} - p_{i-1} - \alpha_i\vec{A_i}}{\beta_i}
$$&lt;/p&gt;
&lt;p&gt;Using this characterization of each point of the bone&amp;rsquo;s trajectory with respect to its neighbors, we seek to conserve each original $\alpha_i$ and $\beta_i$ when deforming the trajectory to address the $ p(t_t) = p_t $ constraint. Let&amp;rsquo;s consider that $p(t_t) = p_k$ for $k \in [1:n]$. We then compute the new bone trajectory as an optimization process minimizing the cost function $L_i$:&lt;/p&gt;
&lt;p&gt;$$ L_i(p_0,&amp;hellip;,p_n) = \sum\limits_j {\left\lVert (\alpha^{init}_j,\beta^{init}_j) - (\alpha_j,\beta_j) \right\rVert}^2 + \gamma {\left\lVert p_k - p_t \right\rVert}^2 $$&lt;/p&gt;
&lt;p&gt;Using a gradient descent method we can update the ${p_i}$ trajectory at each step of the optimization process in which: $\forall j, p_j = p_j -\epsilon\nabla L_i(p_0,&amp;hellip;,p_n)_j$.&lt;/p&gt;
&lt;p&gt;




  











&lt;figure id=&#34;figure-default-slap-animation-which-do-not-reach-the-green-target&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Arlequin_Target_Fail.png&#34; data-caption=&#34;Default Slap animation which do not reach the green target.&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Arlequin_Target_Fail.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Default Slap animation which do not reach the green target.
  &lt;/figcaption&gt;


&lt;/figure&gt;






  











&lt;figure id=&#34;figure-deformed-slap-animation-using-our-optimization-algorithm-making-the-character-reach-the-green-target&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Arlequin_Target_Reach.png&#34; data-caption=&#34;Deformed Slap animation using our optimization algorithm making the character reach the green target.&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Arlequin_Target_Reach.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Deformed Slap animation using our optimization algorithm making the character reach the green target.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/SkyEngine/Slap_LaplacianEdit.gif&#34; alt=&#34;Example 2&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/SkyEngine/Kick_LaplacianEdit.gif&#34; alt=&#34;Example 1&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Animation Transfer</title>
      <link>http://typhomnt.github.io/post/animation_transfer/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/animation_transfer/</guid>
      <description>&lt;p&gt;One main goal of my thesis is to find a way to animate 3D characters using physical props as an animation tool. The motivation is to create an animation sequence from a play with figurines, trying to reproduce what the player was imagining.&lt;/p&gt;
 &lt;!--- (I actually tried to tackle this task when I was still an ENSIMAG student during a 3 weeks project. At that time my first idea was to segment the curve using singular points and try to identify which action was performed by looking at the speed norm and direction. Basically, horizontal motions represented wlaking or running action while the other represented jumps and flying actions.
It is easy to identify the weaknesses of such an approach, it is limited to few actions and above all it is not really accurate, tweaking and thresholding is often needed for identifying singular points.
During my thesis I took inspiration from the paper untitled Motion Doodle where the authors were using 2D curves to created animation sequences . More particulary a given input curve, it was segmented into horizontal, vertical and oblic bins and animation were defined as a regular expression of curve bins. 
At that moment, I took a step back and told myself that they were creating a kind of motion language characterizing the curve by its different direction changes.) --&gt;
&lt;h3 id=&#34;space-time-doodle-transfer-examples--&#34;&gt;Space Time Doodle Transfer examples &lt;/h3&gt;
&lt;p&gt;Below, I show transfered space-time doodles into animation sequences. It is important to note that all actions were learnt for the two first examples as well as for the garden example.
&lt;img src=&#34;http://typhomnt.github.io/Images/Animation_Transfer/Anim_eg1.gif&#34; alt=&#34;Example 1&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Animation_Transfer/Anim_eg2.gif&#34; alt=&#34;Example 2&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Animation_Transfer/Anim_eg3.gif&#34; alt=&#34;Example 3&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Base Mesh Creation</title>
      <link>http://typhomnt.github.io/post/bmesh/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/bmesh/</guid>
      <description>&lt;p&gt;This project is in fact a second attemp to implement a fast base mesh creation method from 
&lt;a href=&#34;https://pdfs.semanticscholar.org/2009/3aea25b50e59c63998ba0377371c59bf007f.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt;.
The main idea is to create a animable mesh from a skeleton whose joints are represented by spheres with variable radius defining the distance from the mesh vertices.
This mesh generation algorithm is decomposed into 3 steps: First a simple init mesh is computed connecting each joint with successive quad extrusion and convex hull computation for T-junctions.
The resulting mesh is then refined through an iterative process, alternating subdivisions and evolutions. Finally, a an edge fairing optimazation is performed whose puporse is to prevent quad deformation as much as possible without changing the geometry.&lt;/p&gt;
&lt;h3 id=&#34;bmesh-evolution-example--&#34;&gt;BMesh Evolution example &lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/BMesh/bmesh0.gif&#34; alt=&#34;Example 1&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/BMesh/B_Mesh_1.gif&#34; alt=&#34;Example 2&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bloom Effect</title>
      <link>http://typhomnt.github.io/post/bloom_effect/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/bloom_effect/</guid>
      <description>&lt;p&gt;Here is an example of Bloom post process effect implemented in the SkyEngine&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/SkyEngine/BloomEffect.gif&#34; alt=&#34;Example 1&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coherent Mark-based Stylization of 3D Scenes at the Compositing Stage</title>
      <link>http://typhomnt.github.io/research/coherent_splat_stylization/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/research/coherent_splat_stylization/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/splat_teaser.jpg&#34; alt=&#34;Teaser&#34;&gt;&lt;/p&gt;
&lt;p&gt;Comptuer Graphics Forum, Eurographics 2021. Article available 
&lt;a href=&#34;https://hal.inria.fr/hal-03143244&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;authors-&#34;&gt;Authors&lt;/h2&gt;
&lt;p&gt;Maxime Garcia
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Romain Vergne
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Mohamed-Amine Farhat
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Pierre Benard
(Inria / LaBRI)&lt;/p&gt;
&lt;p&gt;Camille Nous
(Laboratoire Cogitamus)&lt;/p&gt;
&lt;p&gt;Joelle Thollot
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;h3 id=&#34;abstract-&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present a novel temporally coherent stylized rendering technique working entirely at the compositing stage. We first generate a distribution of 3D anchor points using an implicit grid based on the local object positions stored in a G-buffer, hence following object motion. We then draw splats in screen space anchored to these points so as to be motion coherent. To increase the perceived flatness of the style, we adjust the anchor points density using a fractalization mechanism. Sudden changes are prevented by controlling the anchor points opacity and introducing a new order-independent blending function. We demonstrate the versatility of our method by showing a large variety of styles thanks to the freedom offered by the splats content and their attributes that can be controlled by any G-buffer.&lt;/p&gt;
&lt;h4 id=&#34;main-video-&#34;&gt;Main Video&lt;/h4&gt;
&lt;center&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/466IoPKs0p0&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;
&lt;h4 id=&#34;supplementals-&#34;&gt;Supplementals&lt;/h4&gt;
&lt;center&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/sxU_ZEjwwjI&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;
&lt;h4 id=&#34;app-and-turorial-&#34;&gt;App and Turorial&lt;/h4&gt;
&lt;p&gt;Available soon !&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Functionnal Programming Resources</title>
      <link>http://typhomnt.github.io/teaching/functionnal_programming/functionnal_programming_practs/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/teaching/functionnal_programming/functionnal_programming_practs/</guid>
      <description>&lt;p&gt;This course is proposed to DLST computer science Licence 1 students and presents an introduction to Functionnal Programming in OCaml.&lt;/p&gt;
&lt;h2 id=&#34;practicals&#34;&gt;Practicals&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/L1_Fractales.pdf&#34;&gt;Fractals&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/L1_Fractales_Sol.ml&#34;&gt;Fractals Solution&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/L1_FP_Project.pdf&#34;&gt;Optional Project&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Laban Effort Animation Transfer</title>
      <link>http://typhomnt.github.io/post/laban_transfer/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/laban_transfer/</guid>
      <description>&lt;p&gt;One important aspect of my thesis work is adding expressivity to an input &amp;ldquo;neutral&amp;rdquo; animation. In our work, we decided to take the Laban Effort space as the main representation of the expressivity.
This 4D space is divided into Space, Time, Weight and Flow axis. The Space axis describes how direct or indirect a movment is, Time describes if a movment is rather sudden or sustained while Weight differenciate Light from Strong motions. Finally Flow describes if a motion sequence is free or bound. In this article, I will only present how to transfer Time and Weight to a neutral animation.
To do so, I will first introduce three animation operators: scaling, retiming and shaping operators.&lt;/p&gt;
&lt;h3 id=&#34;scaling-&#34;&gt;Scaling&lt;/h3&gt;
&lt;h3 id=&#34;efforts-comparison--&#34;&gt;Efforts Comparison &lt;/h3&gt;
&lt;p&gt;Finally, I show below the comparison of the 4 efforts for several animations of the Mixamo database
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/kick_laban.gif&#34; alt=&#34;Kick Laban&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/punch_laban.gif&#34; alt=&#34;Punch Laban&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/stomp_laban.gif&#34; alt=&#34;Stomp Laban&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/throw_laban.gif&#34; alt=&#34;Throw Laban&#34;&gt;&lt;/p&gt;
&lt;p&gt;I personnaly think that those modifiers do the job as Laban Effort qualities are recognizable. That remains true as long as the animation phases are correctly defined. Still there are ways of improvment: first, foot sliding should be removed for light and strong modifiers; secondly, light and strong motions are sometimes too fast for some animations. This is due to the fact that some animations present long moments where there is no speed.&lt;/p&gt;
&lt;!---In furture experimentations, I will try to use the equi-affine speed instead of the speed for the retiming operator, taking into account animation breakdowns.--&gt;
&lt;p&gt;Comparisons:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Arm_Gesture_Light.mp4.gif&#34; alt=&#34;Raise Light&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Arm_Gesture_Strong.mp4.gif&#34; alt=&#34;Raise Strong&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Arm_Gesture_Sudden.mp4.gif&#34; alt=&#34;Raise Sudden&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Arm_Gesture_Sustained.mp4.gif&#34; alt=&#34;Raise Sustained&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Punch_Light.mp4.gif&#34; alt=&#34;Punch Light&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Punch_Strong.mp4.gif&#34; alt=&#34;Punch Strong&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Punch_Sudden.mp4.gif&#34; alt=&#34;Punch Sudden&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Punch_Sustained.mp4.gif&#34; alt=&#34;Punch Sustained&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Jump_Light.mp4.gif&#34; alt=&#34;Jump Light&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Jump_Strong.mp4.gif&#34; alt=&#34;Jump Strong&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Jump_Sudden.mp4.gif&#34; alt=&#34;Jump Sudden&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Jump_Sustained.mp4.gif&#34; alt=&#34;Jump Sustained&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Flip_Light.mp4.gif&#34; alt=&#34;Flip Light&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Flip_Strong.mp4.gif&#34; alt=&#34;Flip Strong&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Flip_Sudden.mp4.gif&#34; alt=&#34;Flip Sudden&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Flip_Sustained.mp4.gif&#34; alt=&#34;RaFlipise Sustained&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Stomp_Light.mp4.gif&#34; alt=&#34;Stomp Light&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Stomp_Strong.mp4.gif&#34; alt=&#34;Stomp Strong&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Stomp_Sudden.mp4.gif&#34; alt=&#34;Stomp Sudden&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Stomp_Sustained.mp4.gif&#34; alt=&#34;Stomp Sustained&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Music</title>
      <link>http://typhomnt.github.io/art/music/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/art/music/</guid>
      <description>&lt;p&gt;Here are some of my piano recordings.&lt;/p&gt;
&lt;p&gt;Chopin Etude 5 Op 25 &amp;ldquo;Wrong Notes&amp;rdquo; (Recorded in 2015)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34;&gt;
  &lt;source type=&#34;audio/mp3&#34; src=&#34;http://typhomnt.github.io/Audio/Wrong_Notes_good.mp3&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
&lt;p&gt;To Zanarkand Final Fantasy X (Recorded in 2020)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34;&gt;
  &lt;source type=&#34;audio/mp3&#34; src=&#34;http://typhomnt.github.io/Audio/zanarkand_final.mp3&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
&lt;p&gt;Grandma Nier (Recorded in 2021)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34;&gt;
  &lt;source type=&#34;audio/mp3&#34; src=&#34;http://typhomnt.github.io/Audio/Grandma_clear_ver.mp3&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
</description>
    </item>
    
    <item>
      <title>Ray Tracing Resources</title>
      <link>http://typhomnt.github.io/teaching/ray_tracing/raytracing_practs/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/teaching/ray_tracing/raytracing_practs/</guid>
      <description>&lt;p&gt;This course is proposed to MSIAM M1 students and presents an introduction to Ray casting, Ray tracing, Ray Marching and Physically Based Rendering methods.
I would like to thank Romain Vergne for his course slides and the two first practicals.&lt;/p&gt;
&lt;h2 id=&#34;course-slides&#34;&gt;Course Slides&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/MSIAM_Course1__Ray_Casting.pdf&#34;&gt;Ray Casting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/MSIAM_Course2__Ray_Tracing.pdf&#34;&gt;Ray Tracing/Ray Marching&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/MSIAM_Course3__PBR.pdf&#34;&gt;Physically Based Rendering&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;tutorials&#34;&gt;Tutorials&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;../pbr_intro&#34;&gt;Physically Based Rendering&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;../raymarching_intro&#34;&gt;Ray Marching&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;practicals&#34;&gt;Practicals&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/MSIAM_Code.zip&#34;&gt;Base Code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/MSIAM_TP1__Ray_Casting.pdf&#34;&gt;Ray Casting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/MSIAM_TP2__Ray_Tracing.pdf&#34;&gt;Ray Tracing&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/MSIAM_TP4__Ray_Marching.pdf&#34;&gt;Ray Marching&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recognition of Laban Effort Qualities from Hand Motion</title>
      <link>http://typhomnt.github.io/research/recognition_laban_hand/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/research/recognition_laban_hand/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Classification/effort_cube.jpg&#34; alt=&#34;Teaser&#34;&gt;&lt;/p&gt;
&lt;p&gt;MOCO&amp;rsquo;20 - 7th International Conference on Movement and Computing, Jul 2020, Jersey City/ Virtual, United States. Article available 
&lt;a href=&#34;https://hal.inria.fr/hal-02899999&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;authors-&#34;&gt;Authors&lt;/h2&gt;
&lt;p&gt;Maxime Garcia
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Remi Ronfard
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;h3 id=&#34;abstract-&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this paper, we conduct a study for recognizing motion qualities in hand gestures using virtual reality trackers attached to the hand. From this 6D signal, we extract Euclidean, equi-affine and moving frame features and compare their effectiveness in the task of recognizing Laban Effort qualities. Our experimental results reveal that equi-affine features are highly discriminant features for this task. We also compare two classification methods on this task. In the first method, we trained separate HMM models for the 6 Laban Effort qualities (light, strong, sudden, sustained, direct, indirect). In the second method, we trained separate HMM models for the 8 Laban motion verbs (dab, glide, float, flick, thrust, press, wring, slash) and combined them to recognize individual qualities. In our experiments, the second method gives improved results. Together, those findings suggest that low-dimensional signals from VR trackers can be used to predict motion qualities with reasonable precision.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SkyEngine</title>
      <link>http://typhomnt.github.io/project/skyengine/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/project/skyengine/</guid>
      <description>&lt;!--&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/SkyEngine/SkyLogoMob.png&#34; alt=&#34;SkyLogo&#34; style=&#34;width:200px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;p&gt;SkyEngine is yet another game engine I am working on together with co-workers and former collegues. The main goal of this engine is to implement recent research work, to make video game and animated movie content easier to create. I would like to thank Julien Daval, Guillaume Cordonnier, Valentin Touzeau, Pierre Casati and Remi Galan Alfonso for contributing to this framework.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spatial Motion Doodles: Sketching Animation in VR Using Hand Gestures and Laban Motion Analysis</title>
      <link>http://typhomnt.github.io/research/spatial_motion_doodle/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/research/spatial_motion_doodle/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Animation_Transfer/SMD_Teaser.png&#34; alt=&#34;Teaser&#34;&gt;&lt;/p&gt;
&lt;p&gt;ACM Motion, Interaction and Games 2019 (MIG &amp;lsquo;19)&lt;/p&gt;
&lt;h2 id=&#34;authors-&#34;&gt;Authors&lt;/h2&gt;
&lt;p&gt;Maxime Garcia
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Remi Ronfard
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Marie-Paule Cani
(Ecole Polythecnique / LIX)&lt;/p&gt;
&lt;h3 id=&#34;abstract-&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present a method for easily drafting expressive character animation by playing with instrumented rigid objects. We parse the input 6D trajectories (position and orientation over time) – called spatial motion doodles – into sequences of actions and convert them into detailed character animations using a dataset of parameterized motion clips which are automatically fitted to the doodles in terms of global trajectory and timing. Moreover, we capture the expressiveness of user-manipulation by analyzing Laban effort qualities in the input spatial motion doodles and transferring them to the synthetic motions we generate. We validate the ease of use of our system and the expressiveness of the resulting animations through a series of user studies, showing the interest of our approach for interactive digital storytelling applications dedicated to children and non-expert users, as well as for providing fast drafting tools for animators.&lt;/p&gt;
&lt;center&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/0xG2dlGg_9M&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;
&lt;center&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/5BDqJQ6XK_k&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to Physically Based Rendering</title>
      <link>http://typhomnt.github.io/teaching/ray_tracing/pbr_intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://typhomnt.github.io/teaching/ray_tracing/pbr_intro/</guid>
      <description>&lt;!--
&lt;script type=&#34;text/x-mathjax-config&#34;&gt;
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],
    displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\[&#39;,&#39;\]&#39;]],
    processEscapes: true,
    processEnvironments: true,
    skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;],
    TeX: { equationNumbers: { autoNumber: &#34;AMS&#34; },
         extensions: [&#34;AMSmath.js&#34;, &#34;AMSsymbols.js&#34;] }
  }
});
&lt;/script&gt;

&lt;script type=&#34;text/x-mathjax-config&#34;&gt;
  MathJax.Hub.Queue(function() {
    // Fix &lt;code&gt; tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i &lt; all.length; i += 1) {
        all[i].SourceElement().parentNode.className += &#39; has-jax&#39;;
    }
});
&lt;/script&gt;


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

--&gt;
&lt;p&gt;This tutorial is inspired from 
&lt;a href=&#34;https://learnopengl.com/PBR/Theory&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://learnopengl.com/PBR/Theory&lt;/a&gt; and adapted for the ray-tracing course available 
&lt;a href=&#34;../raytracing_practs&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;introduction-&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;A big challenge in computer graphics is to design shading models mimicking real-life lighting behaviour while allowing intuitive control of object materials. This control is crucial for artists who are creating assets that have to be integrated in a rendering pipeline. For real-time applications like video games, the question of computation time and memory cost is also essential; such models must be flexible enough and allow affordable approximations.&lt;/p&gt;
&lt;p&gt;In this tutorial, we are interested in Physically Based Rendering (PBR) models which aim at simulating light behaviour in a more realistic way, approximating light related equations (models like the Phong model are very simplistic in comparison).
One important aspect of those models is their energy conservative property stating that when interacting with a surface, the amount of outgoing light is equal to the amount of incoming light. More precisely, the amount of light absorbed, scattered and diffused by object surfaces is equal to the amount of light hitting the surface. The figure below illustrates these phenomena.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/LightInteraction.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/LightInteraction.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;!-- &lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/LightInteraction.png&#34; alt=&#34;LightInteract&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;h2 id=&#34;surface-representation-micro-facet-model-&#34;&gt;Surface Representation: Micro-Facet model&lt;/h2&gt;
&lt;p&gt;Simulating this behaviour is highly correlated with how we represent object surfaces. Indeed, light interaction with object surfaces is modeled by the Snell-Descartes law which describes how incident light gets refracted and reflected on a flat surface.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/snelldescartes.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/snelldescartes.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;!-- &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/snelldescartes.png&#34; alt=&#34;SnellDescartes&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;p&gt;However, in practice object surfaces are not completely flat, some are rougher than others. This is something noticeable in real life, especially when you look at specular reflection on different objects. More precisely, rough surfaces tend to produce blurred reflections while smooth surfaces behave like mirrors.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Rough Surface&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Smooth Surface&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;




  











&lt;figure id=&#34;figure-rough-surface&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/rough_photo.png&#34; data-caption=&#34;Rough Surface&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/rough_photo.png&#34; alt=&#34;&#34; width=&#34;300px&#34; &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Rough Surface
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;




  











&lt;figure id=&#34;figure-smooth-surface&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/smooth_photo.png&#34; data-caption=&#34;Smooth Surface&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/smooth_photo.png&#34; alt=&#34;&#34; width=&#34;300px&#34; &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Smooth Surface
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Thus, the microfacet model was introduced and defines a surface by a continuous sequence of flat micro-surfaces that might be oriented differently, simulating the smooth vs rough aspect of macro-surfaces. The roughness property of a material plays a meaningful role in light behavior as it controls the amount of light that gets reflected and refracted as well as the direction of outgoing light.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Rough Surface&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Smooth Surface&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/roughsurface.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/smoothsurface.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;light-equation-&#34;&gt;Light Equation&lt;/h2&gt;
&lt;p&gt;Having stated all this principles, our main goal remains unchanged, compute the color received by the eye (camera) for each pixel of the output image displayed on our screen. More specifically, we are interested in the color and intensity of light that either gets directly reflected from the surface to the eye or that gets refracted and then re-emitted by the object through diffusion (considering that all the light that gets absorbed is lost). On the other hand, in this tutorial we neglect the effect of scattering which gives more realistic results (especially useful when rendering skin) but is more costly to compute.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/normalsurface.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/normalsurface.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;!-- &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/normalsurface.png&#34; alt=&#34;NormalSurf&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;p&gt;The amount of light that gets into a specific direction from a given point on an object surface is governed by laws of physics and more specifically it is given by the reflectance equation:&lt;/p&gt;
&lt;p&gt;$$ L_o(p,v) = \int_A f_r(p,l,v,\alpha_p) L_i(p,l)(n \cdot l)dl $$&lt;/p&gt;
&lt;p&gt;Where $p$ is the point of interest on the object surface (receiving light), $v$ the view direction from $p$ to the eye, $l$ the incident light direction, $\alpha_p$ the surface roughness at point $p$, $L_o$ the output light radiance (stored as a RGB color) perceived by our eye from $p$, $L_i$ the incident light radiance (also stored as a RGB color) gathering at $p$ from direction l, $f_r$ a function controlling the amount of light reflected to direction $v$ with respect to the material property at $p$ and $A$ the hemisphere surrounding $p$ on which we integrate all incoming light directions.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/AreaIntegrate.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/AreaIntegrate.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;!-- &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/AreaIntegrate.png&#34; alt=&#34;AreaInt&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;p&gt;In this tutorial, we further restrict incoming light sources to a given number of point light sources. Thus, the integral over $A$ can be transformed into a sum over the different light sources.&lt;/p&gt;
&lt;p&gt;$$ L_o(p,v) = \sum_l f_r(p,p-c_l,v,\alpha_p) L_i(p,p-c_l)( n \cdot \frac{(c_l-p)}{\left\lVert   c_l - p \right\rVert})  $$&lt;/p&gt;
&lt;p&gt;The incoming reflectance $L_i(p,p-c_l)$ equals to the intensity of the light source $I$, multiplied by its color $C$, and weighted by an attenuation factor which depends on the distance to the source.
In our case, we will consider that the intensity of light decrease with the square distance to the source:&lt;/p&gt;
&lt;p&gt;$$L_i(p,p-c_l) = \frac{IC}{{\left\lVert p - c_l\right\rVert}^2} $$&lt;/p&gt;
&lt;h2 id=&#34;the-bidirectional-reflective-distribution-function-brdf-&#34;&gt;The Bidirectional Reflective Distribution Function (BRDF)&lt;/h2&gt;
&lt;p&gt;The only unknown left is the $f_r$ function that controls the amount of reflected light with respect to materials properties. This function is called a BRDF which stands for Bidirectional Reflective Distribution Function. Several functions were proposed to simulate real-life materials behavior, all of them respect the energy conservation law, meaning that the amount of outgoing light do not exceed the amount of incoming light and above all the later is divided between reflected and refracted light. Another important property of the BRDF functions is that there are intresectly symmetric with respect to incoming and outgoing light because of the principle of reversibility of light.
In our case, we will use the Cook-Torrance BRDF model composed of a diffuse and a specular part:&lt;/p&gt;
&lt;p&gt;$$f_r = k_d f_l + k_s f_c$$&lt;/p&gt;
&lt;p&gt;where $k_d$ is the amount of refracted light that gets re-emitted and $k_s$ the amount of reflected light with:&lt;/p&gt;
&lt;p&gt;$$ k_d = 1 - k_s $$&lt;/p&gt;
&lt;p&gt;The $f_{l}$ function is the Lambertian diffusion distribution (which corresponds to the diffuse part of the Phong model). It considers that the diffused light is equally spread on all direcion:
$$ f_l = \frac{C}{\pi} $$
Where C is the albedo of the object surface at point $p$. We can notice that the dot product between the normal and the light direction is done outside this function and is still present in the sum of in going contribution. $\pi$ is a normalization factor which accounts for the fact that we integrate ingoing light over the hemisphere at point $p$.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/diffuse.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/diffuse.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;!--  &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/diffuse.png&#34; alt=&#34;Diffuse&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;p&gt;At this point it is important to mention that we must differentiate between two type of material: &lt;strong&gt;metals&lt;/strong&gt; and &lt;strong&gt;dielectric&lt;/strong&gt; (non metal) materials.
Indeed, while dielectric materials diffuse light, it is not the case of metals that absorb all refracted light. As a consequence, $k_d = 0$ for metals (with $k_s \leq 1$ because light still get refracted).&lt;/p&gt;
&lt;p&gt;On the other side the $f_c$ is composed of two terms:&lt;/p&gt;
&lt;p&gt;$$f_c = \frac{DG}{4(l \cdot n)(v \cdot n)}$$&lt;/p&gt;
&lt;p&gt;with D called the Normal Distribution Function and G the Geometry function. Additionally, $k_s = F$, where $F$ is the Fresnel term describing the amount of light that gets refracted on a more macroscopic scale with respect to the view direction. The Fresnel term results from an equation that is not easy to solve, however it can be approximated using the Fresnel-Schlick approximation:&lt;/p&gt;
&lt;p&gt;$$ F_{Schlick}(h, v, F_0) = F_0 + (1 - F_0) (1 - (h \cdot v))^5 $$&lt;/p&gt;
&lt;p&gt;With $F_0$ being the base reflectivity of the material and $h = \frac{l + v}{\left\lVert l + v \right\rVert}$ the half vector which corresponds to the normal one facet must have to directly reflect the light into the eye.&lt;/p&gt;
&lt;p&gt;This equation tells us that when $h$ and $v$ are perpendicular the amount of reflected light is at its maximum, in other words reflections occurs more at grazing angles. This effect is especially noticeable on puddles or wooden surfaces when looking from a top view or from a grazing angle.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/Fresnel_Photo.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/Fresnel_Photo.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;!-- &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/Fresnel_Photo.png&#34; alt=&#34;FresnelPhoto&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;p&gt;$F_0$ is computed as the amount of reflected light at normal incidence where $v$ and $l$ are collinear.
It is important to note that this equation can only be applied to dielectric materials, especially because metallic materials absorb all refracted light. However, as $F_0$ for dielectric materials is usually low and  high for metallic materials, a common approximation is to use a common average $F_0$ for dielectic materials and the metal color as $F_0$ for metallic materials. This is plausible because metallic $F_0$ are tinted and give to metals their color. Furthermore, following the metallic workflow we will consider that being metallic or dielectric is not a binary feature, meaning that one material can be sem-metallic with its metalness varying from $0$ to $1$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt;  One might notice that $h$ is replaced by $n$ in the original equation. This is perfectly right in a macroscopic point of view. However, in our case we look at reflections in a microscopic scale, meaning that the normal of the surface is determined by microfacet normals. Additionally the only case where the light is reflected into our eye is when $n = h$ which justify our use of h in this case. This property is further used to approximate the BRDF final expression.&lt;/p&gt;
&lt;/blockquote&gt;





  











&lt;figure id=&#34;figure-fresnel-coefficient-length-computed-on-our-test-scene-see-below-the-final-amount-has-been-remapped-for-visualisation-purpose&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/Fresnel_Vis.png&#34; data-caption=&#34;Fresnel coefficient length computed on our test scene (see below). The final amount has been remapped for visualisation purpose.&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/Fresnel_Vis.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fresnel coefficient length computed on our test scene (see below). The final amount has been remapped for visualisation purpose.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!--  &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/Fresnel_Vis.png&#34; alt=&#34;FresnelEg&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Fresnel coefficient length computed on our test scene (see below). The final amount has been remapped for visualisation purpose.&lt;/div&gt; --&gt;
&lt;p&gt;Let us describe the microfacet model in more details and focus on the roughness parameter that plays a role in the amount of reflected light. More concretely, this parameter describes the amount of micro-facet that are aligned in the same direction; in particular, rough surfaces have a chaotic orientation distribution while smooth surfaces facets are oriented in a single direction (the normal).&lt;/p&gt;
&lt;p&gt;As microfacets represent the surface of the object, their orientation directly affects the direction of reflected light.
That&amp;rsquo;s why smooth surfaces typically behave like mirrors while reflections appear blurrier on rough surfaces.&lt;/p&gt;
&lt;p&gt;We can now describe the role of the Normal Distribution Function D and the Geometric function G.
D represent the amount of microfacet that are aligned with the half vector $h$. This is the same as computing the amount of reflected light rays that are collinear to the view vector $v$.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/ndf_halfvector.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/ndf_halfvector.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;!-- &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/ndf_halfvector.png&#34; alt=&#34;NDFHalfVector&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;p&gt;In our case, we chose the GGX distribution function as NDF:&lt;/p&gt;
&lt;p&gt;$$D = NDF_{GGX TR}(n, h, \alpha) = \frac{\alpha^2}{\pi((n \cdot h)^2 (\alpha^2 - 1) + 1)^2}$$&lt;/p&gt;





  











&lt;figure id=&#34;figure-ggx-normal-distribution-function-computed-on-our-test-scene-with-varying-roughness-left-rough-surface-right-smooth-surface&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/NDF_Vis.png&#34; data-caption=&#34;GGX Normal Distribution Function computed on our test scene with varying roughness (left: rough surface. right: smooth surface)&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/NDF_Vis.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    GGX Normal Distribution Function computed on our test scene with varying roughness (left: rough surface. right: smooth surface)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/NDF_Vis.png&#34; alt=&#34;NDFEg&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;GGX Normal Distribution Function computed on our test scene with varying roughness (left: rough surface. right: smooth surface) &lt;/div&gt; --&gt;
&lt;p&gt;This function behaves like a diract when $\alpha = 0$ (smooth surface) and becomes flatter and flatter when $\alpha$ increases until it reaches a constant function $\frac{1}{\pi}$. That is why this formula produces small rounded highlight on smooth surfaces which are completely spread across the object on rough surfaces.&lt;/p&gt;
&lt;p&gt;The Geometric function simulates two phenomena that occurs between micro-facets namely obstruction and shadowing. In the two cases, either the incoming light cannot reach some micro-facets because there are in the shadows of others (shadowing) or reflected light is blocked by other facets (obstruction).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Shadowing&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Masking&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/shadowing.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/masking.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Therefore, some amount of reflected light is &amp;ldquo;lost&amp;rdquo; and this is exactly the information given by the Geometric function. In our case we chose the Schlick-GGX approximation:&lt;/p&gt;
&lt;p&gt;$$    G_S(n, v, k) =  \frac{n \cdot v}{(n \cdot v)(1 - k) + k }  $$&lt;/p&gt;
&lt;p&gt;Taking into account the two effects:&lt;/p&gt;
&lt;p&gt;$$   G(n,v,l,k) =  G_S(n, v, k)  G_S(n, l, k) $$&lt;/p&gt;
&lt;p&gt;This function intuitively models the fact that at grazing angle with respect to the normal, either incoming light rays of reflected rays have a chance to collide with other facets. This probability is gets higher when the roughness of the surface increases, which is pretty intuitive because the micro-facet gets more chaotic and do not face a single direction.&lt;/p&gt;





  











&lt;figure id=&#34;figure-geometry-function-computed-on-our-test-scene-left-rough-surface-right-smooth-surface&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/Geometry_Vis.png&#34; data-caption=&#34;Geometry function computed on our test scene (left: rough surface. right: smooth surface).&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/Geometry_Vis.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Geometry function computed on our test scene (left: rough surface. right: smooth surface).
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!--  &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/Geometry_Vis.png&#34; alt=&#34;GeometryEg&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Geometry function computed on our test scene (left: rough surface. right: smooth surface).&lt;/div&gt; --&gt;
&lt;p&gt;In this tutorial we made choices for approximation functions but several others can be found in the literature (see &lt;strong&gt;References&lt;/strong&gt; at the bottom of the page), I invite you to take a look at BRDF comparison articles.&lt;/p&gt;
&lt;h2 id=&#34;coding-time-&#34;&gt;Coding Time&lt;/h2&gt;
&lt;p&gt;We are now ready to dive into the code. The base code can be found 
&lt;a href=&#34;http://typhomnt.github.io/Files/MSIAM_Code.zip&#34;&gt;here&lt;/a&gt;.
First, lets go back to our main function. We are working inside a fragment shader displaying a simple quad perfectly fitting our window. For each pixel we cast rays from a virtual camera to the current pixel whose screen coordinates are given by the in variable $fragCoord$.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;in vec2 fragCoord;
void main()
{   
    Ray ray = generatePerspectiveRay(fragCoord);
    outColor = vec4(trace(ray),1);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside the fragment shader, I passed the view matrix $V$ of the trackball that is available from the transform.py file and used it to move the camera.
Notice the inverse operator applied on V as we want to recover the position of the camera in world space.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;uniform mat4 V;
Ray generatePerspectiveRay(in vec2 p)
{
    // p is the current pixel coord, in [-1,1]
    float fov = 30; // Half angle
    float D = 1./tan(radians(fov));
    mat4 inv_view = inverse(V); // Get the matrix of the trackball

    vec3 up = vec3(0,1,0);
    vec3 front = vec3(0,0,-1);
    vec3 right = cross(up,front);
    return  Ray((inv_view*vec4(0,0,-D,1)).xyz,mat3(inv_view)*normalize(p.x*right + p.y*up*aspectRatio + D*front));
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we declare two new structures: the HitSurface structure containing a ray-scene intersection point, the surface normal at this point as well as its material properties expressed as a PBRMat.
As mentioned above these materials contain roughness and metallic properties. We also add an ambient occlusion property which is used when computing an ambient lighting term in the rendering loop.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;struct PBRMat
{
    vec3 color;
    float roughness;
    float metallic;
    float ao;
};

struct HitSurface
{
    vec3 hit_point;
    vec3 normal;
    PBRMat material;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Back to the trace function. We declare $accum$ as the color of the pixel that is incremented at each ray bounce, the $mask$ variable indicates the intensity of the current ray which gradually decrease at each bounce. For each step we compute the intersection between the ray and the objects in the scene and store the nearest intersected object (with a positive distance) in the $io$ variable.&lt;/p&gt;
&lt;p&gt;The material used for the intersected object is chosen with respect to its index which is used to sample from an array of materials. We then compute the normal and the local illumination of the object
at the intersection point inside the directIllumination function. The reflection intensity factor $c_{refl}$ is updated inside this function. Note, that the reflected ray origin is slightly shifted in the normal direction to avoid  wrong self intersection. This tip is also applied when computing the shadow ray.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vec3 trace(in Ray r)
{
    vec3 accum = vec3(0.0f);
    vec3 mask = vec3(1.0f);
    int nb_refl = 2; // Bounce number
    float c_refl = 1.0f;
    Ray curr_ray = r;
    for(int i = 0 ; i &amp;lt;= nb_refl ; i++)
    {
        ISObj io = intersectObjects(curr_ray);
        if(io.t &amp;gt;= 0)
        {
            PBRMat mat = pbr_mat[io.i];

            HitSurface hs = HitSurface(curr_ray.ro + io.d*curr_ray.rd, computeNormal(io,curr_ray),mat);               

            vec3 color = directIllumination(hs,curr_ray,c_refl);
            accum = accum + mask * color;
            mask = mask*c_refl;
            curr_ray  = Ray(hs.hit_point + 0.001*hs.normal,reflect(curr_ray.rd,hs.normal));
        }
        else
        {
            break;
        }
    }

    return accum;

}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside the directIllumination function, we iterate through each light of the scene and determine if the current intersection point is in the shadow of an object (which can be it self) by casting a ray in the direction of the light. If the intersection point is actually lighted, we compute its radiance inside the PBR function. Otherwise we assign an ambient color weighted by the ambient occlusion factor of the material. Finally we update the reflection intensity factor by computing the length of th Fresnel term.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vec3 directIllumination(in HitSurface hit,in Ray r,inout float refl)
{

    vec3 color = vec3(0);
    for(int i = 0 ; i &amp;lt; light_nbr ; i++)
    {
        Ray l_ray = lightRay(hit.hit_point,lights[i]);
        l_ray.ro = hit.hit_point + 0.001*hit.normal;
        ISObj io;
        io = intersectObjects(l_ray);
        float d_light = lightDist(hit.hit_point,lights[i]);

        if(io.t &amp;lt; 0 || (io.t &amp;gt;= 0 &amp;amp;&amp;amp; (io.d &amp;gt;= d_light)))
        {
            color += PBR(hit,r,lights[i]);
        }
        else
        {
            color +=  vec3(0.03) * hit.material.color * hit.material.ao;
        }


        vec3 Ve = normalize(r.ro - hit.hit_point);
        vec3 H = normalize(Ve + l_ray.rd);
        refl = length(fresnelSchlick(max(dot(H, Ve), 0.0),  mix(vec3(0.04), hit.material.color, hit.material.metallic)))*hit.material.ao;
    }

    return color;
}

struct Light 
{
    int type; // 0 dir light, 1 point light
    vec3 dir; // directionnal light
    vec3 center; // point light
    float intensity; // 1 default
    vec3 color; // light color
};

Ray lightRay(in vec3 ro, in Light l) //computes ro to light source ray
{
    if(l.type == 0)
        return Ray(ro,normalize(l.dir));
    else if(l.type == 1)
        return Ray(ro,normalize(l.center - ro));

    return Ray(ro,vec3(1));
 }

float lightDist(in vec3 ro, in Light l) //computes distance to light
{ 
    if(l.type == 0)
         return DIST_MAX;
    else if(l.type == 1)
        return length(l.center - ro);

    return DIST_MAX;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, it is time to actually implement the PBR direct illumination function approximating the light equation.
This is done in two steps. We first compute the ambient term and the material $F_0$. We then compute the normal of the surface at the intersection point, the view vector, the half vector, the light intensity and direction and pass them to the computeReflectance function inside of which we compute the actual output radiance.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vec3 PBR(in HitSurface hit, in Ray r , in Light l)
{
    vec3 ambient = vec3(0.03) * hit.material.color * (1.0 - hit.material.ao);
    //Average F0 for dielectric materials
    vec3 F0 = vec3(0.04);
    // Get Proper F0 if material is not dielectric
    F0 = mix(F0, hit.material.color, hit.material.metallic);
    vec3 N = normalize(hit.normal);
    vec3 Ve = normalize(r.ro - hit.hit_point);

    float intensity = 1.0f;
    if(l.type == 1)
    {
        float l_dist = lightDist(hit.hit_point,l);
        intensity = l.intensity/(l_dist*l_dist);
    }
    vec3 l_dir = lightRay(hit.hit_point,l).rd;
    vec3 H = normalize(Ve + l_dir);
    return ambient + computeReflectance(N,Ve,F0,hit.material.color,l_dir,H,l.color,intensity,hit.material.metallic,hit.material.roughness);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then define all the BRDF functions we mentionned above:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;float DistributionGGX(vec3 N, vec3 H, float roughness)
{
    float a      = roughness*roughness;
    float a2     = a*a;
    float NdotH  = max(dot(N, H), 0.0);
    float NdotH2 = NdotH*NdotH;

    float nom   = a2;
    float denom = (NdotH2 * (a2 - 1.0) + 1.0);
    denom = PI * denom * denom;

    return nom / denom;
}

float GeometrySchlickGGX(float NdotV, float roughness)
{
    float r = (roughness + 1.0);
    float k = (r*r) / 8.0;

    float nom   = NdotV;
    float denom = NdotV * (1.0 - k) + k;

    return nom / denom;
}

float GeometrySmith(vec3 N, vec3 V, vec3 L, float roughness)
{
    float NdotV = max(dot(N, V), 0.0);
    float NdotL = max(dot(N, L), 0.0);
    float ggx2  = GeometrySchlickGGX(NdotV, roughness);
    float ggx1  = GeometrySchlickGGX(NdotL, roughness);

    return ggx1 * ggx2;
}

vec3 fresnelSchlick(float cosTheta, vec3 F0)
{
    return F0 + (1.0 - F0)*pow((1.0 + 0.000001/*avoid negative approximation when cosTheta = 1*/) - cosTheta, 5.0);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we compute the outgoing radiance as the product of the incoming radiance, the BRDF terms and the dot product between the normal and the light direction.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vec3 computeReflectance(vec3 N, vec3 Ve, vec3 F0, vec3 albedo, vec3 L, vec3 H, vec3 light_col, float intensity, float metallic, float roughness)
{
    vec3 radiance =  light_col * intensity; //Incoming Radiance

    // cook-torrance brdf
    float NDF = DistributionGGX(N, H, roughness);
    float G   = GeometrySmith(N, Ve, L,roughness);
    vec3 F    = fresnelSchlick(max(dot(H, Ve), 0.0), F0);

    vec3 kS = F;
    vec3 kD = vec3(1.0) - kS;
    kD *= 1.0 - metallic;

    vec3 nominator    = NDF * G * F;
    float denominator = 4 * max(dot(N, Ve), 0.0) * max(dot(N, L), 0.0) + 0.00001/* avoid divide by zero*/;
    vec3 specular     = nominator / denominator;


    // add to outgoing radiance Lo
    float NdotL = max(dot(N, L), 0.0);
    vec3 diffuse_radiance = kD * (albedo)/ PI;

    return (diffuse_radiance + specular) * radiance * NdotL;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it, we set up everything to compute a more realistic shading of our scene, you can already produce new raytraced results using this setup.&lt;/p&gt;
&lt;p&gt;Below, are rendering examples of the same scene with varying materials. It contains a single white point Light of intensity $I = 40$ and located at (0,0,0). Left to right spheres roughness are 1, 0.9, 0.7, 0.5, 0.3, 0.1 with (.1,.2,.8) as color and are located at (2.5,0,-2), (1.5,0,-2), (0.5,0,-2), (-0.5,0,-2), (-1.5,0,-2) and (-2.5,0,-2) with a radius of 0.3.&lt;/p&gt;





  











&lt;figure id=&#34;figure-dielectric-materials-without-reflection-metalness--0&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/dielec_no_refl.png&#34; data-caption=&#34;Dielectric materials without reflection (metalness = 0)&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/dielec_no_refl.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Dielectric materials without reflection (metalness = 0)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/dielec_no_refl.png&#34; alt=&#34;DielectNoRefl&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Dielectric materials without reflection (metalness = 0)&lt;/div&gt; --&gt;





  











&lt;figure id=&#34;figure-dielectric-materials-with-reflection-metalness--0&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/dielec_refl.png&#34; data-caption=&#34;Dielectric materials with reflection (metalness = 0)&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/dielec_refl.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Dielectric materials with reflection (metalness = 0)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/dielec_refl.png&#34; alt=&#34;DielectRefl&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Dielectric materials with reflection (metalness = 0)&lt;/div&gt; --&gt;





  











&lt;figure id=&#34;figure-metallic-materials-without-reflection-metalness--1&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/metal_no_refl.png&#34; data-caption=&#34;Metallic materials without reflection (metalness = 1)&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/metal_no_refl.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Metallic materials without reflection (metalness = 1)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/metal_no_refl.png&#34; alt=&#34;MetalNoRefl&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Metallic materials without reflection (metalness = 1)&lt;/div&gt; --&gt;





  











&lt;figure id=&#34;figure-metallic-material-with-reflection-metalness--1&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/metal_refl.png&#34; data-caption=&#34;Metallic material with reflection (metalness = 1)&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/metal_refl.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Metallic material with reflection (metalness = 1)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/metal_refl.png&#34; alt=&#34;MetalRefl&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Metallic material with reflection (metalness = 1)&lt;/div&gt; --&gt;
&lt;h2 id=&#34;one-last-step-hdr-and-gamma-correction-&#34;&gt;One last step: HDR and Gamma correction&lt;/h2&gt;
&lt;p&gt;So far we are able to render scenes in a more realistic way. Still, you can notice that if you choose a high intensity for your lights, the results also gets highly saturated, which is already the case in the above examples. This situation is to be expected because radiance can have value bigger than 1. Consequently, we need to find a way to account for High Dynamic Range (HDR) of radiance and process the computed values such that everything is remapped between 0 and 1. I won&amp;rsquo;t enter into details here as it is not the subject of this tutorial. We basically use the Reinhard operator to correct our input colors, remapping them inside the $[0;1]^3$ space with the function $C = \frac{C}{C+1}$.&lt;/p&gt;
&lt;p&gt;Finally, we also need to take into account the color shift that screens produce in response to color values and depending on what we call their gamma value defining this response. Hence, we proceed to a gamma correction of the output colors which rectifies the shift. Again this will be covered in another tutorial, however be careful that this gamma value depends on the monitor configuration you are using.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vec3 trace()
{
    vec3 accum = vec3(0.0);
    ...

    //HDR
    accum = accum / (accum+ vec3(1.0));
    //Gamma
    float gamma = 2.2;
    accum = pow(accum, vec3(1.0/gamma));
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the corrected images of the previous examples with different gamma values.&lt;/p&gt;





  











&lt;figure id=&#34;figure-dielectric-materials-with-reflection-metalness--0-hdr-and-gamma--1&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/dielec_HDR_GAMMA_104.png&#34; data-caption=&#34;Dielectric materials with reflection (metalness = 0) HDR and Gamma = 1&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/dielec_HDR_GAMMA_104.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Dielectric materials with reflection (metalness = 0) HDR and Gamma = 1
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/dielec_HDR_GAMMA_104.png&#34; alt=&#34;MetalRefl&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Dielectric materials with reflection (metalness = 0) HDR and Gamma = 1&lt;/div&gt; --&gt;





  











&lt;figure id=&#34;figure-dielectric-materials-with-reflection-metalness--0-hdr-and-gamma--22&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/dielec_HDR_GAMMA_2.png&#34; data-caption=&#34;Dielectric materials with reflection (metalness = 0) HDR and Gamma = 2.2&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/dielec_HDR_GAMMA_2.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Dielectric materials with reflection (metalness = 0) HDR and Gamma = 2.2
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/dielec_HDR_GAMMA_2.png&#34; alt=&#34;MetalRefl&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Dielectric materials with reflection (metalness = 0) HDR and Gamma = 2.2&lt;/div&gt; --&gt;





  











&lt;figure id=&#34;figure-metallic-material-with-reflection-metalness--1-hdr-and-gamma--1&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/metal_HDR_GAMMA_104.png&#34; data-caption=&#34;Metallic material with reflection (metalness = 1) HDR and Gamma = 1&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/metal_HDR_GAMMA_104.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Metallic material with reflection (metalness = 1) HDR and Gamma = 1
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/metal_HDR_GAMMA_104.png&#34; alt=&#34;MetalRefl&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Metallic material with reflection (metalness = 1) HDR and Gamma = 1&lt;/div&gt; --&gt;





  











&lt;figure id=&#34;figure-metallic-material-with-reflection-metalness--1-hdr-and-gamma--22&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/metal_HDR_GAMMA_2.png&#34; data-caption=&#34;Metallic material with reflection (metalness = 1) HDR and Gamma = 2.2&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/metal_HDR_GAMMA_2.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Metallic material with reflection (metalness = 1) HDR and Gamma = 2.2
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/metal_HDR_GAMMA_2.png&#34; alt=&#34;MetalRefl&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Metallic material with reflection (metalness = 1) HDR and Gamma = 2.2&lt;/div&gt; --&gt;
&lt;p&gt;You can also play with the different parameters and get various materials:&lt;/p&gt;





  











&lt;figure id=&#34;figure-various-materials-gamma--1&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/Final_Eg.png&#34; data-caption=&#34;Various materials, Gamma = 1&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/Final_Eg.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Various materials, Gamma = 1
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/Final_Eg.png&#34; alt=&#34;FinalEg&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Various materials, Gamma = 1&lt;/div&gt; --&gt;
&lt;h2 id=&#34;and-then--whats-next--&#34;&gt;And then ? What&amp;rsquo;s Next ?&lt;/h2&gt;
&lt;p&gt;Rendering is a huge topic and we covered a very small part of it. What is next entirely depends on which direction you want to pursue this journey; you could go into path tracing and ignore real-time approximations or on the contrary go for more real-time rendering effects like Image Based Lighting (IBL), Screen Space Ambient Occlusion (SSAO), Screen Space Reflections (SSR) and so on.&lt;/p&gt;
&lt;p&gt;However, There is one aspect that should be improved in our current raytracing context which is the way we handle reflections. Indeed, currently in our ray-tracing context we cast only one reflected ray from intersection point. While it is accurate to do so for smooth surfaces, it is totally wrong for rough surfaces as we should cast several rays from the each intersection point and average the results in order to get the blurred reflections like I mentioned at the beginning of the tutorial. This is something you could try to implement in a naive manner and see how reflections are improved.
In addition, one can also questions the fact the presence of an ambient lighting term in this energy conservative context and he/she is right, this term is a complete hack to cope with the lack of indirect lighting computation. For real-time applications IBL can be used to better approximate this kind of lighting contribution while path tracing implicitly compute this contribution.&lt;/p&gt;
&lt;p&gt;Finally, I want you to be aware that BRDF models are generalizable and can take into account more light dimensionality, in particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BTDF: Bidirectionnal Transmission Distribution Function 5D: Same as BRDF for opposite side of surface&lt;/li&gt;
&lt;li&gt;SVBRDF: Spatially Varying BRDF 6D: changes over the surface position&lt;/li&gt;
&lt;li&gt;BSSRDF: Bidirectionnal Surface Scattering DF 8D: light exits at another location&lt;/li&gt;
&lt;li&gt;BSDF: Bidirectionnal Scattering Distribution Function XD: General formulation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references-&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Open GL Tutorials: 
&lt;a href=&#34;https://learnopengl.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://learnopengl.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Siggraph Courses: 
&lt;a href=&#34;http://blog.selfshadow.com/publications/s2013-shading-course/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://blog.selfshadow.com/publications/s2013-shading-course/&lt;/a&gt; 
&lt;a href=&#34;http://blog.selfshadow.com/publications/s2014-shading-course/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://blog.selfshadow.com/publications/s2014-shading-course/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Real Time Rendering Advances: 
&lt;a href=&#34;http://advances.realtimerendering.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://advances.realtimerendering.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;BRDF Model Comparison: 
&lt;a href=&#34;https://diglib.eg.org/handle/10.2312/EGWR.EGSR05.117-126&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://diglib.eg.org/handle/10.2312/EGWR.EGSR05.117-126&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Path tracing and Global Illumination: 
&lt;a href=&#34;http://www.graphics.stanford.edu/courses/cs348b-01/course29.hanrahan.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.graphics.stanford.edu/courses/cs348b-01/course29.hanrahan.pdf&lt;/a&gt; 
&lt;a href=&#34;http://web.cs.wpi.edu/~emmanuel/courses/cs563/write_ups/zackw/realistic_raytracing.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://web.cs.wpi.edu/~emmanuel/courses/cs563/write_ups/zackw/realistic_raytracing.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GLSL / Shadertoy: 
&lt;a href=&#34;https://www.opengl.org/documentation/glsl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.opengl.org/documentation/glsl/&lt;/a&gt; 
&lt;a href=&#34;https://www.shadertoy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.shadertoy.com/&lt;/a&gt; 
&lt;a href=&#34;http://www.iquilezles.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.iquilezles.org/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to Raymarching</title>
      <link>http://typhomnt.github.io/teaching/ray_tracing/raymarching_intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://typhomnt.github.io/teaching/ray_tracing/raymarching_intro/</guid>
      <description>&lt;!--
&lt;script type=&#34;text/x-mathjax-config&#34;&gt;
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],
    displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\[&#39;,&#39;\]&#39;]],
    processEscapes: true,
    processEnvironments: true,
    skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;],
    TeX: { equationNumbers: { autoNumber: &#34;AMS&#34; },
         extensions: [&#34;AMSmath.js&#34;, &#34;AMSsymbols.js&#34;] }
  }
});
&lt;/script&gt;

&lt;script type=&#34;text/x-mathjax-config&#34;&gt;
  MathJax.Hub.Queue(function() {
    // Fix &lt;code&gt; tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i &lt; all.length; i += 1) {
        all[i].SourceElement().parentNode.className += &#39; has-jax&#39;;
    }
});
&lt;/script&gt;


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
--&gt;
&lt;p&gt;This tutorial is part of the ray-tracing course available 
&lt;a href=&#34;../raytracing_practs&#34;&gt;here&lt;/a&gt;. /!\ This tutorial is using parts of the 
&lt;a href=&#34;../pbr_intro&#34;&gt;PBR tutorial&lt;/a&gt; which should be completed before starting this one.&lt;/p&gt;
&lt;h2 id=&#34;introduction-&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this tutorial, I will introduce another well known rendering approach which is very similar to ray tracing but operates in a slightly different way, especially as it treats surfaces as distance fields. Ray tracing is based on the principle that we can compute analytically ray-surface intersections, being with triangles or more complex surfaces. However, this can also be a limitation as we are bound to display only surfaces for which we are able to compute ray-surface intersection. Thus, Ray Marching comes into place and allows us to display any surface defined by an expression $f(x,y,z) = 0$ such that $\left\lVert \nabla f \right\rVert = 1$, in other words a distance field. More precisely, in our context the $f$ function can also represent negative distances. It is referred as a Signed Distance Function (SDF) which means that when $f(p) &amp;gt; 0$, $p = (x,y,z)$ lies outside the surface and when $ f(p) &amp;lt; 0 $ it lies inside the surface. Note the D for Distance in SDF which imposes the condition $\left\lVert \nabla f \right\rVert = 1$.&lt;/p&gt;
&lt;h2 id=&#34;main-principle-&#34;&gt;Main Principle&lt;/h2&gt;
&lt;p&gt;The main principle of Ray Marching remains similar to Ray Tracing: for each pixel of the screen, we cast a ray spreading from the camera center to the pixel, however instead of computing ray surface intersections by solving an equation, we iterate through the generated ray step by step and check if we intersect a surface at each step by evaluating the scene SDF at the current location. More precisely, for a given ray $r(t) = \vec{d}t + c_{camera}$ and a given surface, we compute the SDF of the surface $f(r(t))$ and check if it equals 0. If not, we increase $t$ by a given amount $\delta_t$. Note that $t = 0 $ at the beginning of this process and that  $f(r(0))$ should be positive. In practice $f(r(t)) = 0$ never occurs in our programming world, instead we will check if $f(r(t)) \leq 0$ meaning that we went through the surface.&lt;/p&gt;





  











&lt;figure id=&#34;figure-representation-of-a-scene-sdf&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/RayMarch.png&#34; data-caption=&#34;Representation of a scene SDF&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/RayMarch.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Representation of a scene SDF
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Note that each surface like a Sphere or a Plane defines its own SDF that represents the closest distance from any given 3D point $p$ to the surface. Ray Marching also gives us the opportunity to combine each SDF with different operators in order to build multiple scenes with the same surfaces. The basic operation being the union of a SDF : $$SDF_{scene} (p) =  \bigcup SDF (p) = \min\limits_{f \in SDF}(f(p))$$ Several other operators are commonly used like the intersection, the substraction or the smooth union.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Union&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Intersection&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Substraction&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Smooth Union&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;




  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/March_Union.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/March_Union.png&#34; alt=&#34;&#34; width=&#34;300px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;




  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/March_Intersect.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/March_Intersect.png&#34; alt=&#34;&#34; width=&#34;250px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;




  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/March_Substract.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/March_Substract.png&#34; alt=&#34;&#34; width=&#34;245px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;




  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/March_Blend.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/March_Blend.png&#34; alt=&#34;&#34; width=&#34;280px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;marching-&#34;&gt;Marching&lt;/h2&gt;
&lt;p&gt;There are different ways of choosing the value of $\delta_t$ while going through a light ray: one is to increase $t$ by a constant small value until we intersect one surface. Another one is to compute the minimal distance we can travel without intersecting any surface by taking the minimum of all $f(r(t))$. In practice, this minimal distance is equal to $SDF_{scene}(r(t))$. This second approach is called Sphere (or Spherical) Marching and will be the approach we will use in this tutorial.&lt;/p&gt;
&lt;p&gt;




  











&lt;figure id=&#34;figure-step-by-step-marching&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/RayMarchSteps.png&#34; data-caption=&#34;Step by Step Marching&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/RayMarchSteps.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Step by Step Marching
  &lt;/figcaption&gt;


&lt;/figure&gt;






  











&lt;figure id=&#34;figure-spherical-marching&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/RayMarchSphere.png&#34; data-caption=&#34;Spherical Marching&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/RayMarchSphere.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Spherical Marching
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Once we computed the intersection point, we are ready to compute any lighting occuring at this point as well as casting new rays to simulate reflections or refractions.&lt;/p&gt;
&lt;h2 id=&#34;coding-time-&#34;&gt;Coding Time&lt;/h2&gt;
&lt;p&gt;Now that I introduced the theory behind Ray Marching it is time to dig into actual coding. Like in previous ray tracing tutorials, we are working inside a fragment shader displaying a simple quad perfectly fitting our window. For each pixel we cast rays from a virtual camera to the current pixel whose screen coordinates are given by the in variable $fragCoord$.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;void main() 
{   
    Ray ray = generatePerspectiveRay(fragCoord);
    outColor = vec4(march(ray),1);

}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The march function computes  color contribution for the principal the reflected rays similarly to the trace function of previous tutorials. As we are in a PBR context, it also performs tone mapping and gamma correction. The function that actually go through each ray and check intersections with the surfaces in the scene is the rayMarch function. More precisely it returns the minimal $f(r(t))$ among all the scene surfaces. Notice that the normal at the intersection point is computed as the gradient of the surface SDF (see &lt;strong&gt;SDF Gradient&lt;/strong&gt; section).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vec3 march(in Ray r)
{
    vec3 accum = vec3(0.0f);
    vec3 mask = vec3(1.0f);
    int nb_refl = 0;
    float c_refl = 0.3;
    Ray curr_ray = r;
    for(int i = 0 ; i &amp;lt;= nb_refl ; i++)
    {
    ISObj io = rayMarch(curr_ray);
    if(io.t &amp;gt;= 0)
    {
        PBRMat mat = PBRMat(vec3(.9,.1,.1),0.4,0.9,0.3);
        vec3 N = normalize(computeSDFGrad(io,curr_ray.ro + io.d * curr_ray.rd));

        HitSurface hs = HitSurface(curr_ray.ro + io.d*curr_ray.rd
                                   ,N
                                   ,mat
                                   ) ;
        vec3 color = directIllumination(hs,curr_ray,c_refl);
        accum = accum + mask * color;

        mask = mask*c_refl;
        curr_ray  = Ray(hs.hit_point + 0.05*hs.normal,reflect(curr_ray.rd,hs.normal));
    }
    else if(i == 0) //did not intersect anything
    {
        accum = vec3(0.3,0.2,0.8);
    }

    }

    //HDR
    accum = accum / (accum+ vec3(1.0));
    //Gamma
    float gamma = 1.1;
    accum = pow(accum, vec3(1.0/gamma));

    return accum;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The directIllumination functions remains the same as presented in the PBR tutorial, the only change being for the function computing the shadow ray intersection.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vec3 directIllumination(in HitSurface hit,in Ray r,inout float refl)
{

    vec3 color = vec3(0);
    for(int i = 0 ; i &amp;lt; light_nbr ; i++)
    {
    Ray l_ray = lightRay(hit.hit_point,lights[i]);
    l_ray.ro = hit.hit_point + 0.01*hit.normal;
    ISObj io;

    io = rayMarch(l_ray);
	
    float d_light = lightDist(hit.hit_point,lights[i]);

    if(io.t &amp;lt; 0 || (io.t &amp;gt;= 0 &amp;amp;&amp;amp; (io.d &amp;gt;= d_light)))
    {
        color += PBR(hit,r,lights[i]);
    }
    else
    {
        color +=  vec3(0.03) * hit.material.color * hit.material.ao;
    }


    vec3 Ve = normalize(r.ro - hit.hit_point);
    vec3 H = normalize(Ve + l_ray.rd);
    refl = length(fresnelSchlick(max(dot(H, Ve), 0.0),  mix(vec3(0.04), hit.material.color, hit.material.metallic)))*hit.material.ao;
    }

    return color;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets detail the rayMarch function then. We first declare a maximum number of iteration step, defining how many times we can advance in the same ray. Indeed passed this number or a maximum travel distance we consider that nothing is to be intersected by the ray. Elsewise, at step $i$ we define the current point on the ray as $p = r.ro + depth * r.rd$, $depth$ playing the role of our parameter $t$ as explained above. We then compute the scene SDF, at point $p$, composed of one or several primitive surfaces like spheres, planes and boxes. This function returns the distance to the closest surface at point $p$. If this distance is lower than a given epsilon ($march accuracy$) or negative we consider that the current $p$ is the intersection point with the scene, else we continue to iterate over the ray and either increase $depth$ by the closest distance (Spherical Marching) or by a fixed amount.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ISObj rayMarch(in Ray r)
{
    int nb_step = 300;

    float depth = 0.0f;
    float march_accuracy = 0.001;
    float march_step_fact = 0.01;

    for (int i = 0; i &amp;lt; nb_step; i++)
    {
    ISObj io = sceneSDF(r.ro + depth * r.rd);
    if (io.d &amp;lt;= march_accuracy)
    {
        return ISObj(depth,io.t,io.i);
    }

    // Move along the view ray
    //Spherical Marching
    depth += io.d;
    // Step by Step Marching
    //depth += march_step_fact

    if (depth &amp;gt;= DIST_MAX)
    {
        // Gone too far; give up
        return ISObj(DIST_MAX,-1,-1);
    }
    }
    return ISObj(DIST_MAX,-1,-1);
    //return ISObj(depth,last_io.t,last_io.i);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Computing the scene SDF is highly correlated with the way we represent the scene. In the last tutorials, we represented the scene as an union of Spheres, Planes and other surfaces whose intersection with a ray has an analytic solution. As I mentioned earlier, in our context we can combine surfaces using different operators which will change the shape of the represented surface. Thus, we need to add a new wrapper that contains both surfaces and operators that we apply between them. We define a Shape structure containing the type of primitive surface it represents and its index in the corresponding array (of Sphere, Plane,&amp;hellip;). The ShapeOp structure contains one Shape, the operation to apply and the index of the next ShapeOp with whom the operator will be applied. By default we consider that $next = -1$, meaning that no operation will be applied, marking the end of the chained list of operations between primitive surfaces. To be more flexible in the way we build scenes, we allow ourselves to consider as many ShapeOp chain lists as we want. Thus, we define define an array of indices $shop roots$ from which each chained lists of ShapeOp should start.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;struct Shape
{
    int shape_type;
    int shape_id;
};

struct ShapeOp
{
    Shape shape;
    int op_type;
    int next;
};

const int shop_nbr = 1;
ShapeOp shops[shop_nbr] = ShapeOp[](ShapeOp(Shape(1,0),0,-1));

const int shop_root_nbr = 1;
int shop_roots[shop_root_nbr] = int[](0);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a starter, let&amp;rsquo;s consider one simple scene composed of a unique Sphere. Below I put the necessary code to define the three primitives I use in this tutorial. Note that this code should be put before the code section presented just above.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;const int plane_type = 0;
const int sphere_type = 1;
const int box_type = 2;
const int shop_type = 3;

// plane structure
struct Plane 
{
    vec3 n; // normal
    float d; // offset
};

// box structure
struct Box 
{
    vec3 b; // up_right_corner_length
    vec3 c; // center
};


// sphere structure
struct Sphere 
{
    vec3 c; // center
    float r; // radius
};

const int sphere_nbr = 6;

Sphere spheres[sphere_nbr] = Sphere[](Sphere(vec3(0.5,-2.5*cos(time*0.1),0),0.5f)
    ,Sphere(vec3(0.7,-2.5*cos(time*0.2),0),0.5f)
,Sphere(vec3(0.9,-2.5*cos(time*0.3),0),0.5f)
,Sphere(vec3(-0.9,2.5*cos(time*0.4),0),0.5f)
,Sphere(vec3(-0.7,2.5*cos(time*0.5),0),0.5f)
,Sphere(vec3(-0.5,2.5*cos(time*0.6),0),0.5f));

const int box_nbr = 3;
Box boxes[box_nbr] = Box[](Box(vec3(1,1,1),vec3(0,-3,0)),Box(vec3(1,1,1),vec3(0,3,0)),Box(vec3(0.5,0.5,0.5),vec3(0,0,0)));

const int plane_nbr = 6;
Plane planes[plane_nbr] = Plane[](Plane(vec3(0,1,0),5.0f),Plane(vec3(0,-1,0),5.0f),Plane(vec3(0,0,1),5.0f),Plane(vec3(0,0,-1),15.0f)
,Plane(vec3(1,0,0),5.0f),Plane(vec3(-1,0,0),5.0f));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets now compute the scene SDF at the current ray point $p$ within the chained list of surface operators, defining a single sphere. We iterate through each chain list, compute their related SDF by iteratively applying SDF operators and returns the closest computed distance among all chained lists (only one in our case).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ISObj sceneSDF(in vec3 point)
{
    ISObj nearest = ISObj(DIST_MAX,-1,-1);
    for(int j = 0 ; j &amp;lt; shop_root_nbr ;j++)
    {
    float dist = ShapeSDF(shop_roots[j],point);
    if(dist &amp;lt; nearest.d)
        nearest = ISObj(dist,shop_type,shop_roots[j]);
    }
    return nearest;
}

float ShapeSDF(int sh_op_id, vec3 p)
{
    int curr_id = sh_op_id;
    float sdf_f = getBasicSDF(shops[curr_id].shape.shape_type,shops[curr_id].shape.shape_id,p);
    int next_id = shops[sh_op_id].next;

    while(next_id != -1)
    {
    float sdf_i = getBasicSDF(shops[next_id].shape.shape_type,shops[next_id].shape.shape_id,p);
    sdf_f = OpSDF(sdf_f,sdf_i,shops[curr_id].op_type);
    curr_id = next_id;
    next_id = shops[next_id].next;
    }

    return sdf_f;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Eventually, we need to define the SDF of each primitive surfaces in $getBasicSDF$ as well as the surface operators in $OpSDF$.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;float getBasicSDF(int type, int id,in vec3 p)
{
    if(type == plane_type)
    {
        return SDFPlane(planes[id],p);
    }
    else if(type == sphere_type)
    {
    return SDFSphere(spheres[id],p);
    }
    else if(type == box_type)
    {
    return SDFBox(boxes[id],p);
    }

    return 0.0;
}

// signed distance function of sphere 
float SDFSphere(in Sphere s, in vec3 p) 
{
    return length(s.c - p) - s.r;
}

// signed distance function of plane 
float SDFPlane(in Plane pl, in vec3 p) 
{
    return (pl.d + dot(normalize(pl.n),p));
}

// signed distance function of Box 
float SDFBox(in Box b, in vec3 p) 
{
    vec3 d = abs(p - b.c) - (b.b) ;
    return length(max(d,0)) + min(max(max(d.x,d.y),d.z),0);
}


float OpSDF(in float s_sdf, in float e_sdf, int op_t)
{
    if(op_t == 0)
    {
    return opUnion(s_sdf,e_sdf);
    }
    else if(op_t == 1)
    {
    return opSubtraction(s_sdf,e_sdf);
    }
    else if(op_t == 2)
    {
    return opIntersection(s_sdf,e_sdf);
    }
    else if(op_t == 3)
    {
    return opSmoothUnion(s_sdf,e_sdf,1.0f);
    }

    return 0.0;

}

float opUnion( float d1, float d2 ) {  return min(d1,d2); }

float opSubtraction( float d1, float d2 ) { return max(d1,-d2); }

float opIntersection( float d1, float d2 ) { return max(d1,d2); }

float opSmoothUnion( float d1, float d2, float k ) 
{
    float h = clamp( 0.5 + 0.5*(d2-d1)/k, 0.0, 1.0 );
    return mix( d2, d1, h ) - k*h*(1.0-h);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sdf-gradient-&#34;&gt;SDF gradient&lt;/h2&gt;
&lt;p&gt;We are one step away of displaying our first ray marched scene. Indeed, the only unknown left to compute is the normal at the intersection point which is especially used to compute the local illumination. In this context, as we apply SDF operators, we will build new scenes by combining primitive surfaces in different ways. Consequently, we cannot compute an analytical normal at the intersection point. Instead we use finite differences to compute an approximation of this normal which correspond to the gradient of the scene SDF at this point. In the 1D case, approximating such a gradient (derivative) is done as following $f&#39;(x) = \frac{f(x+\epsilon) - f(x - \epsilon)}{2\epsilon}$. As SDFs are 3D functions, this principle is extended to each dimension of the function: $$\nabla f = \frac{(f(p + (\epsilon,0,0)) - f(p - (\epsilon,0,0)), f(p + (0,\epsilon,0)) - f(p - (0,\epsilon,0)), f(p + (0,0,\epsilon)) - f(p - (0,0,\epsilon))) } {2(\epsilon,\epsilon,\epsilon)}$$&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;float getSDF(int type, int id,in vec3 p)
{
    if(type != shop_type)
    {
    return getBasicSDF(type,id,p);
    }
    else
    {
    return ShapeSDF(id,p);
    }
}


#define EPS_GRAD 0.001
vec3 computeSDFGrad(in ISObj is,in vec3 p)
{
    vec3 p_x_p = p + vec3(EPS_GRAD, 0, 0);
    vec3 p_x_m = p - vec3(EPS_GRAD, 0, 0);
    vec3 p_y_p = p + vec3(0, EPS_GRAD, 0);
    vec3 p_y_m = p - vec3(0, EPS_GRAD, 0);
    vec3 p_z_p = p + vec3(0, 0, EPS_GRAD);
    vec3 p_z_m = p - vec3(0, 0, EPS_GRAD);

    float sdf_x_p = getSDF(is.t,is.i,p_x_p);
    float sdf_x_m = getSDF(is.t,is.i,p_x_m);
    float sdf_y_p = getSDF(is.t,is.i,p_y_p);
    float sdf_y_m = getSDF(is.t,is.i,p_y_m);
    float sdf_z_p = getSDF(is.t,is.i,p_z_p);
    float sdf_z_m = getSDF(is.t,is.i,p_z_m);


    return vec3(sdf_x_p - sdf_x_m
            ,sdf_y_p - sdf_y_m
            ,sdf_z_p - sdf_z_m)/(2.*EPS_GRAD);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the following lights in your scene you should now obtain a correctly shaded ray marched sphere.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;const int light_nbr = 2;
Light lights[light_nbr] = Light[](Light(1,vec3(1,1,-1),vec3(-3,1,-3),80,vec3(1))
    ,Light(1,normalize(vec3(-1,1,1)),vec3(3,1,3),80,vec3(1)));
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-ray-marched-sphere&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Sphere.png&#34; data-caption=&#34;Ray Marched Sphere&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Sphere.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Ray Marched Sphere
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;example--march-lava-lamp-&#34;&gt;Example : March Lava Lamp&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s now build a more complex scene using only one ShapeOp chained list. The main idea here is to display a kind of lava lamp using spheres and two boxes.
This can be easily achieved by using the smooth union operator which interpolates between two SDF that are near each other.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;const int shop_nbr = 10;
ShapeOp shops[shop_nbr] = ShapeOp[](ShapeOp(Shape(1,0),3,1)
    ,ShapeOp(Shape(1,1),3,2)
,ShapeOp(Shape(1,2),3,3)
,ShapeOp(Shape(1,3),3,4)
,ShapeOp(Shape(1,4),3,5)
,ShapeOp(Shape(1,5),3,6)
,ShapeOp(Shape(2,0),3,7)
,ShapeOp(Shape(2,1),1,-1)
,ShapeOp(Shape(2,2),1,-1)
,ShapeOp(Shape(0,0),0,-1));
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-lava-lamp&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Lava_Lamp.png&#34; data-caption=&#34;Lava Lamp&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Lava_Lamp.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Lava Lamp
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Lets also put a cube shaped hole inside this lamp using the substract operator.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;const int shop_nbr = 10;
    ShapeOp shops[shop_nbr] = ShapeOp[](ShapeOp(Shape(1,0),3,1)
        ,ShapeOp(Shape(1,1),3,2)
    ,ShapeOp(Shape(1,2),3,3)
    ,ShapeOp(Shape(1,3),3,4)
    ,ShapeOp(Shape(1,4),3,5)
    ,ShapeOp(Shape(1,5),3,6)
    ,ShapeOp(Shape(2,0),3,7)
    ,ShapeOp(Shape(2,1),1,8)
    ,ShapeOp(Shape(2,2),1,-1)
    ,ShapeOp(Shape(0,0),0,-1));
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-lava-lamp-with-a-cube-shaped-hole&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Lava_Lamp_Cube.png&#34; data-caption=&#34;Lava Lamp with a cube shaped hole&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Lava_Lamp_Cube.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Lava Lamp with a cube shaped hole
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Now that you can render more complex scenes and I invite you to test all kind of scene configurations by playing with different primitives and different operators.&lt;/p&gt;
&lt;h2 id=&#34;bonus-effect-ambient-occulsion-&#34;&gt;Bonus Effect: Ambient Occulsion&lt;/h2&gt;
&lt;p&gt;Rendering 3D scenes using ray marching also offer other advantages. Additional rendering effect like Ambient Occlusion can be computed with a reduced cost.
Ambient Occlusion is the equivalent of ambient lighting but for shadowing, it describes which parts of the scene that are likely to not be lighted because of the geometry of its surroundings, preventing light ray to reach those areas. This effect can be simply be noticed at the edges and corners of your room where lights hardly strike those areas.
Ray Marching offers us a simple way to approximate this ambient occlusion term. The main idea is to cast an ambient occlusion ray from the intersection point $p$ in the normal direction $\vec{n}$ and march through this ray step by step with a relatively small $\delta_t$. Then at each step, we compute the scene SDF and compare its value to the distance with respect to the intersection point which is equal to $i \delta_t$, $i$ being the number of the current step. In the case where the intersection point lies on a convex surface like a sphere we expect this distance and the scene SDF to be the same. However, it is far from being true in the general case, meaning that the scene SDF can be lower than $i \delta_t$ at some steps, being nearer to other surfaces.
This simulate the fact that some areas around the intersection point can occlude incoming rays. In practice, we first initialize the occlusion factor at $1$ and compute at each step $\frac{(i \delta_t - SDF_{Scene})^2}{i}$ and subtract this amount to the occlusion factor. Notice the denominator which takes into account that the farther from the intersection point we make this evaluation the less plausible it is as we might look far from the surroundings.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;float AmbientOcclusion(vec3 point, vec3 normal, float step_dist, float step_nbr)
{
    float occlusion = 1.0f;
    while(step_nbr &amp;gt; 0.0)
    {
    occlusion -= pow(step_nbr * step_dist - (sceneSDF( point + normal * step_nbr * step_dist)).d,2) / step_nbr;
    step_nbr--;
    }

    return occlusion;
}


vec3 march(in Ray r)
{
    ...
    for(int i = 0 ; i &amp;lt;= nb_refl ; i++)
    {
        ISObj io = rayMarch(curr_ray);
        if(io.t &amp;gt;= 0)
        {
            ...
            accum = accum + mask * color * pow(AmbientOcclusion(hs.hit_point,hs.normal,0.015,20),40);

            ...
        }
        else if(i == 0) //did not intersect anything
        {
            accum = vec3(0.3,0.2,0.8);
        }

    }
    ...

    return accum;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is important to note that when computing the ambient occlusion term, the total travel distance should remain as short as possible because. Indeed, like mentioned earlier, the farther you travel through this ray, the more artifact you are likely to get because surfaces that are too far away from the intersection point might contribute to the occlusion term while not being part of its surroundings. With the parameters I used, you should have an ambient occlusion terms like depicted below. Note that I amplified its effect using a power function with a relatively high factor.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Ambient_Occlusion.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Ambient_Occlusion.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Below is what you should obtain by multiplying the ambient occlusion to the output color so that it plays a shadowing role.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Lava_Lamp_AO.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Lava_Lamp_AO.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Finally, we can also re-enable relections in the main loop and get the following result.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Final_Lava_Lamp.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Final_Lava_Lamp.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;what-is-next-&#34;&gt;What is next&lt;/h2&gt;
&lt;p&gt;This tutorial is a brief introduction to Ray Marching, there are still many things to cover. At least, three important aspects are to be studied, the first one being terrain marching, the second one fractal marching and the last one being the use of impostors in real-time rendering pipelines that are rendered using raymarching. In a future update, I will probably add a section about terrain marching, stay tuned.&lt;/p&gt;
&lt;h2 id=&#34;references-&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Open GL Tutorials: 
&lt;a href=&#34;https://learnopengl.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://learnopengl.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ray Marching Game: 
&lt;a href=&#34;https://www.youtube.com/watch?v=9U0XVdvQwAI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=9U0XVdvQwAI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cloud Raymarching in Unity: 
&lt;a href=&#34;https://www.youtube.com/watch?v=4QOcCGI6xOU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=4QOcCGI6xOU&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Converting 3D scenes to SDF with 3D textures: 
&lt;a href=&#34;https://kosmonautblog.wordpress.com/2017/05/01/signed-distance-field-rendering-journey-pt-1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kosmonautblog.wordpress.com/2017/05/01/signed-distance-field-rendering-journey-pt-1/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Inigo Quilez tutorials: 
&lt;a href=&#34;https://www.iquilezles.org/www/articles/terrainmarching/terrainmarching.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.iquilezles.org/www/articles/terrainmarching/terrainmarching.htm&lt;/a&gt; 
&lt;a href=&#34;https://www.iquilezles.org/www/articles/raymarchingdf/raymarchingdf.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.iquilezles.org/www/articles/raymarchingdf/raymarchingdf.htm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GLSL / Shadertoy: 
&lt;a href=&#34;https://www.opengl.org/documentation/glsl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.opengl.org/documentation/glsl/&lt;/a&gt; 
&lt;a href=&#34;https://www.shadertoy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.shadertoy.com/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
