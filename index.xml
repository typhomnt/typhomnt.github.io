<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Maxime Garcia</title>
    <link>http://typhomnt.github.io/</link>
      <atom:link href="http://typhomnt.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Maxime Garcia</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 20 Feb 2019 16:29:59 +0100</lastBuildDate>
    <image>
      <url>http://typhomnt.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_box_center_3.png</url>
      <title>Maxime Garcia</title>
      <link>http://typhomnt.github.io/</link>
    </image>
    
    <item>
      <title>Making a NPR Shader in Blender</title>
      <link>http://typhomnt.github.io/post/blender_npr/</link>
      <pubDate>Sun, 26 Sep 2021 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/blender_npr/</guid>
      <description>&lt;p&gt;This article is the result of my first attempt at making an NPR shader inside Blender.&lt;/p&gt;
&lt;p&gt;I first tried to play around the shading node system as well as the post processing effects like Ambient Occlusion or Bloom and test various things. As I would like to focus on &amp;ldquo;real-time&amp;rdquo; rendering, I tested everything with the EEVEE rendering engine. I also looked at many videos and works that have been done inside Blender for NPR.&lt;/p&gt;
&lt;p&gt;If you are interested in the subject, you can take a look at (or even join) the 
&lt;a href=&#34;https://www.facebook.com/groups/BNPRandFreestyle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blender NPR community&lt;/a&gt; and also check out the Lightning Boy Studio 
&lt;a href=&#34;https://www.youtube.com/channel/UCd9i2MKimSaKezat1xkn8-A&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;youtube channel&lt;/a&gt;, there are many resources for that field.&lt;/p&gt;
&lt;p&gt;In this article, I am going to describe a simple version of a Blender shader NPR that I built. Of course, I took inspiration from what is available on the Internet and also tried to propose my own vision. It is very far from perfect but I hope it will give you a basic understanding in order to start exploring NPR on your own.&lt;/p&gt;
&lt;h2 id=&#34;shader-structure&#34;&gt;Shader Structure&lt;/h2&gt;
&lt;p&gt;The shader is represented as a node group which itself is divided into several groups that correspond to light contributions.
More precisely, this NPR shader light contributions are divided into 5 components: Diffuse, Specular, Ambient occlusion, Sub-Surface Scattering (SSS) and Emission.
Furthermore, I also include Rim light and Outline contributions which are heavily present in NPR shaders.&lt;/p&gt;
&lt;p&gt;First let me tease the kind of results you can obtain with this shader.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Clare_2.png&#34; alt=&#34;NPR Example&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Clare_1.png&#34; alt=&#34;NPR Example&#34;&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: center;&#34;&gt; model: Clare by ruslik85 on Sketchfab &lt;/p&gt;
&lt;p&gt;As you can see the range of rendering styles can vary from very cartoonish shading to more realistic results.
Below, you can notice the difference compared to a more realistic look using the principled Shader (model: Ruby Rose by theStoff on Sketchfab) :&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Realistic&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Cartoon&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;video autoplay loop muted controls&gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/Realistic_Animated.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;video autoplay loop muted controls&gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/NPR_Animated.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The main tool to make such variations is to use color ramps to tweak the output of base shaders, like the Diffuse or Glossy (Specular) shaders.
There is more to it of course, and we will go into details in the next sections.&lt;/p&gt;
&lt;p&gt;As you can see, the NPR look is more contrasted and &amp;ldquo;flashier&amp;rdquo; than the realistic look. It also while masks details at the same time.
However, thanks to effects like Outline and Rim Light, we are also able to make relevant parts of the model (like the silhouettes) stand out.
This is especially useful to improve the perception of characters&amp;rsquo; poses and their motion.&lt;/p&gt;
&lt;p&gt;This simple Blender NPR shader is build as a node group that can be tweaked with numerous parameters:&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/NPR_Mat.PNG&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/NPR_Mat.PNG&#34; alt=&#34;NPR Material&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And here is an overview of what the inside node group looks like:&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/NPR_Group.PNG&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/NPR_Group.PNG&#34; alt=&#34;NPR Group&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This node group is itself divided into 7 sub-groups, each corresponding to different light contributions. We then blend them together by choosing an appropriate blending operation. Each of these contributions will be detailed in its own section as well as their blending.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s first dig into each component independently for a scene containing a plane, a sphere, Suzanne and a sun light.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; Before going further, it is important to switch the color management to Standard in the Render Properties panel as we want to control our color output precisely &lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Color_Managment.PNG&#34; alt=&#34;Color Managment&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;diffuse-component&#34;&gt;Diffuse Component&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_Group.PNG&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_Group.PNG&#34; alt=&#34;Diffuse Group&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The diffuse component represents the amount of light re-emitted by the surface after receiving incoming light, here coming from lamps. This amount can be obtained using the Diffuse BRDF node which computes a physically based diffusion.&lt;/p&gt;
&lt;p&gt;As our objective is to produce stylized results, we would like to tweak this output and have some artistic control over it.
Here comes into play the basic node that everyone uses in the NPR community, namely the ShaderToRGB Node. Indeed, this magic node converts the shader output into a color (works only for EEVEE at the time I write this article) and lets us tweak this output color.&lt;/p&gt;
&lt;p&gt;One convenient way of tweaking this color is to use a Color Ramp which allows us to remap an input color into an interpolated color computed from parameterized colors spread into a $ [0:1] $ range. The control offered by color ramps is pretty intuitive and thus, it makes a good candidate for us to use in our stylization process.
To tweak the diffuse component we will proceed the following way: first set the albedo of the surface as pure white in the diffuse shader and plug the output color (after ShaderToRGB) as the input of a ColorRamp.
Then we can modify the color ramp content to stylize the diffuse contribution.
And finally, we can get back the color of the surface we actually want (diffuse albedo) by multiplying the output of the color ramp with the surface color.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; Note that I use a Separate HSV node right after ShaderToRGB to use the &lt;strong&gt;value&lt;/strong&gt; of the input color as it is the only attribute that interests us.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_1.png&#34; alt=&#34;Diffuse Example&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_1_Mat.png&#34; alt=&#34;Diffuse Example&#34;&gt;&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s a good start, but several questions need to be answered: how do we modify the color ramp to get a stylized result ? What about metallic materials ? How to get back the influence of the incoming light color which is discarded with our current setup ?&lt;/p&gt;
&lt;h3 id=&#34;playing-with-color-ramps&#34;&gt;Playing with color ramps&lt;/h3&gt;
&lt;p&gt;We were able to get the light contribution of the Diffuse component inside a color ramp, it is now time to play with it in order to stylize this contribution.
To do it properly, we first need to understand what is happening inside the Diffuse node and think about how we can get closer to a stylized result like we can see in cartoons/anime.&lt;/p&gt;
&lt;p&gt;To put it simply, the Diffusion or the light re-emitted by the surface of an object hit by light, depends on the surface orientation, referred as the normal $N$, with respect to the incoming direction of the light $L$. If the surface faces the incoming light direction it will re-emit more light. This can be simply modeled using a dot product between L and N of the surface.&lt;/p&gt;
&lt;p&gt;$$ D_{iffuse} \approx max(-L \cdot N,0) $$
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_Fig.png&#34; alt=&#34;Diffuse Fig&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is the basic computation that occurs inside the Diffuse node (it is a little bit more complicated in practice but let&amp;rsquo;s keep it simple in this tutorial).
For smooth surfaces like humanoid characters, this Diffuse contribution also varies in a smooth fashion alongside the model, which is not the case when you look at cartoons/anime.
Usually you get one, two or three grading levels with abrupt changes.&lt;/p&gt;
&lt;p&gt;To get these results with the color ramp, we simply have to add levels to the ramp (press the &amp;lsquo;+&amp;rsquo; button inside the ramp) and change the interpolation mode to constant, restraining the number of light levels diffused by the model to a fixed number.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_2.png&#34; alt=&#34;Diffuse Example&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_2_Mat.PNG&#34; alt=&#34;Diffuse Example&#34;&gt;&lt;/p&gt;
&lt;p&gt;Et voila ! you get a cartoonish/anime shading. You are free to tweak the levels at your own convenience as well as to use a smoother interpolation and even mix the discrete and smooth Diffusion, like I suggest in the final version of the Diffuse shader.&lt;/p&gt;
&lt;h3 id=&#34;metallic-diffusion-&#34;&gt;Metallic Diffusion ?&lt;/h3&gt;
&lt;p&gt;As you probably guess there is something to take into account with metallic materials regarding Diffusion.
Indeed, something to know is that metallic materials do not diffuse light, only absorbing and reflecting it.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s why we took it into consideration by multiplying the result of the diffusion with $ 1 - metalness $.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_3.png&#34; alt=&#34;Diffuse Example&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_3_Mat.PNG&#34; alt=&#34;Diffuse Example&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;light-color-influence&#34;&gt;Light Color Influence&lt;/h3&gt;
&lt;p&gt;Using our current node configuration, you can notice that changing the light color has no effect on the shading of our objects.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_Light_Influence_Objs.PNG&#34; alt=&#34;Light Influence Off&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_Light_Influence.PNG&#34; alt=&#34;Light Influence On&#34;&gt;&lt;/p&gt;
&lt;p&gt;And this is due to the fact that we use the output of the gray scaled color ramp as the Diffuse light contribution. And so, light color influence is something we lost from the Diffuse BSDF that takes this factor into account.
The question is how to get it back ?
The answer is quite simple. As we already recover the light intensity of our shading (&lt;strong&gt;value&lt;/strong&gt; of the HSV node) we just have to get back the &lt;strong&gt;hue&lt;/strong&gt; and &lt;strong&gt;saturation&lt;/strong&gt; resulting from  a multiplication between the object color and the light color.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_Light_Influence_Objs_On.PNG&#34; alt=&#34;Light Influence On&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_Light_Influence_Objs_On_Mat.PNG&#34; alt=&#34;Light Influence On Mat&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;smoothness-and-responsiveness&#34;&gt;Smoothness and Responsiveness&lt;/h3&gt;
&lt;p&gt;It is also possible to give more artistic control to the Diffuse component, and this is something we will also use for the Specular contribution.&lt;/p&gt;
&lt;p&gt;First, it is very handy to add a smoothness control to your Diffuse shading.
And this can be done easily by adding a second color ramp with a linear interpolation this time, and blend it with the previous color ramp we built using a Mix Color node and a smoothness parameter.&lt;/p&gt;
&lt;p&gt;Secondly, it can also be useful to add a control that tweaks the amount of light re-emitted using a Map range node that maps $ [a : b] $ to $ [c : d] $. Here, we use an attenuation parameter that we plug as $b$ so that $ [0:b] $ gets mapped into $ [0:1] $, thus the higher $b$ is the less light will be re-emitted.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_4.png&#34; alt=&#34;Diffuse Example&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Diffuse_4_Mat.PNG&#34; alt=&#34;Diffuse Example&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;specular-component&#34;&gt;Specular Component&lt;/h2&gt;
&lt;p&gt;The Specular component represents the amount of incoming light that is reflected by the object surface.
One simple way of formulating its contribution is : from the camera point of view, how well does it see the light source reflected on the object ?&lt;/p&gt;
&lt;p&gt;The simplest model describing this contribution has been introduced by Phong, and simply look at the dot product between the reflected light vector $ R = reflect(L,N) $ and the view vector $ V $.&lt;/p&gt;
&lt;p&gt;$$ S_{pecular} \approx R \cdot V $$
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Specular_Fig.png&#34; alt=&#34;Specular Fig&#34;&gt;&lt;/p&gt;
&lt;p&gt;Inside Blender, computations are more complicated as it relies on physically-based algorithms and I invite you to take a look at my 
&lt;a href=&#34;../../teaching/ray_tracing/pbr_intro&#34;&gt;Physically-Based Rendering tutorial&lt;/a&gt; if you want to know more about it.&lt;/p&gt;
&lt;p&gt;The way we will implement this component is highly similar to what we did for the Diffuse component and thus I will not detail everything in this section.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Specular_Group.PNG&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Specular_Group.PNG&#34; alt=&#34;Specular Group&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The first thing to note is that I used the Principled BRDF node to get the specular component as the other nodes don&amp;rsquo;t provide support for metallic materials. As we are only interested in the Specular component here, I put a perfectly black color for the base color, so that we remove the Diffuse contribution provided by the Principled BRDF node. I also put the specular parameter at $1$ and plugged in the roughness parameter that plays a big role for that contribution. It is also important to notice that the metallness used is actually 1 minus the metalness and this is for stylization purposes.&lt;/p&gt;
&lt;p&gt;The rest of the Specular node group is exactly the same as the Diffuse component. We sample the intensity (or &lt;strong&gt;value&lt;/strong&gt;) of the Specular contribution using color ramps, possibly attenuating it beforehand, and mix the result with the specular color and the light color influence.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Specular.png&#34; alt=&#34;Specular Example&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; A small note for metallic materials. To enhance the look of metallic materials, you might use an environment map for the World material to get more reflections and thus a more believable look.
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/World_Mat_Env.PNG&#34; alt=&#34;World Mat&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;No Env Map&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Env Map on&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;video autoplay loop muted controls&gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/Magnemite_1.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;video autoplay loop muted controls&gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/Magnemite_2.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;ambient-occlusion&#34;&gt;Ambient Occlusion&lt;/h2&gt;
&lt;p&gt;Ambient Occlusion (AO) is actually not a light contribution but more like a shadow contribution. It describes the area of the scene where light rays are less likely to hit.
This is especially the case of corners and creases and it is quite noticeable if you look at your room for instance.&lt;/p&gt;
&lt;p&gt;Ambient occlusion greatly enhances the realism of real-time renders and is something that artists are aware of when drawing and painting.
In our case, we would like to use it to give more contrast and color variation to our renders.&lt;/p&gt;
&lt;p&gt;The first thing to do when using EEVEE, is to enable Ambient Occlusion in the Render Properties tab.
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Ambient_Occlusion_Panel.PNG&#34; alt=&#34;Ambient Occlusion Panel&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are a couple of parameters like the Distance, Factor and Trace Precision as well as Bents Normals and Bounces Approximation. I won&amp;rsquo;t go into details on how Ambient Occlusion is computed but for our purpose let&amp;rsquo;s keep Factor and Trace Precision at 1 and enable Bents Normals and Bounces Approximation. The distance in this tab is not relevant and will be tweakable from the Ambient Occlusion node directly.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Ambient_Occlusion_Group.PNG&#34; alt=&#34;Ambient Group&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/ambient_occlusion.png&#34; alt=&#34;Ambient Example&#34;&gt;&lt;/p&gt;
&lt;p&gt;Like for Diffusion and Specular Reflexion, we use a color ramp to limit the number of grayscale levels AO can contribute to.
Notice that I limited AO to two levels for the cartoonish/anime shading as it is often more subtle than the previous components.&lt;/p&gt;
&lt;p&gt;You will also have to adjust the Distance parameter depending on the scale of your model, especially for small and large models. In the first case your model might be fully occluded while in the second case you might not notice any occlusion if you don&amp;rsquo;t tune this parameter carefully.&lt;br&gt;
I also added a Math power node to tweak the contrast of AO and give it more importance.&lt;/p&gt;
&lt;p&gt;Finally, to further increase the color contrast, we assign a color to the non occluded areas with a Multiply node.&lt;/p&gt;
&lt;h2 id=&#34;emission-component&#34;&gt;Emission Component&lt;/h2&gt;
&lt;p&gt;The emissive contribution is the easiest to take into account as Blender already does all the work for us with the Emissive shader.&lt;/p&gt;
&lt;p&gt;However, it is important to note that when using EEVEE and real-time renderers, you have to enable Bloom to get a greater impact.
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Bloom_Panel.PNG&#34; alt=&#34;Bloom Panel&#34;&gt;&lt;/p&gt;
&lt;p&gt;It is a post-processing effect that highlights the brightest parts of your scene which is the case of emissive materials.
This is something widely used in video games to make fire, lasers, lamps and other special effects to stand out.
As you can see, there are also few parameters to tune the Bloom effect. You might want to tweak the Threshold parameter that determines when a given color intensity (&lt;strong&gt;value&lt;/strong&gt;) is considered as bright.&lt;/p&gt;
&lt;p&gt;Notice the difference when Bloom is enabled.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;No Bloom&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Bloom On&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Emission.png&#34; alt=&#34;Emission Example&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Emission_Bloom.png&#34; alt=&#34;Emission Example&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Emission_Group.PNG&#34; alt=&#34;Emission Group&#34;&gt;&lt;/p&gt;
&lt;p&gt;Currently it is quite costly to handle emissive materials as light sources in a real-time fashion.
Handling emissive materials like path tracers (Cycles in Blender) might change in the future with real time ray tracing being faster and more optimized.&lt;/p&gt;
&lt;h2 id=&#34;rim-light-and-outline&#34;&gt;Rim Light and Outline&lt;/h2&gt;
&lt;p&gt;Rim lighting and Outline are tools used by artists to better depict the contours and the pose of characters.
They can also be used to set a specific mood and to enhance the perception of motion.&lt;/p&gt;
&lt;h3 id=&#34;extracting-the-contours&#34;&gt;Extracting the contours&lt;/h3&gt;
&lt;p&gt;There are plenty of techniques to get the contours of a 3D model and draw art lines on top of it. There are pros and cons for each technique, but state-of-the-art methods tend to give a lot of freedom with computational costs that get shorter and shorter.

&lt;a href=&#34;https://hal.inria.fr/hal-02189483/document&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Benard et al.&lt;/a&gt; made a survey summarizing many existing methods to draw art lines on top of 3D models.&lt;/p&gt;
&lt;p&gt;In this tutorial, I described the simplest approach which is far from being perfect and you might want to use other methods like the 
&lt;a href=&#34;https://www.blendersecrets.org/secrets/inverted-hull-toon-outline-bnpr-blender-tutorial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;inverted hull&lt;/a&gt;, at least for the outline.&lt;/p&gt;
&lt;p&gt;The simplest way of getting the contours is to use the dot product between the view direction $ V $ and the normal $ N $.
Indeed, one way of defining parts of a 3D model that are near the silhouette is to look at where this dot product is near zero, or formulated differently, where the normal of the surface is orthogonal to the view direction.&lt;/p&gt;
&lt;h3 id=&#34;outline&#34;&gt;Outline&lt;/h3&gt;
&lt;p&gt;There is an easy way to get this dot product in Blender within one node, namely the Fresnel node.
It takes into account an index of refraction which is a fundamental parameter to determine the amount of light that goes through the object, which also tells us the amount of light that gets reflected as its complementary.
This amount of light being reflected is directly linked to the dot product between the view direction $ V $ and  the normal $ N $.
And so, to get the Outline, we can use the output of the Fresnel node and threshold it with a color ramp like we did for the other components.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Outline_Group.PNG&#34; alt=&#34;Outline Group&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/outline.png&#34; alt=&#34;Outline Example&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;rim-light&#34;&gt;Rim light&lt;/h3&gt;
&lt;p&gt;Rim Light is also referred as a backlight that highlights the full silhouette or only one part of it.&lt;/p&gt;
&lt;p&gt;To achieve this effect with 3D geometry, we can also compute the dot product between the normal $ N $ and the camera view $ V $ to get the silhouette and use color ramps to tweak its values.
This time I intentionally computed the dot product using Vector Math nodes.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Rim_Light_Group.PNG&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Rim_Light_Group.PNG&#34; alt=&#34;Rim Light Group&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We also need to limit the highlighted area to one side of the model (Side Masking). More precisely, we have to think in view space to correctly achieve this effect.
Therefore, we convert the normal into view space using the Transform node.
Basically what we want is to limit the silhouette lighting where the view space normal is aligned with a desired rim light direction.
Computing an alignment with a vector is equivalent to compute the dot product between the normal and the Rim light direction.&lt;/p&gt;
&lt;p&gt;And that&amp;rsquo;s it, using this trick we can highlight one side of the silhouette and mask the other side.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; Note that this Rim light direction is encoded as a 3D vector as Blender only supports them, but in practice we do not use the Z coordinate, hence the Vector Multiplication node.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a final touch, like precedently, we can use color ramps to sharpen or soften the Rim Light depending on the style you want to convey.
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/rim_light.png&#34; alt=&#34;Rim Light Example&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;sub-surface-scaterring&#34;&gt;Sub-Surface Scaterring&lt;/h2&gt;
&lt;p&gt;Sub-Surface Scattering (SSS) is one of the most difficult contributions to handle as it is also a more complex phenomena.
It corresponds to light rays that completely traverse an opaque medium, like your body, and that reaches your eye. One common example given to behold this phenomena is to place your hand between you and the sun and look at the space between your fingers, you will notice a reddish color at the silhouette of your hand. This is the light going through a thin opaque medium that is your skin.&lt;/p&gt;
&lt;p&gt;Although it is a quite common phenomena, it is also very difficult to achieve in a real-time fashion and for this tutorial we will use the approximation proposed by 
&lt;a href=&#34;https://www.alanzucconi.com/2017/08/30/fast-subsurface-scattering-1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alan Zucconi&lt;/a&gt; (based on a GDC talk) to achieve our NPR SSS inside Blender. Note that we will not use the SSS shader provided by Blender as the method proposed by Alan Zucconi offers more expressiveness and  better artistic control.
We will also extend this approach by adding two additional contributions.&lt;/p&gt;
&lt;h3 id=&#34;approximated-sss&#34;&gt;Approximated SSS&lt;/h3&gt;
&lt;p&gt;In this section, I will  mostly paraphrase the tutorial of 
&lt;a href=&#34;https://www.alanzucconi.com/2017/08/30/fast-subsurface-scattering-1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alan Zucconi&lt;/a&gt;, I will skip few details and explain how to implement it inside Blender.&lt;/p&gt;
&lt;p&gt;Like I mentioned above, SSS is a complex phenomenon where light rays go through the surface and are also deviated from their incoming direction.
If we look at the simple situation of a directional light casting rays to our object, we can notice that the non lighted region should be dark for an opaque material.
However, for translucent materials this is not the case and this area should be lighted somehow.&lt;br&gt;
The trick to take into account this phenomena is to create a fictive light contribution from the opposite direction of the incoming light.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s suppose that the incoming light from L is not deviated inside the material, that means that a viewer will best see light rays that go through the object if his gaze is aligned with L and on the contrary if it is orthogonal, he/she won&amp;rsquo;t see light. This means that the intensity of visible traversing light is proportional to the dot product of $ V  $ and $ L $.&lt;/p&gt;
&lt;p&gt;$$ I_{SSS} = clamp(V \cdot L ,0,1) $$&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;SSS Off&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;SSS On&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_No.png&#34; alt=&#34;SSS Fig&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_On.png&#34; alt=&#34;SSS On Fig&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;However, inside the object, the light undergoes some deviation and most noticeably at the object surface. So let&amp;rsquo;s take into account the deviation of the light rays that occurs when they leave the inside of an object.
This can be done using the normal $ N $ and the subsurface distortion $\delta$ that tells how much the normal will distort the light towards an halfway direction $ (N - L) $.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_Deviation.png&#34; alt=&#34;SSS Deviation Fig&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s also give more artistic control to this contribution, tweaking the contrast $ p $ and the scale $ S $with two parameters:&lt;/p&gt;
&lt;p&gt;$$ I_{SSS} = clamp(V \cdot - (\delta N - L),0,1)^pS $$&lt;/p&gt;
&lt;p&gt;Finally, we can also take into account the thickness of the object light rays are traversing. Indeed, the thicker the object is the more light will be absorbed and restrained inside the object.
Hence, the final formula of this approximated SSS taking into account the thickness:&lt;/p&gt;
&lt;p&gt;$$ I_{SSS} = clamp(V \cdot - (\delta N - L),0,1)^pS * (1 - thickness) $$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; Getting the true thickness, meaning the distance that a light ray traversed inside the object, can be costly as its computation is based on ray tracing. Instead, we can rely on a rough approximation using Ambient Occlusion like stated by Alan Zucconi. The trick is to compute AO on the inverted faces of the model, so that we compute the occlusion of the inside of the model and get information of the local thickness which can be sufficient for our case. Thankfully, the Ambient Occlusion node Blender provides this information by checking the &lt;strong&gt;Inside&lt;/strong&gt; option.
&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_Thickness.png&#34; alt=&#34;SSS Group&#34;&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that I explained the theory behind this version of SSS, it is time to implement it inside Blender. One difficulty we encounter here with the node system, is the fact that we do not have access to the light direction $ L $.
Thankfully, there is a way to overcome this issue using Blender Drivers which are basically linking scene properties to variables inside the node editor.
However, it is important to note that SSS will only work for a unique light source and will not be taken into account for others, if you have any on your scene.&lt;/p&gt;
&lt;p&gt;To get $ L $ from our main directional light we will add a CombineXYZ node and right click on the x coordinate and choose &amp;lsquo;Add Driver&amp;rsquo;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Driver.png&#34; alt=&#34;Driver 1&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then, a panel will appear that will allow you to choose the scene property that will be linked to this coordinate. Select your light source in the object field and choose its X rotation as we deal with a directional light (could be the position for a point light).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Driver_Light.png&#34; alt=&#34;Driver 2&#34;&gt;&lt;/p&gt;
&lt;p&gt;Repeat this process for the y and z coordinates and you get the rotation of the light inside this node. Now let&amp;rsquo;s convert this rotation into a direction vector.
The Vector Rotation node Blender provides does exactly what we need, it applies a rotation to an input vector from the Euler angles (that we retrieve with the Drivers).
Notice that the z coordinate of the input vector is $ -1 $ so that we get $ -L $ directly from the rotation node.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/Driver_Light_Dir.png&#34; alt=&#34;Driver 3&#34;&gt;&lt;/p&gt;
&lt;p&gt;We finally compute the dot product with the view Vector $ V $, apply the scale, thickness and power factor and get our approximated SSS contribution.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_Approx.png&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_Approx.png&#34; alt=&#34;SSS Group&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;video autoplay loop muted &gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/SSS_Approx.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/p&gt;
&lt;h3 id=&#34;inverse-diffuse-lighting&#34;&gt;Inverse Diffuse Lighting&lt;/h3&gt;
&lt;p&gt;It is possible to go further in the stylization of SSS by adding an additional effect which is much more &amp;ldquo;artistic&amp;rdquo; than the approximation I just described.
The main idea is to add the complementary of the diffuse component as a contribution in order to make non lighted areas pop out a little bit.
The contribution is divided into two parts : front inverse diffuse and back inverse diffuse. The first adds the complementary of the incoming light while the later adds the complementary of the light coming from the opposite direction.&lt;/p&gt;
&lt;p&gt;Mathematically speaking :&lt;/p&gt;
&lt;p&gt;$$ F_{SSS} = 1 - max(L \cdot N,0)  $$
$$ B_{SSS} = 1 - max(-L \cdot N,0)  $$&lt;/p&gt;
&lt;p&gt;In the same manner as for the approximated SSS, we can add more control to these contributions :&lt;/p&gt;
&lt;p&gt;$$ F_{SSS} = (1 - max(L \cdot N,0))^p*\alpha_{front} $$
$$ B_{SSS} = (1 - max(-L \cdot N,0))^p*\alpha_{back}  $$&lt;/p&gt;
&lt;p&gt;And finally, the thickness of the object should also be taken into consideration:&lt;/p&gt;
&lt;p&gt;$$ I_{nverse Diffuse} = ((F_{SSS} + B_{SSS})&lt;em&gt;S)^{p_i}&lt;/em&gt; (1 - thickness) $$&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_Front_Back.png&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_Front_Back.png&#34; alt=&#34;SSS Group&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;video autoplay loop muted &gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/SSS_Inverse_Diffuse.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/p&gt;
&lt;h3 id=&#34;final-sss-node-group&#34;&gt;Final SSS Node Group&lt;/h3&gt;
&lt;p&gt;The final SSS contribution results in the sum of the approximated SSS and the inverse diffuse contribution.&lt;/p&gt;
&lt;p&gt;$$ SSS = (I_{SSS} + I_{nverse Diffuse})*(1 - metalness) $$&lt;/p&gt;
&lt;p&gt;As metallic materials cannot be translucent, we take into account the metalness of the material with a Multiply node and nullify the contribution of the SSS if the material is a metal.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_All.png&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/SSS_All.png&#34; alt=&#34;SSS Group&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Notice that eventually tweak the final result with color ramps to discretize or smooth the contribution.&lt;/p&gt;
&lt;p&gt;&lt;video autoplay loop muted &gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/SSS_Example.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/p&gt;
&lt;h2 id=&#34;blending-components&#34;&gt;Blending Components&lt;/h2&gt;
&lt;p&gt;Now that we computed all the 7 different contributions, it is time to blend them together.
Diffuse, Specular and SSS components should be added together.
However, we first apply the Ambient Occlusion to the Diffuse component in Overlay mode in order to increase the contrast between the non occluded and occluded regions.
We then proceed to the SSS coloring using a simple multiplication with the SSS color. Note that, at this stage, we could decide to take into account the influence of the material color over the SSS output color.
We then successively add the SSS and the Specular component with an Add node.&lt;/p&gt;
&lt;p&gt;Finally we add the Rim lighting using a Mix node so that it goes on top of all the other contributions and re-iterate this operation for the Outline that also goes on top of the Rim light.&lt;/p&gt;
&lt;p&gt;Et voilà ! We implemented our first NPR shader inside Blender !&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/NPR_Group.PNG&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/Blender_NPR/NPR_Group.PNG&#34; alt=&#34;Blending&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; This way of blending contributions together is one of the infinite possibilities you can test. There is no true way to blend these contributions together as we deal with NPR, so tweak these blend modes to your heart content !&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;p&gt;The main limitation of the shader is that it only supports one light for the main lighting and the SSS as we use drivers. If you want to add more light I recommend that you use point, spot and area lights even though the SSS won&amp;rsquo;t be computed for these additional lights.
Additionally, Inverse Diffuse Lighting can produce artifacts when it is not properly aligned with the Diffuse contribution, and that is because we compute it as the complementary of a fake Diffuse component.
This part should be fixed and take into account the output of the true Diffuse component instead.&lt;/p&gt;
&lt;p&gt;There is not too much room for stylizing metallic materials with this shader, it mainly depends on the environment map you use. This aspect can surely be improved a lot.&lt;/p&gt;
&lt;p&gt;Finally, we did not cover the artifacts that are due to the topology of the model that may not be adapted to cartoon/anime style, I&amp;rsquo;ll be definitely working on that part.&lt;/p&gt;
&lt;h2 id=&#34;going-further-&#34;&gt;Going further ?&lt;/h2&gt;
&lt;p&gt;From that point, I strongly encourage you to test this shader in more complex scenes like the one below. Do not hesitate to try variations, new ideas often come from testing !&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Sakura Tree 1&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Sakura Tree 2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;video autoplay loop muted controls &gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/Sakura_Tree.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;video autoplay loop muted controls &gt;&lt;source src=&#34;http://typhomnt.github.io/Videos/NPR/Blender_NPR/Sakura_Tree_2.mp4&#34; type=&#34;video/mp4&#34;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This tutorial only covers the basics but you may want to go further and include texture effects like cross hatching, stippling and watercolor. These effects may rely on noises that will disturb contributions by a small amount. Of course, more advanced methods outside the scope of Blender exist like our post processing method we introduced in 
&lt;a href=&#34;../../research/coherent_splat_stylization&#34;&gt;our paper&lt;/a&gt;. As for the Watercolor and other painterly looks, you can definitely check the tools proposed by 
&lt;a href=&#34;https://artineering.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Artineering&lt;/a&gt; !&lt;/p&gt;
&lt;p&gt;You may also go further in shading control and tweak the normals of your model to get rid of unwanted artifacts. Indeed, if you want to go for anime style, unless you adapt the topology of your model to have sharp transitions and add/remove some shadows, you will have to tweak the normals. Inside Blender, you can edit your normals or you can use proxy objects to achieve a more consistent anime look. That is the topic of other tutorials you can find on the Internet but as a good start you can check the 
&lt;a href=&#34;https://www.youtube.com/watch?v=yhGjCzxJV3E&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Guilty Gear GDC conference&lt;/a&gt; that gives a good insight on how to achieve anime looking style in 3D.&lt;/p&gt;
&lt;p&gt;There is a lot more to be discovered in that field of course, never stop exploring !&lt;/p&gt;
&lt;h2 id=&#34;tutorial-files&#34;&gt;Tutorial Files&lt;/h2&gt;
&lt;p&gt;You can find this tutorial NPR material inside this 
&lt;a href=&#34;http://typhomnt.github.io/Files/shading_npr.blend&#34;&gt;blend file&lt;/a&gt; as well as the file generating the teaser image 
&lt;a href=&#34;http://typhomnt.github.io/Files/shading_samples.blend&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Music</title>
      <link>http://typhomnt.github.io/art/music/</link>
      <pubDate>Sun, 05 Sep 2021 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/art/music/</guid>
      <description>&lt;p&gt;Here are some of my piano recordings.&lt;/p&gt;
&lt;p&gt;Chopin - Etude 5 Op 25 &amp;ldquo;Wrong Notes&amp;rdquo; (Recorded in 2015)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34;&gt;
  &lt;source type=&#34;audio/mp3&#34; src=&#34;http://typhomnt.github.io/Audio/Wrong_Notes_good.mp3&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
&lt;p&gt;Final Fantasy X - To Zanarkand  (Recorded in 2020)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34;&gt;
  &lt;source type=&#34;audio/mp3&#34; src=&#34;http://typhomnt.github.io/Audio/zanarkand_final.mp3&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
&lt;p&gt;Nier - Grandma  (Recorded in 2021)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34;&gt;
  &lt;source type=&#34;audio/mp3&#34; src=&#34;http://typhomnt.github.io/Audio/Grandma_clear_ver.mp3&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
&lt;p&gt;Nier - Song of The Ancients (Recorded in 2021)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34;&gt;
  &lt;source type=&#34;audio/mp3&#34; src=&#34;http://typhomnt.github.io/Audio/Song_Of_The_Ancient.mp3&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
&lt;p&gt;Nier - Yonah  (Recorded in 2021)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34;&gt;
  &lt;source type=&#34;audio/mp3&#34; src=&#34;http://typhomnt.github.io/Audio/Yonah.mp3&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
&lt;p&gt;Nier - Emil  (Recorded in 2022)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34;&gt;
  &lt;source type=&#34;audio/mp3&#34; src=&#34;http://typhomnt.github.io/Audio/Emil.mp3&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
&lt;p&gt;Neon Genesis Evangelion - Those women longed for the touch of other&amp;rsquo;s lips and thus invited their kisses (Recorded in 2023)&lt;/p&gt;
&lt;audio controls=&#34;controls&#34;&gt;
  &lt;source type=&#34;audio/mp3&#34; src=&#34;http://typhomnt.github.io/Audio/those_women_longed_for_the_touch_of_other_s_lips_and_thus_invited_their_kisses.mp3&#34;&gt;&lt;/source&gt;
&lt;/audio&gt;
</description>
    </item>
    
    <item>
      <title>Post Processing Effects</title>
      <link>http://typhomnt.github.io/post/post_proc_effects/</link>
      <pubDate>Fri, 20 Aug 2021 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/post_proc_effects/</guid>
      <description>&lt;p&gt;Here is a compilation of post processing effects implemented inside Skyengine. I implemented some of them inside Unity as well.&lt;/p&gt;
&lt;p&gt;Gundam model : Gundam GAT 105 modified by Rupen Kanwar on Sketchfab&lt;/p&gt;
&lt;h3 id=&#34;bloom&#34;&gt;Bloom&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Post_Proc_Effects/Bloom_Sky_Gundam.PNG&#34; alt=&#34;Bloom&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;outline&#34;&gt;Outline&lt;/h3&gt;
&lt;p&gt;Based on a Sobel filter on the Depth and Normals&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Post_Proc_Effects/Sobel_Sky_Gundam.PNG&#34; alt=&#34;Outline&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;contrast&#34;&gt;Contrast&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Post_Proc_Effects/Contrast_Sky_Gundam.PNG&#34; alt=&#34;Contast&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;kuwahara-filter&#34;&gt;Kuwahara Filter&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Post_Proc_Effects/Kuwahara_Sky_Gundam.PNG&#34; alt=&#34;Kuwahara&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Oxyde ⌀  Redux edition</title>
      <link>http://typhomnt.github.io/project/oxyde/</link>
      <pubDate>Mon, 12 Apr 2021 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/project/oxyde/</guid>
      <description>&lt;!--&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/SkyEngine/SkyLogoMob.png&#34; alt=&#34;SkyLogo&#34; style=&#34;width:200px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;p&gt;Oxyde ⌀  Redux Edition is an arcade game I worked on with 5 teammates during the 
&lt;a href=&#34;https://www.grenoblegamelab.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scientific Game Jam&lt;/a&gt; 2020. This game jam focuses on creating games in 48 hours whose theme is linked with PhD students&amp;rsquo; research. And here it is, we created this game in a style between Super Hexagon and Touhou, where the player has to maintain the temperature of a cooling metalic glass at a medium while avoiding oxydation. The main principle is to catch heat particles but not too much while protecting the core with a controlable shield against both heat and oxygen particles. During the project I worked on the graphics of the game that are mainly built on Unity Shader Graphs.&lt;/p&gt;
&lt;p&gt;You can download and test the game on the 
&lt;a href=&#34;https://eikins.itch.io/oxyde-redux-edition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Itch page&lt;/a&gt; of the project.&lt;/p&gt;
&lt;video autoplay loop muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Oxyde/oxyde.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
</description>
    </item>
    
    <item>
      <title>3D Art</title>
      <link>http://typhomnt.github.io/art/3d_art/</link>
      <pubDate>Wed, 30 Sep 2020 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/art/3d_art/</guid>
      <description>&lt;h3 id=&#34;sketchfab-models&#34;&gt;Sketchfab Models&lt;/h3&gt;
&lt;div class=&#34;block_container&#34;&gt;
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt; &lt;iframe title=&#34;Splatoon Squid Form&#34; frameborder=&#34;0&#34; allowfullscreen mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34; allow=&#34;autoplay; fullscreen; xr-spatial-tracking&#34; xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/2c9bdb36324f4c85a24f8df5076400b0/embed&#34;&gt; &lt;/iframe&gt; &lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt; &lt;a href=&#34;https://sketchfab.com/3d-models/splatoon-squid-form-2c9bdb36324f4c85a24f8df5076400b0?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=2c9bdb36324f4c85a24f8df5076400b0&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; Splatoon Squid Form &lt;/a&gt; by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=2c9bdb36324f4c85a24f8df5076400b0&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; typhomnt &lt;/a&gt; on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=2c9bdb36324f4c85a24f8df5076400b0&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt; &lt;iframe title=&#34;Monster Hunter Great Sword&#34; frameborder=&#34;0&#34; allowfullscreen mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34; allow=&#34;autoplay; fullscreen; xr-spatial-tracking&#34; xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/e7393eb02a1b40e8b11e2cfcd09281f9/embed&#34;&gt; &lt;/iframe&gt; &lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt; &lt;a href=&#34;https://sketchfab.com/3d-models/monster-hunter-great-sword-e7393eb02a1b40e8b11e2cfcd09281f9?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=e7393eb02a1b40e8b11e2cfcd09281f9&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; Monster Hunter Great Sword &lt;/a&gt; by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=e7393eb02a1b40e8b11e2cfcd09281f9&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; typhomnt &lt;/a&gt; on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=e7393eb02a1b40e8b11e2cfcd09281f9&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt; &lt;iframe title=&#34;Monster Hunter Ratalos Great Sword&#34; frameborder=&#34;0&#34; allowfullscreen mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34; allow=&#34;autoplay; fullscreen; xr-spatial-tracking&#34; xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/289efb1c60bb46039e129312dffe3aff/embed&#34;&gt; &lt;/iframe&gt; &lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt; &lt;a href=&#34;https://sketchfab.com/3d-models/monster-hunter-ratalos-great-sword-289efb1c60bb46039e129312dffe3aff?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=289efb1c60bb46039e129312dffe3aff&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; Monster Hunter Ratalos Great Sword &lt;/a&gt; by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=289efb1c60bb46039e129312dffe3aff&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; typhomnt &lt;/a&gt; on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=289efb1c60bb46039e129312dffe3aff&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;block_container&#34;&gt;
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt; &lt;iframe title=&#34;Phantasy Star Online 2 Ray Partizan&#34; frameborder=&#34;0&#34; allowfullscreen mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34; allow=&#34;autoplay; fullscreen; xr-spatial-tracking&#34; xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/e0567743ac454070968d1005428ec3d6/embed&#34;&gt; &lt;/iframe&gt; &lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt; &lt;a href=&#34;https://sketchfab.com/3d-models/phantasy-star-online-2-ray-partizan-e0567743ac454070968d1005428ec3d6?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=e0567743ac454070968d1005428ec3d6&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; Phantasy Star Online 2 Ray Partizan &lt;/a&gt; by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=e0567743ac454070968d1005428ec3d6&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; typhomnt &lt;/a&gt; on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=e0567743ac454070968d1005428ec3d6&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt; &lt;iframe title=&#34;Phantasy Star Online 2 Red Sword&#34; frameborder=&#34;0&#34; allowfullscreen mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34; allow=&#34;autoplay; fullscreen; xr-spatial-tracking&#34; xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/2b8c364ea80740d4ac765edafc5ce38e/embed&#34;&gt; &lt;/iframe&gt; &lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt; &lt;a href=&#34;https://sketchfab.com/3d-models/phantasy-star-online-2-red-sword-2b8c364ea80740d4ac765edafc5ce38e?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=2b8c364ea80740d4ac765edafc5ce38e&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; Phantasy Star Online 2 Red Sword &lt;/a&gt; by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=2b8c364ea80740d4ac765edafc5ce38e&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; typhomnt &lt;/a&gt; on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=2b8c364ea80740d4ac765edafc5ce38e&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt; &lt;iframe title=&#34;Phantasy Star Online 2 Gigush&#34; frameborder=&#34;0&#34; allowfullscreen mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34; allow=&#34;autoplay; fullscreen; xr-spatial-tracking&#34; xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/bc3b516fd1c04aea882d295ca4a7284a/embed&#34;&gt; &lt;/iframe&gt; &lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt; &lt;a href=&#34;https://sketchfab.com/3d-models/phantasy-star-online-2-gigush-bc3b516fd1c04aea882d295ca4a7284a?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=bc3b516fd1c04aea882d295ca4a7284a&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; Phantasy Star Online 2 Gigush &lt;/a&gt; by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=bc3b516fd1c04aea882d295ca4a7284a&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; typhomnt &lt;/a&gt; on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=bc3b516fd1c04aea882d295ca4a7284a&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt; &lt;iframe title=&#34;Katana Phantasy Star Online 2&#34; frameborder=&#34;0&#34; allowfullscreen mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34; allow=&#34;autoplay; fullscreen; xr-spatial-tracking&#34; xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/e16df1a7c8b8414aa0425ab94cb7d3b6/embed&#34;&gt; &lt;/iframe&gt; &lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt; &lt;a href=&#34;https://sketchfab.com/3d-models/katana-phantasy-star-online-2-e16df1a7c8b8414aa0425ab94cb7d3b6?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=e16df1a7c8b8414aa0425ab94cb7d3b6&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; Katana Phantasy Star Online 2 &lt;/a&gt; by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=e16df1a7c8b8414aa0425ab94cb7d3b6&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; typhomnt &lt;/a&gt; on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=e16df1a7c8b8414aa0425ab94cb7d3b6&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt; 
&lt;div class=&#34;block_container&#34;&gt;
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt; &lt;iframe title=&#34;Sci-Fi Racing Ship&#34; frameborder=&#34;0&#34; allowfullscreen mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34; allow=&#34;autoplay; fullscreen; xr-spatial-tracking&#34; xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/5a4fdb48bc94458a94de15802cadaa8c/embed&#34;&gt; &lt;/iframe&gt; &lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt; &lt;a href=&#34;https://sketchfab.com/3d-models/sci-fi-racing-ship-5a4fdb48bc94458a94de15802cadaa8c?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=5a4fdb48bc94458a94de15802cadaa8c&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; Sci-Fi Racing Ship &lt;/a&gt; by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=5a4fdb48bc94458a94de15802cadaa8c&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; typhomnt &lt;/a&gt; on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=5a4fdb48bc94458a94de15802cadaa8c&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt; &lt;iframe title=&#34;Backpack&#34; frameborder=&#34;0&#34; allowfullscreen mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34; allow=&#34;autoplay; fullscreen; xr-spatial-tracking&#34; xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/5655596f9efe47deb09df57a79b44e04/embed&#34;&gt; &lt;/iframe&gt; &lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt; &lt;a href=&#34;https://sketchfab.com/3d-models/backpack-5655596f9efe47deb09df57a79b44e04?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=5655596f9efe47deb09df57a79b44e04&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; Backpack &lt;/a&gt; by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=5655596f9efe47deb09df57a79b44e04&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; typhomnt &lt;/a&gt; on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=5655596f9efe47deb09df57a79b44e04&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt; 
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt; &lt;iframe title=&#34;One Piece - Mera Mera no Mi&#34; frameborder=&#34;0&#34; allowfullscreen mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34; allow=&#34;autoplay; fullscreen; xr-spatial-tracking&#34; xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/ac8c016683fb4712ac4c8f6781f4babf/embed&#34;&gt; &lt;/iframe&gt; &lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt; &lt;a href=&#34;https://sketchfab.com/3d-models/one-piece-mera-mera-no-mi-ac8c016683fb4712ac4c8f6781f4babf?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=ac8c016683fb4712ac4c8f6781f4babf&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; One Piece - Mera Mera no Mi &lt;/a&gt; by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=ac8c016683fb4712ac4c8f6781f4babf&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; typhomnt &lt;/a&gt; on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=ac8c016683fb4712ac4c8f6781f4babf&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;block_container&#34;&gt;
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt; &lt;iframe title=&#34;Soul Edge Fan art First Attempt&#34; frameborder=&#34;0&#34; allowfullscreen mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34; allow=&#34;autoplay; fullscreen; xr-spatial-tracking&#34; xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/157800b903a647308a25627f134132b6/embed&#34;&gt; &lt;/iframe&gt; &lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt; &lt;a href=&#34;https://sketchfab.com/3d-models/soul-edge-fan-art-first-attempt-157800b903a647308a25627f134132b6?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=157800b903a647308a25627f134132b6&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; Soul Edge Fan art First Attempt &lt;/a&gt; by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=157800b903a647308a25627f134132b6&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; typhomnt &lt;/a&gt; on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=157800b903a647308a25627f134132b6&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt; &lt;iframe title=&#34;Soul Calibur Fan Art First Attempt&#34; frameborder=&#34;0&#34; allowfullscreen mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34; allow=&#34;autoplay; fullscreen; xr-spatial-tracking&#34; xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/df663648a1cc41928ba21dc09f7b4dba/embed&#34;&gt; &lt;/iframe&gt; &lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt; &lt;a href=&#34;https://sketchfab.com/3d-models/soul-calibur-fan-art-first-attempt-df663648a1cc41928ba21dc09f7b4dba?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=df663648a1cc41928ba21dc09f7b4dba&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; Soul Calibur Fan Art First Attempt &lt;/a&gt; by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=df663648a1cc41928ba21dc09f7b4dba&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; typhomnt &lt;/a&gt; on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=df663648a1cc41928ba21dc09f7b4dba&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&#34;sketchfab-embed-wrapper&#34;&gt; &lt;iframe title=&#34;Excalibur Sword&#34; frameborder=&#34;0&#34; allowfullscreen mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34; allow=&#34;autoplay; fullscreen; xr-spatial-tracking&#34; xr-spatial-tracking execution-while-out-of-viewport execution-while-not-rendered web-share width=&#34;320&#34; height=&#34;240&#34; src=&#34;https://sketchfab.com/models/02bce8f203074d1fb5eb465ca8b1b1e2/embed&#34;&gt; &lt;/iframe&gt; &lt;p style=&#34;font-size: 13px; font-weight: normal; margin: 5px; color: #4A4A4A;&#34;&gt; &lt;a href=&#34;https://sketchfab.com/3d-models/excalibur-sword-02bce8f203074d1fb5eb465ca8b1b1e2?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=02bce8f203074d1fb5eb465ca8b1b1e2&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; Excalibur Sword &lt;/a&gt; by &lt;a href=&#34;https://sketchfab.com/typhomnt?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=02bce8f203074d1fb5eb465ca8b1b1e2&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt; typhomnt &lt;/a&gt; on &lt;a href=&#34;https://sketchfab.com?utm_medium=embed&amp;utm_campaign=share-popup&amp;utm_content=02bce8f203074d1fb5eb465ca8b1b1e2&#34; target=&#34;_blank&#34; style=&#34;font-weight: bold; color: #1CAAD9;&#34;&gt;Sketchfab&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;3d-challenges&#34;&gt;3D Challenges&lt;/h3&gt;
&lt;p&gt;3D Projects made for 3DC challenges (quick challenges) on the 3DFR Discord Server.








  
  


&lt;div class=&#34;gallery&#34;&gt;

  
  
  
  
    
    
    
    
    
      
        
      
    
  &lt;a data-fancybox=&#34;gallery-3DC_Images&#34; href=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/3DC_Can.png&#34; data-caption=&#34;Can with procedural textures&#34;&gt;
  &lt;img data-src=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/3DC_Can_hufbc3b231a6e228efd72eb7d59fa8c922_2578240_0x190_resize_box_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;338&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
      
    
  &lt;a data-fancybox=&#34;gallery-3DC_Images&#34; href=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/Crystal_Table.png&#34; &gt;
  &lt;img data-src=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/Crystal_Table_hu4521d10d33b83edcf467a9c48d43873e_2842001_0x190_resize_box_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;338&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
      
    
  &lt;a data-fancybox=&#34;gallery-3DC_Images&#34; href=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/Sky_Lantern.png&#34; &gt;
  &lt;img data-src=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/Sky_Lantern_hue90aa73afc960b83ee08100c5c4c19a4_2859613_0x190_resize_box_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;338&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
      
    
  &lt;a data-fancybox=&#34;gallery-3DC_Images&#34; href=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/Throne_Nier.png&#34; &gt;
  &lt;img data-src=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/Throne_Nier_hu077fe2a572b0279ccb7185de22ffd95e_2184552_0x190_resize_box_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;338&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
      
    
  &lt;a data-fancybox=&#34;gallery-3DC_Images&#34; href=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/torii_test.png&#34; &gt;
  &lt;img data-src=&#34;http://typhomnt.github.io/art/3d_art/3DC_Images/torii_test_hu509ed74a72079062670ef1d3bb70ccac_2139358_0x190_resize_box_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;338&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  

  
&lt;/div&gt;&lt;/p&gt;
&lt;video controls muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Art/3DC/Sakura_Tree.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;h3 id=&#34;galactik-football-project&#34;&gt;Galactik Football Project&lt;/h3&gt;
&lt;p&gt;Work in progress project where I try to recreate characters and animations from the french cartoon Galactik Football,&lt;/p&gt;
&lt;h4 id=&#34;tia&#34;&gt;Tia&lt;/h4&gt;
&lt;video controls playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Art/Galactik_Football/Breath_Shoot_Montage_V4.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;h4 id=&#34;genesis-stadium&#34;&gt;Genesis Stadium&lt;/h4&gt;
&lt;video controls muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Art/Galactik_Football/Genese_V03.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;h3 id=&#34;office-project&#34;&gt;Office Project&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Art/PhD_Office/Step_11.png&#34; alt=&#34;Office&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;anima-podi&#34;&gt;Anima Podi&lt;/h3&gt;
&lt;p&gt;Animation made during the MOOC Anima Podi (Gobelins school)&lt;/p&gt;
&lt;video controls muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Art/Anima_Podi/Ball_Parkour_F.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;video controls muted playsinline width=&#34;320&#34;&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Art/Anima_Podi/Jump_Forward_VF.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;video controls muted playsinline width=&#34;320&#34;&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Art/Anima_Podi/Jump_In_Place_VFinal2.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;video controls muted playsinline width=&#34;320&#34;&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Art/Anima_Podi/Walk_In_Place_VF.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;video controls muted playsinline width=&#34;320&#34;&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Art/Anima_Podi/Mascott_Final_VF.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
</description>
    </item>
    
    <item>
      <title>Animation Target Constraint</title>
      <link>http://typhomnt.github.io/post/animation_target_ctr/</link>
      <pubDate>Mon, 16 Dec 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/animation_target_ctr/</guid>
      <description>&lt;p&gt;One common task game developers come across when using animated characters is to modify them in real-time in order to satisfy a given task.
More particularly, re-using the same grab/take animation or push/punch/kick animation to target a specific object is a feature every programmer want and may have to implement.&lt;/p&gt;
&lt;p&gt;When dealing with targets in animation, the first word that came into our mind is Inverse Kinematic (IK). And actually, that is a good start to tackle our problem. Indeed, for instance when a character has to grab an item, IK automatically find rotations and positions of the elbow and shoulder (can be more than that) so that the hand reach the target.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now define our problem more accurately. At a given frame of one animation, we want some body part of our character to reach a given target.
As I like to consider an animation as a time function instead of a function of frame, we define $t_t$ our target time and $p_t$ the position of the given target.&lt;/p&gt;
&lt;p&gt;For a given bone, let&amp;rsquo;s say the hand, we compute its trajectory $p(t)$ or ${p_i}$ (discretized) during the whole animation.
One could say that our problem is solved as we just have to call our IK solver at time $t_t$ so that $p(t_t) = p_t$.
However, this solution while satisfying our constraint, induce a high discontinuity in the animation. Moreover, the character&amp;rsquo;s hand reach the target at the given frame without actually taking the trajectory to reach it as the rest of the animation remains unchanged.
Our goal is then to change the overall hand motion so that it feels like the character aims to reach our target.
One formulation of this problem is to also impose that while satisfying the target constraint, our initial animation remains as consistent as possible with respect to the original.
This can be formulated as an as-rigid-as-possible (ARAP) deformation of the hand trajectory. While it was first used to 
&lt;a href=&#34;https://igl.ethz.ch/projects/ARAP/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deform 3D models&lt;/a&gt;, 
&lt;a href=&#34;https://dl.acm.org/doi/10.1145/1531326.1531385&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt; used the same principle but for bones trajectories by noticing each point of one trajectory could be expressed in a local frame of defined by its neighbours:&lt;/p&gt;
&lt;p&gt;$$
\forall i, p_i = p_{i-1} + \alpha_i \vec{A_i} + \beta_i \vec{B_i}
$$&lt;/p&gt;
&lt;p&gt;$$
\text{with } \vec{A_i} = \frac{p_{i+1} - p_{i-1}}{\left \lVert p_{i+1} - p_{i-1} \right \rVert}
$$&lt;/p&gt;





  











&lt;figure id=&#34;figure-expressing-a-point-curve-p_i-with-respect-to-its-two-neighbors-this-point-is-computed-in-its-previous-neighbor-local-frame-directed-by-the-p_i-1p_i1-vector&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Laplacian_formulation.png&#34; data-caption=&#34;Expressing a point curve $P_i$ with respect to its two neighbors. This point is computed in its previous neighbor local frame directed by the $P_{i-1}P_{i&amp;#43;1}$ vector.&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Laplacian_formulation.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Expressing a point curve $P_i$ with respect to its two neighbors. This point is computed in its previous neighbor local frame directed by the $P_{i-1}P_{i+1}$ vector.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This formulation translates the fact that $p_i$ can be expressed in a frame in which the frontal vector $\vec{A_i}$ and left vector $\vec{B_i}$ are contained in the $\hat{ p_{i-1}p_ip_{i+1}}$ plane, with $p_i$ being expressed with respect to its neighbors. Using simple trigonometry it is quite simple to compute $\alpha_i$ and $\beta_i$:&lt;/p&gt;
&lt;p&gt;$$
\alpha_i = dot(p_i - p_{i-1}\vec{A_i})
$$&lt;/p&gt;
&lt;p&gt;$$
\beta_i = \left \lVert cross(p_i - p_{i-1}\vec{A_i}) \right \rVert
$$&lt;/p&gt;
&lt;p&gt;Finally, we can deduce:&lt;/p&gt;
&lt;p&gt;$$
\vec{B_i} = \frac{p_{i} - p_{i-1} - \alpha_i\vec{A_i}}{\beta_i}
$$&lt;/p&gt;
&lt;p&gt;Using this characterization of each point of the bone&amp;rsquo;s trajectory with respect to its neighbors, we seek to conserve each original $\alpha_i$ and $\beta_i$ when deforming the trajectory to address the $ p(t_t) = p_t $ constraint. Let&amp;rsquo;s consider that $p(t_t) = p_k$ for $k \in [1:n]$. We then compute the new bone trajectory as an optimization process minimizing the cost function $L_i$:&lt;/p&gt;
&lt;p&gt;$$ L_i(p_0,&amp;hellip;,p_n) = \sum\limits_j {\left\lVert (\alpha^{init}_j,\beta^{init}_j) - (\alpha_j,\beta_j) \right\rVert}^2 + \gamma {\left\lVert p_k - p_t \right\rVert}^2 $$&lt;/p&gt;
&lt;p&gt;Using a gradient descent method we can update the ${p_i}$ trajectory at each step of the optimization process in which: $\forall j, p_j = p_j -\epsilon\nabla L_i(p_0,&amp;hellip;,p_n)_j$.&lt;/p&gt;
&lt;p&gt;




  











&lt;figure id=&#34;figure-default-slap-animation-which-do-not-reach-the-green-target&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Arlequin_Target_Fail.png&#34; data-caption=&#34;Default Slap animation which do not reach the green target.&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Arlequin_Target_Fail.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Default Slap animation which do not reach the green target.
  &lt;/figcaption&gt;


&lt;/figure&gt;






  











&lt;figure id=&#34;figure-deformed-slap-animation-using-our-optimization-algorithm-making-the-character-reach-the-green-target&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Arlequin_Target_Reach.png&#34; data-caption=&#34;Deformed Slap animation using our optimization algorithm making the character reach the green target.&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/ARAP_Anim/Arlequin_Target_Reach.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Deformed Slap animation using our optimization algorithm making the character reach the green target.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;video autoplay loop muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Animation/Animation_Target_CTR/Slap_Laplacian.mp4&#34; type=&#34;video/mp4&#34;&#34;&gt;
&lt;/video&gt;
</description>
    </item>
    
    <item>
      <title>Animation Transfer</title>
      <link>http://typhomnt.github.io/post/animation_transfer/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/animation_transfer/</guid>
      <description>&lt;p&gt;One main goal of my thesis is to find a way to animate 3D characters using physical props as an animation tool. The motivation is to create an animation sequence from a play with figurines, trying to reproduce what the player was imagining.&lt;/p&gt;
 &lt;!--- (I actually tried to tackle this task when I was still an ENSIMAG student during a 3 weeks project. At that time my first idea was to segment the curve using singular points and try to identify which action was performed by looking at the speed norm and direction. Basically, horizontal motions represented wlaking or running action while the other represented jumps and flying actions.
It is easy to identify the weaknesses of such an approach, it is limited to few actions and above all it is not really accurate, tweaking and thresholding is often needed for identifying singular points.
During my thesis I took inspiration from the paper untitled Motion Doodle where the authors were using 2D curves to created animation sequences . More particulary a given input curve, it was segmented into horizontal, vertical and oblic bins and animation were defined as a regular expression of curve bins. 
At that moment, I took a step back and told myself that they were creating a kind of motion language characterizing the curve by its different direction changes.) --&gt;
&lt;h3 id=&#34;space-time-doodle-transfer-examples&#34;&gt;Space Time Doodle Transfer examples &lt;/h3&gt;
&lt;p&gt;Below, I show transfered space-time doodles into animation sequences. It is important to note that all actions were learnt for the two first examples as well as for the garden example.&lt;/p&gt;
&lt;video autoplay loop muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Animation/Animation_Transfer/PhD_Baseline.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;video autoplay loop muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Animation/Animation_Transfer/Max_Eg.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;video autoplay loop muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Animation/Animation_Transfer/Pierre_Eg.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;video autoplay loop muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/Animation/Animation_Transfer/Remi_Eg.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
</description>
    </item>
    
    <item>
      <title>Base Mesh Creation</title>
      <link>http://typhomnt.github.io/post/bmesh/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/bmesh/</guid>
      <description>&lt;p&gt;This project is a second attemp to implement a fast base mesh creation method from 
&lt;a href=&#34;https://pdfs.semanticscholar.org/2009/3aea25b50e59c63998ba0377371c59bf007f.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt;.
The main idea is to create a animable mesh from a skeleton whose joints are represented by spheres with variable radius defining the distance from the mesh vertices.
This mesh generation algorithm is decomposed into 3 steps: First a simple init mesh is computed connecting each joint with successive quad extrusion and convex hull computation for T-junctions.
The resulting mesh is then refined through an iterative process, alternating subdivisions and evolutions. Finally, an edge fairing optimazation is performed whose puporse is to prevent quad deformation as much as possible without changing the geometry.&lt;/p&gt;
&lt;h3 id=&#34;bmesh-evolution-example&#34;&gt;BMesh Evolution example &lt;/h3&gt;
&lt;video autoplay loop muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/BMesh/B_Mesh_0.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;video autoplay loop muted playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/BMesh/B_Mesh_1.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
</description>
    </item>
    
    <item>
      <title>Coherent Mark-based Stylization of 3D Scenes at the Compositing Stage</title>
      <link>http://typhomnt.github.io/post/npr_splatting/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/npr_splatting/</guid>
      <description>&lt;p&gt;This article describes the mark-based stylization technique that we developped at Inria with Romain Vergne, Mohamed-Amine Farhat Pierre Benard and Joelle Thollot.
This work will be presented as a full paper at the Eurographics conference and publshed in the Compter &amp;amp; Graphics Forum Journal. Our author version of the article can be found 
&lt;a href=&#34;&#34;&gt;here&lt;/a&gt;.
Our technique guarentees temporal coherence and operates at the compositing stage using G-Buffers as input, easing the integration into existing pipelines.
The source code is avaible 
&lt;a href=&#34;&#34;&gt;here&lt;/a&gt; and necessitate that you use Gratin, an open-source node-based compositing software that you can find 
&lt;a href=&#34;http://jcgt.org/published/0004/04/03/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
For windows users, we created an archive containing an executable that you can use right out of the box.&lt;/p&gt;
&lt;p&gt;This post is divided into three parts. First, I will describe the main principle behing our technique, then I will explain how to compile and use the application, then I will detail it works in the code.&lt;/p&gt;
&lt;h3 id=&#34;main-principle&#34;&gt;Main principle&lt;/h3&gt;
&lt;p&gt;In the ocean of NPR techniques, we can distinguish two different families when it come to stylizing color regions: texture-based and mark-based approaches.
The first embeds marks in an image which is then mapped or pached either onto the 3D scene or over the entire screen. Here are some reference of previous research works falling into that category.
More concretly, this kind of technique is quite common when using 3D software like Maya or Blender.
The later consists in attaching 2D or 3D marks (brush strokes) to anchor points distributed onto the 3D scene.&lt;/p&gt;
&lt;p&gt;Our method belongs to the familly of mark-based methods but borrows ideas from texture-based methods.
Moreover, it consists in dynamically generating a motion-coherent 3D anchor point distribution that are drawn as billboard brush strokes.
In order to ensure temporal continuity as well as motion coherence at the compositing stage, the anchor point generation is based on implicit Voronoi noise generated from G-Buffers. This allows our technique to operate  regardless of the geometric representation or animation technique.&lt;/p&gt;
&lt;h4 id=&#34;anchor-point-generation&#34;&gt;Anchor Point Generation&lt;/h4&gt;
&lt;p&gt;Our anchor point generation algorithm is two folds: a first process generates anchor points around the objects based on a 3D Worley noise which takes 3D positions stored in the G-Buffers as input. Then, we group together all pixels that generated the same anchor point and compute its final position in screen-space as the average of the screen-space position of these pixels. To ensure a quasi constant density of the points, especially when zooming in or out, we introduce a fractalization process which generate several layers of points. Moreover, at each pixel of the position G-Buffer, we generate several points using the same process as earlier but with a different frequency for each layer. For each layer its sampling frequency is related to the gradient of the position at the considered pixel such that when the gradient is low, meaning that the surface is flat, we increase the sampling frequency and increase it other wise. The intuition behind this process is to sample more points when the surface is flat because at a given frequency of the Worley noise, more points are generated when the position varies, the surface of the object intersecting more virtual 3D cells than when it is flat.&lt;/p&gt;
&lt;p&gt;Let me show you the difference of our anchor point genereation process with and without the fractalization :&lt;/p&gt;
&lt;p&gt;&lt;video autoplay loop muted playsinline&gt;]
&lt;source src=&#34;http://typhomnt.github.io/Videos/SplatNPR/Fractalization_Zoom.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;
&lt;p&gt;&lt;video autoplay loop muted playsinline&gt;]
&lt;source src=&#34;http://typhomnt.github.io/Videos/SplatNPR/Splat_Generation_Animation_Sub.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;
&lt;h4 id=&#34;improve-temporal-continuity&#34;&gt;Improve Temporal Continuity&lt;/h4&gt;
&lt;p&gt;Until this stage we focused&lt;/p&gt;
&lt;h3 id=&#34;use-the-application&#34;&gt;Use the application&lt;/h3&gt;
&lt;h3 id=&#34;understanding-the-code&#34;&gt;Understanding the code&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Coherent Mark-based Stylization of 3D Scenes at the Compositing Stage</title>
      <link>http://typhomnt.github.io/research/coherent_splat_stylization/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/research/coherent_splat_stylization/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/splat_teaser.jpg&#34; alt=&#34;Teaser&#34;&gt;&lt;/p&gt;
&lt;p&gt;Comptuer Graphics Forum, Eurographics 2021. Article available 
&lt;a href=&#34;https://hal.inria.fr/hal-03143244&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;
&lt;p&gt;Maxime Garcia
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Romain Vergne
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Mohamed-Amine Farhat
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Pierre Benard
(Inria / LaBRI)&lt;/p&gt;
&lt;p&gt;Camille Nous
(Laboratoire Cogitamus)&lt;/p&gt;
&lt;p&gt;Joelle Thollot
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present a novel temporally coherent stylized rendering technique working entirely at the compositing stage. We first generate a distribution of 3D anchor points using an implicit grid based on the local object positions stored in a G-buffer, hence following object motion. We then draw splats in screen space anchored to these points so as to be motion coherent. To increase the perceived flatness of the style, we adjust the anchor points density using a fractalization mechanism. Sudden changes are prevented by controlling the anchor points opacity and introducing a new order-independent blending function. We demonstrate the versatility of our method by showing a large variety of styles thanks to the freedom offered by the splats content and their attributes that can be controlled by any G-buffer.&lt;/p&gt;
&lt;h4 id=&#34;eurographics-fast-forward&#34;&gt;Eurographics Fast Forward&lt;/h4&gt;
&lt;video controls playsinline&gt;
	&lt;source src=&#34;http://typhomnt.github.io/Videos/SplatNPR/FF_Version_2.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;Watch it in full screen for a better experience.&lt;/p&gt;
&lt;p&gt;Models: Female Orca by neurodolphin and Shiny Fish by GenEugen on Sketchfab.&lt;/p&gt;
&lt;h4 id=&#34;main-video&#34;&gt;Main Video&lt;/h4&gt;
&lt;center&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/466IoPKs0p0&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;
&lt;h4 id=&#34;supplementals&#34;&gt;Supplementals&lt;/h4&gt;
&lt;center&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/sxU_ZEjwwjI&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;
&lt;h4 id=&#34;app-and-turorial&#34;&gt;App and Turorial&lt;/h4&gt;
&lt;p&gt;Available soon !&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Functionnal Programming Resources</title>
      <link>http://typhomnt.github.io/teaching/functionnal_programming/functionnal_programming_practs/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/teaching/functionnal_programming/functionnal_programming_practs/</guid>
      <description>&lt;p&gt;This course is proposed to DLST computer science Licence 1 students and presents an introduction to Functionnal Programming in OCaml.&lt;/p&gt;
&lt;h2 id=&#34;practicals&#34;&gt;Practicals&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/L1_Fractales.pdf&#34;&gt;Fractals&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/L1_Fractales_Sol.ml&#34;&gt;Fractals Solution&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/L1_FP_Project.pdf&#34;&gt;Optional Project&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Laban Effort Animation Transfer</title>
      <link>http://typhomnt.github.io/post/laban_transfer/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/post/laban_transfer/</guid>
      <description>&lt;p&gt;One important aspect of my thesis work is adding expressivity to an input &amp;ldquo;neutral&amp;rdquo; animation. In our work, we decided to take the Laban Effort space as the main representation of the expressivity.
This 4D space is divided into Space, Time, Weight and Flow axis. The Space axis describes how direct or indirect a movment is, Time describes if a movment is rather sudden or sustained while Weight differenciate Light from Strong motions. Finally Flow describes if a motion sequence is free or bound. In this article, I will only present how to transfer Time and Weight to a neutral animation.
To do so, I will first introduce three animation operators: scaling, retiming and shaping operators.&lt;/p&gt;
&lt;h3 id=&#34;scaling&#34;&gt;Scaling&lt;/h3&gt;
&lt;h3 id=&#34;efforts-comparison&#34;&gt;Efforts Comparison &lt;/h3&gt;
&lt;p&gt;Finally, I show below the comparison of the 4 efforts for several animations of the Mixamo database
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/kick_laban.gif&#34; alt=&#34;Kick Laban&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/punch_laban.gif&#34; alt=&#34;Punch Laban&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/stomp_laban.gif&#34; alt=&#34;Stomp Laban&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/throw_laban.gif&#34; alt=&#34;Throw Laban&#34;&gt;&lt;/p&gt;
&lt;p&gt;I personnaly think that those modifiers do the job as Laban Effort qualities are recognizable. That remains true as long as the animation phases are correctly defined. Still there are ways of improvment: first, foot sliding should be removed for light and strong modifiers; secondly, light and strong motions are sometimes too fast for some animations. This is due to the fact that some animations present long moments where there is no speed.&lt;/p&gt;
&lt;!---In furture experimentations, I will try to use the equi-affine speed instead of the speed for the retiming operator, taking into account animation breakdowns.--&gt;
&lt;p&gt;Comparisons:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Arm_Gesture_Light.mp4.gif&#34; alt=&#34;Raise Light&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Arm_Gesture_Strong.mp4.gif&#34; alt=&#34;Raise Strong&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Arm_Gesture_Sudden.mp4.gif&#34; alt=&#34;Raise Sudden&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Arm_Gesture_Sustained.mp4.gif&#34; alt=&#34;Raise Sustained&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Punch_Light.mp4.gif&#34; alt=&#34;Punch Light&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Punch_Strong.mp4.gif&#34; alt=&#34;Punch Strong&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Punch_Sudden.mp4.gif&#34; alt=&#34;Punch Sudden&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Punch_Sustained.mp4.gif&#34; alt=&#34;Punch Sustained&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Jump_Light.mp4.gif&#34; alt=&#34;Jump Light&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Jump_Strong.mp4.gif&#34; alt=&#34;Jump Strong&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Jump_Sudden.mp4.gif&#34; alt=&#34;Jump Sudden&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Jump_Sustained.mp4.gif&#34; alt=&#34;Jump Sustained&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Flip_Light.mp4.gif&#34; alt=&#34;Flip Light&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Flip_Strong.mp4.gif&#34; alt=&#34;Flip Strong&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Flip_Sudden.mp4.gif&#34; alt=&#34;Flip Sudden&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Flip_Sustained.mp4.gif&#34; alt=&#34;RaFlipise Sustained&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Stomp_Light.mp4.gif&#34; alt=&#34;Stomp Light&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Stomp_Strong.mp4.gif&#34; alt=&#34;Stomp Strong&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Stomp_Sudden.mp4.gif&#34; alt=&#34;Stomp Sudden&#34;&gt;
&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Transfer/Stomp_Sustained.mp4.gif&#34; alt=&#34;Stomp Sustained&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ray Tracing Resources</title>
      <link>http://typhomnt.github.io/teaching/ray_tracing/raytracing_practs/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/teaching/ray_tracing/raytracing_practs/</guid>
      <description>&lt;p&gt;This course is proposed to MSIAM M1 students and presents an introduction to Ray casting, Ray tracing, Ray Marching and Physically Based Rendering methods.
I would like to thank Romain Vergne for his course slides and the two first practicals.&lt;/p&gt;
&lt;h2 id=&#34;course-slides&#34;&gt;Course Slides&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/MSIAM_Course1__Ray_Casting.pdf&#34;&gt;Ray Casting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/MSIAM_Course2__Ray_Tracing.pdf&#34;&gt;Ray Tracing/Ray Marching&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/MSIAM_Course3__PBR.pdf&#34;&gt;Physically Based Rendering&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;tutorials&#34;&gt;Tutorials&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;../pbr_intro&#34;&gt;Physically Based Rendering&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;../raymarching_intro&#34;&gt;Ray Marching&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;practicals&#34;&gt;Practicals&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/MSIAM_Code.zip&#34;&gt;Base Code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/MSIAM_TP1__Ray_Casting.pdf&#34;&gt;Ray Casting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/MSIAM_TP2__Ray_Tracing.pdf&#34;&gt;Ray Tracing&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://typhomnt.github.io/Files/MSIAM_TP4__Ray_Marching.pdf&#34;&gt;Ray Marching&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recognition of Laban Effort Qualities from Hand Motion</title>
      <link>http://typhomnt.github.io/research/recognition_laban_hand/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/research/recognition_laban_hand/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Classification/effort_cube.jpg&#34; alt=&#34;Teaser&#34;&gt;&lt;/p&gt;
&lt;p&gt;MOCO&#39;20 - 7th International Conference on Movement and Computing, Jul 2020, Jersey City/ Virtual, United States. Article available 
&lt;a href=&#34;https://hal.inria.fr/hal-02899999&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;
&lt;p&gt;Maxime Garcia
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Remi Ronfard
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this paper, we conduct a study for recognizing motion qualities in hand gestures using virtual reality trackers attached to the hand. From this 6D signal, we extract Euclidean, equi-affine and moving frame features and compare their effectiveness in the task of recognizing Laban Effort qualities. Our experimental results reveal that equi-affine features are highly discriminant features for this task. We also compare two classification methods on this task. In the first method, we trained separate HMM models for the 6 Laban Effort qualities (light, strong, sudden, sustained, direct, indirect). In the second method, we trained separate HMM models for the 8 Laban motion verbs (dab, glide, float, flick, thrust, press, wring, slash) and combined them to recognize individual qualities. In our experiments, the second method gives improved results. Together, those findings suggest that low-dimensional signals from VR trackers can be used to predict motion qualities with reasonable precision.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SkyEngine</title>
      <link>http://typhomnt.github.io/project/skyengine/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/project/skyengine/</guid>
      <description>&lt;!--&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/SkyEngine/SkyLogoMob.png&#34; alt=&#34;SkyLogo&#34; style=&#34;width:200px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;p&gt;SkyEngine is yet another game engine I am working on together with co-workers and former collegues. The main goal of this engine is to implement recent research work, to make video game and animated movie content easier to create. I would like to thank Julien Daval, Guillaume Cordonnier, Valentin Touzeau, Pierre Casati and Remi Galan Alfonso for contributing to this framework.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spatial Motion Doodles: Sketching Animation in VR Using Hand Gestures and Laban Motion Analysis</title>
      <link>http://typhomnt.github.io/research/spatial_motion_doodle/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/research/spatial_motion_doodle/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Animation_Transfer/SMD_Teaser.png&#34; alt=&#34;Teaser&#34;&gt;&lt;/p&gt;
&lt;p&gt;ACM Motion, Interaction and Games 2019 (MIG &amp;lsquo;19). Article available 
&lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-02303803/document&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;authors&#34;&gt;Authors&lt;/h2&gt;
&lt;p&gt;Maxime Garcia
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Remi Ronfard
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Marie-Paule Cani
(Ecole Polythecnique / LIX)&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present a method for easily drafting expressive character animation by playing with instrumented rigid objects. We parse the input 6D trajectories (position and orientation over time) – called spatial motion doodles – into sequences of actions and convert them into detailed character animations using a dataset of parameterized motion clips which are automatically fitted to the doodles in terms of global trajectory and timing. Moreover, we capture the expressiveness of user-manipulation by analyzing Laban effort qualities in the input spatial motion doodles and transferring them to the synthetic motions we generate. We validate the ease of use of our system and the expressiveness of the resulting animations through a series of user studies, showing the interest of our approach for interactive digital storytelling applications dedicated to children and non-expert users, as well as for providing fast drafting tools for animators.&lt;/p&gt;
&lt;center&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/0xG2dlGg_9M&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;
&lt;center&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/5BDqJQ6XK_k&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to Physically Based Rendering</title>
      <link>http://typhomnt.github.io/teaching/ray_tracing/pbr_intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://typhomnt.github.io/teaching/ray_tracing/pbr_intro/</guid>
      <description>&lt;p&gt;This tutorial is inspired from 
&lt;a href=&#34;https://learnopengl.com/PBR/Theory&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://learnopengl.com/PBR/Theory&lt;/a&gt; and adapted for the ray-tracing course available 
&lt;a href=&#34;../raytracing_practs&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;A big challenge in computer graphics is to design shading models mimicking real-life lighting behaviour while allowing intuitive control of object materials. This control is crucial for artists who are creating assets that have to be integrated in a rendering pipeline. For real-time applications like video games, the question of computation time and memory cost is also essential; such models must be flexible enough and allow affordable approximations.&lt;/p&gt;
&lt;p&gt;In this tutorial, we are interested in Physically Based Rendering (PBR) models which aim at simulating light behaviour in a more realistic way, approximating light related equations (models like the Phong model are very simplistic in comparison).
One important aspect of those models is their energy conservative property stating that when interacting with a surface, the amount of outgoing light is equal to the amount of incoming light. More precisely, the amount of light absorbed, scattered and diffused by object surfaces is equal to the amount of light hitting the surface. The figure below illustrates these phenomena.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/LightInteraction.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/LightInteraction.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;!-- &lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/LightInteraction.png&#34; alt=&#34;LightInteract&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;h2 id=&#34;surface-representation-micro-facet-model&#34;&gt;Surface Representation: Micro-Facet model&lt;/h2&gt;
&lt;p&gt;Simulating this behaviour is highly correlated with how we represent object surfaces. Indeed, light interaction with object surfaces is modeled by the Snell-Descartes law which describes how incident light gets refracted and reflected on a flat surface.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/snelldescartes.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/snelldescartes.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;!-- &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/snelldescartes.png&#34; alt=&#34;SnellDescartes&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;p&gt;However, in practice object surfaces are not completely flat, some are rougher than others. This is something noticeable in real life, especially when you look at specular reflection on different objects. More precisely, rough surfaces tend to produce blurred reflections while smooth surfaces behave like mirrors.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Rough Surface&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Smooth Surface&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;




  











&lt;figure id=&#34;figure-rough-surface&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/rough_photo.png&#34; data-caption=&#34;Rough Surface&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/rough_photo.png&#34; alt=&#34;&#34; width=&#34;300px&#34; &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Rough Surface
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;




  











&lt;figure id=&#34;figure-smooth-surface&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/smooth_photo.png&#34; data-caption=&#34;Smooth Surface&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/smooth_photo.png&#34; alt=&#34;&#34; width=&#34;300px&#34; &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Smooth Surface
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Thus, the microfacet model was introduced and defines a surface by a continuous sequence of flat micro-surfaces that might be oriented differently, simulating the smooth vs rough aspect of macro-surfaces. The roughness property of a material plays a meaningful role in light behavior as it controls the amount of light that gets reflected and refracted as well as the direction of outgoing light.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Rough Surface&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Smooth Surface&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/roughsurface.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/smoothsurface.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;light-equation&#34;&gt;Light Equation&lt;/h2&gt;
&lt;p&gt;Having stated all this principles, our main goal remains unchanged, compute the color received by the eye (camera) for each pixel of the output image displayed on our screen. More specifically, we are interested in the color and intensity of light that either gets directly reflected from the surface to the eye or that gets refracted and then re-emitted by the object through diffusion (considering that all the light that gets absorbed is lost). On the other hand, in this tutorial we neglect the effect of scattering which gives more realistic results (especially useful when rendering skin) but is more costly to compute.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/normalsurface.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/normalsurface.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;!-- &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/normalsurface.png&#34; alt=&#34;NormalSurf&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;p&gt;The amount of light that gets into a specific direction from a given point on an object surface is governed by laws of physics and more specifically it is given by the reflectance equation:&lt;/p&gt;
&lt;p&gt;$$ L_o(p,v) = \int_A f_r(p,l,v,\alpha_p) L_i(p,l)(n \cdot l)dl $$&lt;/p&gt;
&lt;p&gt;Where $p$ is the point of interest on the object surface (receiving light), $v$ the view direction from $p$ to the eye, $l$ the incident light direction, $\alpha_p$ the surface roughness at point $p$, $L_o$ the output light radiance (stored as a RGB color) perceived by our eye from $p$, $L_i$ the incident light radiance (also stored as a RGB color) gathering at $p$ from direction l, $f_r$ a function controlling the amount of light reflected to direction $v$ with respect to the material property at $p$ and $A$ the hemisphere surrounding $p$ on which we integrate all incoming light directions.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/AreaIntegrate.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/AreaIntegrate.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;!-- &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/AreaIntegrate.png&#34; alt=&#34;AreaInt&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;p&gt;In this tutorial, we further restrict incoming light sources to a given number of point light sources. Thus, the integral over $A$ can be transformed into a sum over the different light sources.&lt;/p&gt;
&lt;p&gt;$$ L_o(p,v) = \sum_l f_r(p,p-c_l,v,\alpha_p) L_i(p,p-c_l)( n \cdot \frac{(c_l-p)}{\left\lVert   c_l - p \right\rVert})  $$&lt;/p&gt;
&lt;p&gt;The incoming reflectance $L_i(p,p-c_l)$ equals to the intensity of the light source $I$, multiplied by its color $C$, and weighted by an attenuation factor which depends on the distance to the source.
In our case, we will consider that the intensity of light decrease with the square distance to the source:&lt;/p&gt;
&lt;p&gt;$$L_i(p,p-c_l) = \frac{IC}{{\left\lVert p - c_l\right\rVert}^2} $$&lt;/p&gt;
&lt;h2 id=&#34;the-bidirectional-reflective-distribution-function-brdf&#34;&gt;The Bidirectional Reflective Distribution Function (BRDF)&lt;/h2&gt;
&lt;p&gt;The only unknown left is the $f_r$ function that controls the amount of reflected light with respect to materials properties. This function is called a BRDF which stands for Bidirectional Reflective Distribution Function. Several functions were proposed to simulate real-life materials behavior, all of them respect the energy conservation law, meaning that the amount of outgoing light do not exceed the amount of incoming light and above all the later is divided between reflected and refracted light. Another important property of the BRDF functions is that there are intresectly symmetric with respect to incoming and outgoing light because of the principle of reversibility of light.
In our case, we will use the Cook-Torrance BRDF model composed of a diffuse and a specular part:&lt;/p&gt;
&lt;p&gt;$$f_r = k_d f_l + k_s f_c$$&lt;/p&gt;
&lt;p&gt;where $k_d$ is the amount of refracted light that gets re-emitted and $k_s$ the amount of reflected light with:&lt;/p&gt;
&lt;p&gt;$$ k_d = 1 - k_s $$&lt;/p&gt;
&lt;p&gt;The $f_{l}$ function is the Lambertian diffusion distribution (which corresponds to the diffuse part of the Phong model). It considers that the diffused light is equally spread on all direcion:
$$ f_l = \frac{C}{\pi} $$
Where C is the albedo of the object surface at point $p$. We can notice that the dot product between the normal and the light direction is done outside this function and is still present in the sum of in going contribution. $\pi$ is a normalization factor which accounts for the fact that we integrate ingoing light over the hemisphere at point $p$.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/diffuse.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/diffuse.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;!--  &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/diffuse.png&#34; alt=&#34;Diffuse&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;p&gt;At this point it is important to mention that we must differentiate between two type of material: &lt;strong&gt;metals&lt;/strong&gt; and &lt;strong&gt;dielectric&lt;/strong&gt; (non metal) materials.
Indeed, while dielectric materials diffuse light, it is not the case of metals that absorb all refracted light. As a consequence, $k_d = 0$ for metals (with $k_s \leq 1$ because light still get refracted).&lt;/p&gt;
&lt;p&gt;On the other side the $f_c$ is composed of two terms:&lt;/p&gt;
&lt;p&gt;$$f_c = \frac{DG}{4(l \cdot n)(v \cdot n)}$$&lt;/p&gt;
&lt;p&gt;with D called the Normal Distribution Function and G the Geometry function. Additionally, $k_s = F$, where $F$ is the Fresnel term describing the amount of light that gets refracted on a more macroscopic scale with respect to the view direction. The Fresnel term results from an equation that is not easy to solve, however it can be approximated using the Fresnel-Schlick approximation:&lt;/p&gt;
&lt;p&gt;$$ F_{Schlick}(h, v, F_0) = F_0 + (1 - F_0) (1 - (h \cdot v))^5 $$&lt;/p&gt;
&lt;p&gt;With $F_0$ being the base reflectivity of the material and $h = \frac{l + v}{\left\lVert l + v \right\rVert}$ the half vector which corresponds to the normal one facet must have to directly reflect the light into the eye.&lt;/p&gt;
&lt;p&gt;This equation tells us that when $h$ and $v$ are perpendicular the amount of reflected light is at its maximum, in other words reflections occurs more at grazing angles. This effect is especially noticeable on puddles or wooden surfaces when looking from a top view or from a grazing angle.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/Fresnel_Photo.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/Fresnel_Photo.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;!-- &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/Fresnel_Photo.png&#34; alt=&#34;FresnelPhoto&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;p&gt;$F_0$ is computed as the amount of reflected light at normal incidence where $v$ and $l$ are collinear.
It is important to note that this equation can only be applied to dielectric materials, especially because metallic materials absorb all refracted light. However, as $F_0$ for dielectric materials is usually low and  high for metallic materials, a common approximation is to use a common average $F_0$ for dielectic materials and the metal color as $F_0$ for metallic materials. This is plausible because metallic $F_0$ are tinted and give to metals their color. Furthermore, following the metallic workflow we will consider that being metallic or dielectric is not a binary feature, meaning that one material can be sem-metallic with its metalness varying from $0$ to $1$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt;  One might notice that $h$ is replaced by $n$ in the original equation. This is perfectly right in a macroscopic point of view. However, in our case we look at reflections in a microscopic scale, meaning that the normal of the surface is determined by microfacet normals. Additionally the only case where the light is reflected into our eye is when $n = h$ which justify our use of h in this case. This property is further used to approximate the BRDF final expression.&lt;/p&gt;
&lt;/blockquote&gt;





  











&lt;figure id=&#34;figure-fresnel-coefficient-length-computed-on-our-test-scene-see-below-the-final-amount-has-been-remapped-for-visualisation-purpose&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/Fresnel_Vis.png&#34; data-caption=&#34;Fresnel coefficient length computed on our test scene (see below). The final amount has been remapped for visualisation purpose.&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/Fresnel_Vis.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fresnel coefficient length computed on our test scene (see below). The final amount has been remapped for visualisation purpose.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!--  &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/Fresnel_Vis.png&#34; alt=&#34;FresnelEg&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Fresnel coefficient length computed on our test scene (see below). The final amount has been remapped for visualisation purpose.&lt;/div&gt; --&gt;
&lt;p&gt;Let us describe the microfacet model in more details and focus on the roughness parameter that plays a role in the amount of reflected light. More concretely, this parameter describes the amount of micro-facet that are aligned in the same direction; in particular, rough surfaces have a chaotic orientation distribution while smooth surfaces facets are oriented in a single direction (the normal).&lt;/p&gt;
&lt;p&gt;As microfacets represent the surface of the object, their orientation directly affects the direction of reflected light.
That&amp;rsquo;s why smooth surfaces typically behave like mirrors while reflections appear blurrier on rough surfaces.&lt;/p&gt;
&lt;p&gt;We can now describe the role of the Normal Distribution Function D and the Geometric function G.
D represent the amount of microfacet that are aligned with the half vector $h$. This is the same as computing the amount of reflected light rays that are collinear to the view vector $v$.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/ndf_halfvector.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/ndf_halfvector.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;!-- &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/ndf_halfvector.png&#34; alt=&#34;NDFHalfVector&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt; --&gt;
&lt;p&gt;In our case, we chose the GGX distribution function as NDF:&lt;/p&gt;
&lt;p&gt;$$D = NDF_{GGX TR}(n, h, \alpha) = \frac{\alpha^2}{\pi((n \cdot h)^2 (\alpha^2 - 1) + 1)^2}$$&lt;/p&gt;





  











&lt;figure id=&#34;figure-ggx-normal-distribution-function-computed-on-our-test-scene-with-varying-roughness-left-rough-surface-right-smooth-surface&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/NDF_Vis.png&#34; data-caption=&#34;GGX Normal Distribution Function computed on our test scene with varying roughness (left: rough surface. right: smooth surface)&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/NDF_Vis.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    GGX Normal Distribution Function computed on our test scene with varying roughness (left: rough surface. right: smooth surface)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/NDF_Vis.png&#34; alt=&#34;NDFEg&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;GGX Normal Distribution Function computed on our test scene with varying roughness (left: rough surface. right: smooth surface) &lt;/div&gt; --&gt;
&lt;p&gt;This function behaves like a diract when $\alpha = 0$ (smooth surface) and becomes flatter and flatter when $\alpha$ increases until it reaches a constant function $\frac{1}{\pi}$. That is why this formula produces small rounded highlight on smooth surfaces which are completely spread across the object on rough surfaces.&lt;/p&gt;
&lt;p&gt;The Geometric function simulates two phenomena that occurs between micro-facets namely obstruction and shadowing. In the two cases, either the incoming light cannot reach some micro-facets because there are in the shadows of others (shadowing) or reflected light is blocked by other facets (obstruction).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Shadowing&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Masking&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/shadowing.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/masking.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Therefore, some amount of reflected light is &amp;ldquo;lost&amp;rdquo; and this is exactly the information given by the Geometric function. In our case we chose the Schlick-GGX approximation:&lt;/p&gt;
&lt;p&gt;$$    G_S(n, v, k) =  \frac{n \cdot v}{(n \cdot v)(1 - k) + k }  $$&lt;/p&gt;
&lt;p&gt;Taking into account the two effects:&lt;/p&gt;
&lt;p&gt;$$   G(n,v,l,k) =  G_S(n, v, k)  G_S(n, l, k) $$&lt;/p&gt;
&lt;p&gt;This function intuitively models the fact that at grazing angle with respect to the normal, either incoming light rays of reflected rays have a chance to collide with other facets. This probability is gets higher when the roughness of the surface increases, which is pretty intuitive because the micro-facet gets more chaotic and do not face a single direction.&lt;/p&gt;





  











&lt;figure id=&#34;figure-geometry-function-computed-on-our-test-scene-left-rough-surface-right-smooth-surface&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/Geometry_Vis.png&#34; data-caption=&#34;Geometry function computed on our test scene (left: rough surface. right: smooth surface).&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/Geometry_Vis.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Geometry function computed on our test scene (left: rough surface. right: smooth surface).
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!--  &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/Geometry_Vis.png&#34; alt=&#34;GeometryEg&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Geometry function computed on our test scene (left: rough surface. right: smooth surface).&lt;/div&gt; --&gt;
&lt;p&gt;In this tutorial we made choices for approximation functions but several others can be found in the literature (see &lt;strong&gt;References&lt;/strong&gt; at the bottom of the page), I invite you to take a look at BRDF comparison articles.&lt;/p&gt;
&lt;h2 id=&#34;coding-time&#34;&gt;Coding Time&lt;/h2&gt;
&lt;p&gt;We are now ready to dive into the code. The base code can be found 
&lt;a href=&#34;http://typhomnt.github.io/Files/MSIAM_Code.zip&#34;&gt;here&lt;/a&gt;.
First, lets go back to our main function. We are working inside a fragment shader displaying a simple quad perfectly fitting our window. For each pixel we cast rays from a virtual camera to the current pixel whose screen coordinates are given by the in variable $fragCoord$.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;in vec2 fragCoord;
void main()
{   
    Ray ray = generatePerspectiveRay(fragCoord);
    outColor = vec4(trace(ray),1);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside the fragment shader, I passed the view matrix $V$ of the trackball that is available from the transform.py file and used it to move the camera.
Notice the inverse operator applied on V as we want to recover the position of the camera in world space.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;uniform mat4 V;
Ray generatePerspectiveRay(in vec2 p)
{
    // p is the current pixel coord, in [-1,1]
    float fov = 30; // Half angle
    float D = 1./tan(radians(fov));
    mat4 inv_view = inverse(V); // Get the matrix of the trackball

    vec3 up = vec3(0,1,0);
    vec3 front = vec3(0,0,-1);
    vec3 right = cross(up,front);
    return  Ray((inv_view*vec4(0,0,-D,1)).xyz,mat3(inv_view)*normalize(p.x*right + p.y*up*aspectRatio + D*front));
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we declare two new structures: the HitSurface structure containing a ray-scene intersection point, the surface normal at this point as well as its material properties expressed as a PBRMat.
As mentioned above these materials contain roughness and metallic properties. We also add an ambient occlusion property which is used when computing an ambient lighting term in the rendering loop.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;struct PBRMat
{
    vec3 color;
    float roughness;
    float metallic;
    float ao;
};

struct HitSurface
{
    vec3 hit_point;
    vec3 normal;
    PBRMat material;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Back to the trace function. We declare $accum$ as the color of the pixel that is incremented at each ray bounce, the $mask$ variable indicates the intensity of the current ray which gradually decrease at each bounce. For each step we compute the intersection between the ray and the objects in the scene and store the nearest intersected object (with a positive distance) in the $io$ variable.&lt;/p&gt;
&lt;p&gt;The material used for the intersected object is chosen with respect to its index which is used to sample from an array of materials. We then compute the normal and the local illumination of the object
at the intersection point inside the directIllumination function. The reflection intensity factor $c_{refl}$ is updated inside this function. Note, that the reflected ray origin is slightly shifted in the normal direction to avoid  wrong self intersection. This tip is also applied when computing the shadow ray.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vec3 trace(in Ray r)
{
    vec3 accum = vec3(0.0f);
    vec3 mask = vec3(1.0f);
    int nb_refl = 2; // Bounce number
    float c_refl = 1.0f;
    Ray curr_ray = r;
    for(int i = 0 ; i &amp;lt;= nb_refl ; i++)
    {
        ISObj io = intersectObjects(curr_ray);
        if(io.t &amp;gt;= 0)
        {
            PBRMat mat = pbr_mat[io.i];

            HitSurface hs = HitSurface(curr_ray.ro + io.d*curr_ray.rd, computeNormal(io,curr_ray),mat);               

            vec3 color = directIllumination(hs,curr_ray,c_refl);
            accum = accum + mask * color;
            mask = mask*c_refl;
            curr_ray  = Ray(hs.hit_point + 0.001*hs.normal,reflect(curr_ray.rd,hs.normal));
        }
        else
        {
            break;
        }
    }

    return accum;

}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside the directIllumination function, we iterate through each light of the scene and determine if the current intersection point is in the shadow of an object (which can be it self) by casting a ray in the direction of the light. If the intersection point is actually lighted, we compute its radiance inside the PBR function. Otherwise we assign an ambient color weighted by the ambient occlusion factor of the material. Finally we update the reflection intensity factor by computing the length of th Fresnel term.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vec3 directIllumination(in HitSurface hit,in Ray r,inout float refl)
{

    vec3 color = vec3(0);
    for(int i = 0 ; i &amp;lt; light_nbr ; i++)
    {
        Ray l_ray = lightRay(hit.hit_point,lights[i]);
        l_ray.ro = hit.hit_point + 0.001*hit.normal;
        ISObj io;
        io = intersectObjects(l_ray);
        float d_light = lightDist(hit.hit_point,lights[i]);

        if(io.t &amp;lt; 0 || (io.t &amp;gt;= 0 &amp;amp;&amp;amp; (io.d &amp;gt;= d_light)))
        {
            color += PBR(hit,r,lights[i]);
        }
        else
        {
            color +=  vec3(0.03) * hit.material.color * hit.material.ao;
        }


        vec3 Ve = normalize(r.ro - hit.hit_point);
        vec3 H = normalize(Ve + l_ray.rd);
        refl = length(fresnelSchlick(max(dot(H, Ve), 0.0),  mix(vec3(0.04), hit.material.color, hit.material.metallic)))*hit.material.ao;
    }

    return color;
}

struct Light 
{
    int type; // 0 dir light, 1 point light
    vec3 dir; // directionnal light
    vec3 center; // point light
    float intensity; // 1 default
    vec3 color; // light color
};

Ray lightRay(in vec3 ro, in Light l) //computes ro to light source ray
{
    if(l.type == 0)
        return Ray(ro,normalize(l.dir));
    else if(l.type == 1)
        return Ray(ro,normalize(l.center - ro));

    return Ray(ro,vec3(1));
 }

float lightDist(in vec3 ro, in Light l) //computes distance to light
{ 
    if(l.type == 0)
         return DIST_MAX;
    else if(l.type == 1)
        return length(l.center - ro);

    return DIST_MAX;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, it is time to actually implement the PBR direct illumination function approximating the light equation.
This is done in two steps. We first compute the ambient term and the material $F_0$. We then compute the normal of the surface at the intersection point, the view vector, the half vector, the light intensity and direction and pass them to the computeReflectance function inside of which we compute the actual output radiance.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vec3 PBR(in HitSurface hit, in Ray r , in Light l)
{
    vec3 ambient = vec3(0.03) * hit.material.color * (1.0 - hit.material.ao);
    //Average F0 for dielectric materials
    vec3 F0 = vec3(0.04);
    // Get Proper F0 if material is not dielectric
    F0 = mix(F0, hit.material.color, hit.material.metallic);
    vec3 N = normalize(hit.normal);
    vec3 Ve = normalize(r.ro - hit.hit_point);

    float intensity = 1.0f;
    if(l.type == 1)
    {
        float l_dist = lightDist(hit.hit_point,l);
        intensity = l.intensity/(l_dist*l_dist);
    }
    vec3 l_dir = lightRay(hit.hit_point,l).rd;
    vec3 H = normalize(Ve + l_dir);
    return ambient + computeReflectance(N,Ve,F0,hit.material.color,l_dir,H,l.color,intensity,hit.material.metallic,hit.material.roughness);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then define all the BRDF functions we mentionned above:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;float DistributionGGX(vec3 N, vec3 H, float roughness)
{
    float a      = roughness*roughness;
    float a2     = a*a;
    float NdotH  = max(dot(N, H), 0.0);
    float NdotH2 = NdotH*NdotH;

    float nom   = a2;
    float denom = (NdotH2 * (a2 - 1.0) + 1.0);
    denom = PI * denom * denom;

    return nom / denom;
}

float GeometrySchlickGGX(float NdotV, float roughness)
{
    float r = (roughness + 1.0);
    float k = (r*r) / 8.0;

    float nom   = NdotV;
    float denom = NdotV * (1.0 - k) + k;

    return nom / denom;
}

float GeometrySmith(vec3 N, vec3 V, vec3 L, float roughness)
{
    float NdotV = max(dot(N, V), 0.0);
    float NdotL = max(dot(N, L), 0.0);
    float ggx2  = GeometrySchlickGGX(NdotV, roughness);
    float ggx1  = GeometrySchlickGGX(NdotL, roughness);

    return ggx1 * ggx2;
}

vec3 fresnelSchlick(float cosTheta, vec3 F0)
{
    return F0 + (1.0 - F0)*pow((1.0 + 0.000001/*avoid negative approximation when cosTheta = 1*/) - cosTheta, 5.0);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we compute the outgoing radiance as the product of the incoming radiance, the BRDF terms and the dot product between the normal and the light direction.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vec3 computeReflectance(vec3 N, vec3 Ve, vec3 F0, vec3 albedo, vec3 L, vec3 H, vec3 light_col, float intensity, float metallic, float roughness)
{
    vec3 radiance =  light_col * intensity; //Incoming Radiance

    // cook-torrance brdf
    float NDF = DistributionGGX(N, H, roughness);
    float G   = GeometrySmith(N, Ve, L,roughness);
    vec3 F    = fresnelSchlick(max(dot(H, Ve), 0.0), F0);

    vec3 kS = F;
    vec3 kD = vec3(1.0) - kS;
    kD *= 1.0 - metallic;

    vec3 nominator    = NDF * G * F;
    float denominator = 4 * max(dot(N, Ve), 0.0) * max(dot(N, L), 0.0) + 0.00001/* avoid divide by zero*/;
    vec3 specular     = nominator / denominator;


    // add to outgoing radiance Lo
    float NdotL = max(dot(N, L), 0.0);
    vec3 diffuse_radiance = kD * (albedo)/ PI;

    return (diffuse_radiance + specular) * radiance * NdotL;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it, we set up everything to compute a more realistic shading of our scene, you can already produce new raytraced results using this setup.&lt;/p&gt;
&lt;p&gt;Below, are rendering examples of the same scene with varying materials. It contains a single white point Light of intensity $I = 40$ and located at (0,0,0). Left to right spheres roughness are 1, 0.9, 0.7, 0.5, 0.3, 0.1 with (.1,.2,.8) as color and are located at (2.5,0,-2), (1.5,0,-2), (0.5,0,-2), (-0.5,0,-2), (-1.5,0,-2) and (-2.5,0,-2) with a radius of 0.3.&lt;/p&gt;





  











&lt;figure id=&#34;figure-dielectric-materials-without-reflection-metalness--0&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/dielec_no_refl.png&#34; data-caption=&#34;Dielectric materials without reflection (metalness = 0)&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/dielec_no_refl.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Dielectric materials without reflection (metalness = 0)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/dielec_no_refl.png&#34; alt=&#34;DielectNoRefl&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Dielectric materials without reflection (metalness = 0)&lt;/div&gt; --&gt;





  











&lt;figure id=&#34;figure-dielectric-materials-with-reflection-metalness--0&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/dielec_refl.png&#34; data-caption=&#34;Dielectric materials with reflection (metalness = 0)&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/dielec_refl.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Dielectric materials with reflection (metalness = 0)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/dielec_refl.png&#34; alt=&#34;DielectRefl&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Dielectric materials with reflection (metalness = 0)&lt;/div&gt; --&gt;





  











&lt;figure id=&#34;figure-metallic-materials-without-reflection-metalness--1&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/metal_no_refl.png&#34; data-caption=&#34;Metallic materials without reflection (metalness = 1)&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/metal_no_refl.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Metallic materials without reflection (metalness = 1)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/metal_no_refl.png&#34; alt=&#34;MetalNoRefl&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Metallic materials without reflection (metalness = 1)&lt;/div&gt; --&gt;





  











&lt;figure id=&#34;figure-metallic-material-with-reflection-metalness--1&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/metal_refl.png&#34; data-caption=&#34;Metallic material with reflection (metalness = 1)&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/metal_refl.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Metallic material with reflection (metalness = 1)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/metal_refl.png&#34; alt=&#34;MetalRefl&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Metallic material with reflection (metalness = 1)&lt;/div&gt; --&gt;
&lt;h2 id=&#34;one-last-step-hdr-and-gamma-correction&#34;&gt;One last step: HDR and Gamma correction&lt;/h2&gt;
&lt;p&gt;So far we are able to render scenes in a more realistic way. Still, you can notice that if you choose a high intensity for your lights, the results also gets highly saturated, which is already the case in the above examples. This situation is to be expected because radiance can have value bigger than 1. Consequently, we need to find a way to account for High Dynamic Range (HDR) of radiance and process the computed values such that everything is remapped between 0 and 1. I won&amp;rsquo;t enter into details here as it is not the subject of this tutorial. We basically use the Reinhard operator to correct our input colors, remapping them inside the $[0;1]^3$ space with the function $C = \frac{C}{C+1}$.&lt;/p&gt;
&lt;p&gt;Finally, we also need to take into account the color shift that screens produce in response to color values and depending on what we call their gamma value defining this response. Hence, we proceed to a gamma correction of the output colors which rectifies the shift. Again this will be covered in another tutorial, however be careful that this gamma value depends on the monitor configuration you are using.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vec3 trace()
{
    vec3 accum = vec3(0.0);
    ...

    //HDR
    accum = accum / (accum+ vec3(1.0));
    //Gamma
    float gamma = 2.2;
    accum = pow(accum, vec3(1.0/gamma));
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the corrected images of the previous examples with different gamma values.&lt;/p&gt;





  











&lt;figure id=&#34;figure-dielectric-materials-with-reflection-metalness--0-hdr-and-gamma--1&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/dielec_HDR_GAMMA_104.png&#34; data-caption=&#34;Dielectric materials with reflection (metalness = 0) HDR and Gamma = 1&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/dielec_HDR_GAMMA_104.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Dielectric materials with reflection (metalness = 0) HDR and Gamma = 1
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/dielec_HDR_GAMMA_104.png&#34; alt=&#34;MetalRefl&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Dielectric materials with reflection (metalness = 0) HDR and Gamma = 1&lt;/div&gt; --&gt;





  











&lt;figure id=&#34;figure-dielectric-materials-with-reflection-metalness--0-hdr-and-gamma--22&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/dielec_HDR_GAMMA_2.png&#34; data-caption=&#34;Dielectric materials with reflection (metalness = 0) HDR and Gamma = 2.2&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/dielec_HDR_GAMMA_2.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Dielectric materials with reflection (metalness = 0) HDR and Gamma = 2.2
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/dielec_HDR_GAMMA_2.png&#34; alt=&#34;MetalRefl&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Dielectric materials with reflection (metalness = 0) HDR and Gamma = 2.2&lt;/div&gt; --&gt;





  











&lt;figure id=&#34;figure-metallic-material-with-reflection-metalness--1-hdr-and-gamma--1&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/metal_HDR_GAMMA_104.png&#34; data-caption=&#34;Metallic material with reflection (metalness = 1) HDR and Gamma = 1&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/metal_HDR_GAMMA_104.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Metallic material with reflection (metalness = 1) HDR and Gamma = 1
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/metal_HDR_GAMMA_104.png&#34; alt=&#34;MetalRefl&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Metallic material with reflection (metalness = 1) HDR and Gamma = 1&lt;/div&gt; --&gt;





  











&lt;figure id=&#34;figure-metallic-material-with-reflection-metalness--1-hdr-and-gamma--22&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/metal_HDR_GAMMA_2.png&#34; data-caption=&#34;Metallic material with reflection (metalness = 1) HDR and Gamma = 2.2&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/metal_HDR_GAMMA_2.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Metallic material with reflection (metalness = 1) HDR and Gamma = 2.2
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/metal_HDR_GAMMA_2.png&#34; alt=&#34;MetalRefl&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Metallic material with reflection (metalness = 1) HDR and Gamma = 2.2&lt;/div&gt; --&gt;
&lt;p&gt;You can also play with the different parameters and get various materials:&lt;/p&gt;





  











&lt;figure id=&#34;figure-various-materials-gamma--1&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/Final_Eg.png&#34; data-caption=&#34;Various materials, Gamma = 1&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/PBR_Intro/Final_Eg.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Various materials, Gamma = 1
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- &lt;div style=&#34;width:image width px; font-size:100%; text-align:center;&#34;&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;http://typhomnt.github.io/Images/PBR_Intro/Final_Eg.png&#34; alt=&#34;FinalEg&#34; style=&#34;width:700px;&#34;/&gt;&lt;/p&gt;Various materials, Gamma = 1&lt;/div&gt; --&gt;
&lt;h2 id=&#34;and-then--whats-next-&#34;&gt;And then ? What&amp;rsquo;s Next ?&lt;/h2&gt;
&lt;p&gt;Rendering is a huge topic and we covered a very small part of it. What is next entirely depends on which direction you want to pursue this journey; you could go into path tracing and ignore real-time approximations or on the contrary go for more real-time rendering effects like Image Based Lighting (IBL), Screen Space Ambient Occlusion (SSAO), Screen Space Reflections (SSR) and so on.&lt;/p&gt;
&lt;p&gt;However, There is one aspect that should be improved in our current raytracing context which is the way we handle reflections. Indeed, currently in our ray-tracing context we cast only one reflected ray from intersection point. While it is accurate to do so for smooth surfaces, it is totally wrong for rough surfaces as we should cast several rays from the each intersection point and average the results in order to get the blurred reflections like I mentioned at the beginning of the tutorial. This is something you could try to implement in a naive manner and see how reflections are improved.
In addition, one can also questions the fact the presence of an ambient lighting term in this energy conservative context and he/she is right, this term is a complete hack to cope with the lack of indirect lighting computation. For real-time applications IBL can be used to better approximate this kind of lighting contribution while path tracing implicitly compute this contribution.&lt;/p&gt;
&lt;p&gt;Finally, I want you to be aware that BRDF models are generalizable and can take into account more light dimensionality, in particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BTDF: Bidirectionnal Transmission Distribution Function 5D: Same as BRDF for opposite side of surface&lt;/li&gt;
&lt;li&gt;SVBRDF: Spatially Varying BRDF 6D: changes over the surface position&lt;/li&gt;
&lt;li&gt;BSSRDF: Bidirectionnal Surface Scattering DF 8D: light exits at another location&lt;/li&gt;
&lt;li&gt;BSDF: Bidirectionnal Scattering Distribution Function XD: General formulation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Open GL Tutorials: 
&lt;a href=&#34;https://learnopengl.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://learnopengl.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Siggraph Courses: 
&lt;a href=&#34;http://blog.selfshadow.com/publications/s2013-shading-course/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://blog.selfshadow.com/publications/s2013-shading-course/&lt;/a&gt; 
&lt;a href=&#34;http://blog.selfshadow.com/publications/s2014-shading-course/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://blog.selfshadow.com/publications/s2014-shading-course/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Real Time Rendering Advances: 
&lt;a href=&#34;http://advances.realtimerendering.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://advances.realtimerendering.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;BRDF Model Comparison: 
&lt;a href=&#34;https://diglib.eg.org/handle/10.2312/EGWR.EGSR05.117-126&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://diglib.eg.org/handle/10.2312/EGWR.EGSR05.117-126&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Path tracing and Global Illumination: 
&lt;a href=&#34;http://www.graphics.stanford.edu/courses/cs348b-01/course29.hanrahan.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.graphics.stanford.edu/courses/cs348b-01/course29.hanrahan.pdf&lt;/a&gt; 
&lt;a href=&#34;http://web.cs.wpi.edu/~emmanuel/courses/cs563/write_ups/zackw/realistic_raytracing.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://web.cs.wpi.edu/~emmanuel/courses/cs563/write_ups/zackw/realistic_raytracing.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GLSL / Shadertoy: 
&lt;a href=&#34;https://www.opengl.org/documentation/glsl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.opengl.org/documentation/glsl/&lt;/a&gt; 
&lt;a href=&#34;https://www.shadertoy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.shadertoy.com/&lt;/a&gt; 
&lt;a href=&#34;http://www.iquilezles.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.iquilezles.org/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to Raymarching</title>
      <link>http://typhomnt.github.io/teaching/ray_tracing/raymarching_intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://typhomnt.github.io/teaching/ray_tracing/raymarching_intro/</guid>
      <description>&lt;!--
&lt;script type=&#34;text/x-mathjax-config&#34;&gt;
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],
    displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\[&#39;,&#39;\]&#39;]],
    processEscapes: true,
    processEnvironments: true,
    skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;],
    TeX: { equationNumbers: { autoNumber: &#34;AMS&#34; },
         extensions: [&#34;AMSmath.js&#34;, &#34;AMSsymbols.js&#34;] }
  }
});
&lt;/script&gt;

&lt;script type=&#34;text/x-mathjax-config&#34;&gt;
  MathJax.Hub.Queue(function() {
    // Fix &lt;code&gt; tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i &lt; all.length; i += 1) {
        all[i].SourceElement().parentNode.className += &#39; has-jax&#39;;
    }
});
&lt;/script&gt;


&lt;script type=&#34;text/javascript&#34; src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
--&gt;
&lt;p&gt;This tutorial is part of the ray-tracing course available 
&lt;a href=&#34;../raytracing_practs&#34;&gt;here&lt;/a&gt;. /!\ This tutorial is using parts of the 
&lt;a href=&#34;../pbr_intro&#34;&gt;PBR tutorial&lt;/a&gt; which should be completed before starting this one.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this tutorial, I will introduce another well known rendering approach which is very similar to ray tracing but operates in a slightly different way, especially as it treats surfaces as distance fields. Ray tracing is based on the principle that we can compute analytically ray-surface intersections, being with triangles or more complex surfaces. However, this can also be a limitation as we are bound to display only surfaces for which we are able to compute ray-surface intersection. Thus, Ray Marching comes into place and allows us to display any surface defined by an expression $f(x,y,z) = 0$ such that $\left\lVert \nabla f \right\rVert = 1$, in other words a distance field. More precisely, in our context the $f$ function can also represent negative distances. It is referred as a Signed Distance Function (SDF) which means that when $f(p) &amp;gt; 0$, $p = (x,y,z)$ lies outside the surface and when $ f(p) &amp;lt; 0 $ it lies inside the surface. Note the D for Distance in SDF which imposes the condition $\left\lVert \nabla f \right\rVert = 1$.&lt;/p&gt;
&lt;h2 id=&#34;main-principle&#34;&gt;Main Principle&lt;/h2&gt;
&lt;p&gt;The main principle of Ray Marching remains similar to Ray Tracing: for each pixel of the screen, we cast a ray spreading from the camera center to the pixel, however instead of computing ray surface intersections by solving an equation, we iterate through the generated ray step by step and check if we intersect a surface at each step by evaluating the scene SDF at the current location. More precisely, for a given ray $r(t) = \vec{d}t + c_{camera}$ and a given surface, we compute the SDF of the surface $f(r(t))$ and check if it equals 0. If not, we increase $t$ by a given amount $\delta_t$. Note that $t = 0 $ at the beginning of this process and that  $f(r(0))$ should be positive. In practice $f(r(t)) = 0$ never occurs in our programming world, instead we will check if $f(r(t)) \leq 0$ meaning that we went through the surface.&lt;/p&gt;





  











&lt;figure id=&#34;figure-representation-of-a-scene-sdf&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/RayMarch.png&#34; data-caption=&#34;Representation of a scene SDF&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/RayMarch.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Representation of a scene SDF
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Note that each surface like a Sphere or a Plane defines its own SDF that represents the closest distance from any given 3D point $p$ to the surface. Ray Marching also gives us the opportunity to combine each SDF with different operators in order to build multiple scenes with the same surfaces. The basic operation being the union of a SDF : $$SDF_{scene} (p) =  \bigcup SDF (p) = \min\limits_{f \in SDF}(f(p))$$ Several other operators are commonly used like the intersection, the substraction or the smooth union.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Union&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Intersection&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Substraction&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Smooth Union&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;




  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/March_Union.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/March_Union.png&#34; alt=&#34;&#34; width=&#34;300px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;




  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/March_Intersect.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/March_Intersect.png&#34; alt=&#34;&#34; width=&#34;250px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;




  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/March_Substract.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/March_Substract.png&#34; alt=&#34;&#34; width=&#34;245px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;




  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/March_Blend.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/March_Blend.png&#34; alt=&#34;&#34; width=&#34;280px&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;marching&#34;&gt;Marching&lt;/h2&gt;
&lt;p&gt;There are different ways of choosing the value of $\delta_t$ while going through a light ray: one is to increase $t$ by a constant small value until we intersect one surface. Another one is to compute the minimal distance we can travel without intersecting any surface by taking the minimum of all $f(r(t))$. In practice, this minimal distance is equal to $SDF_{scene}(r(t))$. This second approach is called Sphere (or Spherical) Marching and will be the approach we will use in this tutorial.&lt;/p&gt;
&lt;p&gt;




  











&lt;figure id=&#34;figure-step-by-step-marching&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/RayMarchSteps.png&#34; data-caption=&#34;Step by Step Marching&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/RayMarchSteps.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Step by Step Marching
  &lt;/figcaption&gt;


&lt;/figure&gt;






  











&lt;figure id=&#34;figure-spherical-marching&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/RayMarchSphere.png&#34; data-caption=&#34;Spherical Marching&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/RayMarchSphere.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Spherical Marching
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Once we computed the intersection point, we are ready to compute any lighting occuring at this point as well as casting new rays to simulate reflections or refractions.&lt;/p&gt;
&lt;h2 id=&#34;coding-time&#34;&gt;Coding Time&lt;/h2&gt;
&lt;p&gt;Now that I introduced the theory behind Ray Marching it is time to dig into actual coding. Like in previous ray tracing tutorials, we are working inside a fragment shader displaying a simple quad perfectly fitting our window. For each pixel we cast rays from a virtual camera to the current pixel whose screen coordinates are given by the in variable $fragCoord$.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;void main() 
{   
	Ray ray = generatePerspectiveRay(fragCoord);
	outColor = vec4(march(ray),1);

}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The march function computes  color contribution for the principal the reflected rays similarly to the trace function of previous tutorials. As we are in a PBR context, it also performs tone mapping and gamma correction. The function that actually go through each ray and check intersections with the surfaces in the scene is the rayMarch function. More precisely it returns the minimal $f(r(t))$ among all the scene surfaces. Notice that the normal at the intersection point is computed as the gradient of the surface SDF (see &lt;strong&gt;SDF Gradient&lt;/strong&gt; section).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vec3 march(in Ray r)
{
    vec3 accum = vec3(0.0f);
    vec3 mask = vec3(1.0f);
    int nb_refl = 0;
    float c_refl = 0.3;
    Ray curr_ray = r;
    for(int i = 0 ; i &amp;lt;= nb_refl ; i++)
    {
	ISObj io = rayMarch(curr_ray);
	if(io.t &amp;gt;= 0)
	{
	    PBRMat mat = PBRMat(vec3(.9,.1,.1),0.4,0.9,0.3);
	    vec3 N = normalize(computeSDFGrad(io,curr_ray.ro + io.d * curr_ray.rd));

	    HitSurface hs = HitSurface(curr_ray.ro + io.d*curr_ray.rd
	                               ,N
	                               ,mat
	                               ) ;
	    vec3 color = directIllumination(hs,curr_ray,c_refl);
	    accum = accum + mask * color;

	    mask = mask*c_refl;
	    curr_ray  = Ray(hs.hit_point + 0.05*hs.normal,reflect(curr_ray.rd,hs.normal));
	}
	else if(i == 0) //did not intersect anything
	{
	    accum = vec3(0.3,0.2,0.8);
	}

    }

    //HDR
    accum = accum / (accum+ vec3(1.0));
    //Gamma
    float gamma = 1.1;
    accum = pow(accum, vec3(1.0/gamma));

    return accum;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The directIllumination functions remains the same as presented in the PBR tutorial, the only change being for the function computing the shadow ray intersection.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vec3 directIllumination(in HitSurface hit,in Ray r,inout float refl)
{

    vec3 color = vec3(0);
    for(int i = 0 ; i &amp;lt; light_nbr ; i++)
    {
	Ray l_ray = lightRay(hit.hit_point,lights[i]);
	l_ray.ro = hit.hit_point + 0.01*hit.normal;
	ISObj io;

	io = rayMarch(l_ray);
	
	float d_light = lightDist(hit.hit_point,lights[i]);

	if(io.t &amp;lt; 0 || (io.t &amp;gt;= 0 &amp;amp;&amp;amp; (io.d &amp;gt;= d_light)))
	{
	    color += PBR(hit,r,lights[i]);
	}
	else
	{
	    color +=  vec3(0.03) * hit.material.color * hit.material.ao;
	}


	vec3 Ve = normalize(r.ro - hit.hit_point);
	vec3 H = normalize(Ve + l_ray.rd);
	refl = length(fresnelSchlick(max(dot(H, Ve), 0.0),  mix(vec3(0.04), hit.material.color, hit.material.metallic)))*hit.material.ao;
    }

    return color;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets detail the rayMarch function then. We first declare a maximum number of iteration step, defining how many times we can advance in the same ray. Indeed passed this number or a maximum travel distance we consider that nothing is to be intersected by the ray. Elsewise, at step $i$ we define the current point on the ray as $p = r.ro + depth * r.rd$, $depth$ playing the role of our parameter $t$ as explained above. We then compute the scene SDF, at point $p$, composed of one or several primitive surfaces like spheres, planes and boxes. This function returns the distance to the closest surface at point $p$. If this distance is lower than a given epsilon ($march accuracy$) or negative we consider that the current $p$ is the intersection point with the scene, else we continue to iterate over the ray and either increase $depth$ by the closest distance (Spherical Marching) or by a fixed amount.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ISObj rayMarch(in Ray r)
{
    int nb_step = 300;

    float depth = 0.0f;
    float march_accuracy = 0.001;
    float march_step_fact = 0.01;

    for (int i = 0; i &amp;lt; nb_step; i++)
    {
	ISObj io = sceneSDF(r.ro + depth * r.rd);
	if (io.d &amp;lt;= march_accuracy)
	{
	    return ISObj(depth,io.t,io.i);
	}

	// Move along the view ray
	//Spherical Marching
	depth += io.d;
	// Step by Step Marching
	//depth += march_step_fact

	if (depth &amp;gt;= DIST_MAX)
	{
	    // Gone too far; give up
	    return ISObj(DIST_MAX,-1,-1);
	}
    }
    return ISObj(DIST_MAX,-1,-1);
    //return ISObj(depth,last_io.t,last_io.i);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Computing the scene SDF is highly correlated with the way we represent the scene. In the last tutorials, we represented the scene as an union of Spheres, Planes and other surfaces whose intersection with a ray has an analytic solution. As I mentioned earlier, in our context we can combine surfaces using different operators which will change the shape of the represented surface. Thus, we need to add a new wrapper that contains both surfaces and operators that we apply between them. We define a Shape structure containing the type of primitive surface it represents and its index in the corresponding array (of Sphere, Plane,&amp;hellip;). The ShapeOp structure contains one Shape, the operation to apply and the index of the next ShapeOp with whom the operator will be applied. By default we consider that $next = -1$, meaning that no operation will be applied, marking the end of the chained list of operations between primitive surfaces. To be more flexible in the way we build scenes, we allow ourselves to consider as many ShapeOp chain lists as we want. Thus, we define define an array of indices $shop roots$ from which each chained lists of ShapeOp should start.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;struct Shape
{
    int shape_type;
    int shape_id;
};

struct ShapeOp
{
    Shape shape;
    int op_type;
    int next;
};

const int shop_nbr = 1;
ShapeOp shops[shop_nbr] = ShapeOp[](ShapeOp(Shape(1,0),0,-1));

const int shop_root_nbr = 1;
int shop_roots[shop_root_nbr] = int[](0);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a starter, let&amp;rsquo;s consider one simple scene composed of a unique Sphere. Below I put the necessary code to define the three primitives I use in this tutorial. Note that this code should be put before the code section presented just above.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;const int plane_type = 0;
const int sphere_type = 1;
const int box_type = 2;
const int shop_type = 3;

// plane structure
struct Plane 
{
    vec3 n; // normal
    float d; // offset
};

// box structure
struct Box 
{
    vec3 b; // up_right_corner_length
    vec3 c; // center
};


// sphere structure
struct Sphere 
{
    vec3 c; // center
    float r; // radius
};

const int sphere_nbr = 6;

Sphere spheres[sphere_nbr] = Sphere[](Sphere(vec3(0.5,-2.5*cos(time*0.1),0),0.5f)
	,Sphere(vec3(0.7,-2.5*cos(time*0.2),0),0.5f)
,Sphere(vec3(0.9,-2.5*cos(time*0.3),0),0.5f)
,Sphere(vec3(-0.9,2.5*cos(time*0.4),0),0.5f)
,Sphere(vec3(-0.7,2.5*cos(time*0.5),0),0.5f)
,Sphere(vec3(-0.5,2.5*cos(time*0.6),0),0.5f));

const int box_nbr = 3;
Box boxes[box_nbr] = Box[](Box(vec3(1,1,1),vec3(0,-3,0)),Box(vec3(1,1,1),vec3(0,3,0)),Box(vec3(0.5,0.5,0.5),vec3(0,0,0)));

const int plane_nbr = 6;
Plane planes[plane_nbr] = Plane[](Plane(vec3(0,1,0),5.0f),Plane(vec3(0,-1,0),5.0f),Plane(vec3(0,0,1),5.0f),Plane(vec3(0,0,-1),15.0f)
,Plane(vec3(1,0,0),5.0f),Plane(vec3(-1,0,0),5.0f));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets now compute the scene SDF at the current ray point $p$ within the chained list of surface operators, defining a single sphere. We iterate through each chain list, compute their related SDF by iteratively applying SDF operators and returns the closest computed distance among all chained lists (only one in our case).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ISObj sceneSDF(in vec3 point)
{
    ISObj nearest = ISObj(DIST_MAX,-1,-1);
    for(int j = 0 ; j &amp;lt; shop_root_nbr ;j++)
    {
	float dist = ShapeSDF(shop_roots[j],point);
	if(dist &amp;lt; nearest.d)
	    nearest = ISObj(dist,shop_type,shop_roots[j]);
    }
    return nearest;
}

float ShapeSDF(int sh_op_id, vec3 p)
{
    int curr_id = sh_op_id;
    float sdf_f = getBasicSDF(shops[curr_id].shape.shape_type,shops[curr_id].shape.shape_id,p);
    int next_id = shops[sh_op_id].next;

    while(next_id != -1)
    {
	float sdf_i = getBasicSDF(shops[next_id].shape.shape_type,shops[next_id].shape.shape_id,p);
	sdf_f = OpSDF(sdf_f,sdf_i,shops[curr_id].op_type);
	curr_id = next_id;
	next_id = shops[next_id].next;
    }

    return sdf_f;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Eventually, we need to define the SDF of each primitive surfaces in $getBasicSDF$ as well as the surface operators in $OpSDF$.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;float getBasicSDF(int type, int id,in vec3 p)
{
    if(type == plane_type)
    {
	    return SDFPlane(planes[id],p);
    }
    else if(type == sphere_type)
    {
	return SDFSphere(spheres[id],p);
    }
    else if(type == box_type)
    {
	return SDFBox(boxes[id],p);
    }

    return 0.0;
}

// signed distance function of sphere 
float SDFSphere(in Sphere s, in vec3 p) 
{
    return length(s.c - p) - s.r;
}

// signed distance function of plane 
float SDFPlane(in Plane pl, in vec3 p) 
{
    return (pl.d + dot(normalize(pl.n),p));
}

// signed distance function of Box 
float SDFBox(in Box b, in vec3 p) 
{
    vec3 d = abs(p - b.c) - (b.b) ;
    return length(max(d,0)) + min(max(max(d.x,d.y),d.z),0);
}


float OpSDF(in float s_sdf, in float e_sdf, int op_t)
{
    if(op_t == 0)
    {
	return opUnion(s_sdf,e_sdf);
    }
    else if(op_t == 1)
    {
	return opSubtraction(s_sdf,e_sdf);
    }
    else if(op_t == 2)
    {
	return opIntersection(s_sdf,e_sdf);
    }
    else if(op_t == 3)
    {
	return opSmoothUnion(s_sdf,e_sdf,1.0f);
    }

    return 0.0;

}

float opUnion( float d1, float d2 ) {  return min(d1,d2); }

float opSubtraction( float d1, float d2 ) { return max(d1,-d2); }

float opIntersection( float d1, float d2 ) { return max(d1,d2); }

float opSmoothUnion( float d1, float d2, float k ) 
{
    float h = clamp( 0.5 + 0.5*(d2-d1)/k, 0.0, 1.0 );
    return mix( d2, d1, h ) - k*h*(1.0-h);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sdf-gradient&#34;&gt;SDF gradient&lt;/h2&gt;
&lt;p&gt;We are one step away of displaying our first ray marched scene. Indeed, the only unknown left to compute is the normal at the intersection point which is especially used to compute the local illumination. In this context, as we apply SDF operators, we will build new scenes by combining primitive surfaces in different ways. Consequently, we cannot compute an analytical normal at the intersection point. Instead we use finite differences to compute an approximation of this normal which correspond to the gradient of the scene SDF at this point. In the 1D case, approximating such a gradient (derivative) is done as following $f&#39;(x) = \frac{f(x+\epsilon) - f(x - \epsilon)}{2\epsilon}$. As SDFs are 3D functions, this principle is extended to each dimension of the function: $$\nabla f = \frac{(f(p + (\epsilon,0,0)) - f(p - (\epsilon,0,0)), f(p + (0,\epsilon,0)) - f(p - (0,\epsilon,0)), f(p + (0,0,\epsilon)) - f(p - (0,0,\epsilon))) } {2(\epsilon,\epsilon,\epsilon)}$$&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;float getSDF(int type, int id,in vec3 p)
{
    if(type != shop_type)
    {
	return getBasicSDF(type,id,p);
    }
    else
    {
	return ShapeSDF(id,p);
    }
}


#define EPS_GRAD 0.001
vec3 computeSDFGrad(in ISObj is,in vec3 p)
{
    vec3 p_x_p = p + vec3(EPS_GRAD, 0, 0);
    vec3 p_x_m = p - vec3(EPS_GRAD, 0, 0);
    vec3 p_y_p = p + vec3(0, EPS_GRAD, 0);
    vec3 p_y_m = p - vec3(0, EPS_GRAD, 0);
    vec3 p_z_p = p + vec3(0, 0, EPS_GRAD);
    vec3 p_z_m = p - vec3(0, 0, EPS_GRAD);

    float sdf_x_p = getSDF(is.t,is.i,p_x_p);
    float sdf_x_m = getSDF(is.t,is.i,p_x_m);
    float sdf_y_p = getSDF(is.t,is.i,p_y_p);
    float sdf_y_m = getSDF(is.t,is.i,p_y_m);
    float sdf_z_p = getSDF(is.t,is.i,p_z_p);
    float sdf_z_m = getSDF(is.t,is.i,p_z_m);


    return vec3(sdf_x_p - sdf_x_m
	        ,sdf_y_p - sdf_y_m
	        ,sdf_z_p - sdf_z_m)/(2.*EPS_GRAD);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the following lights in your scene you should now obtain a correctly shaded ray marched sphere.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;const int light_nbr = 2;
Light lights[light_nbr] = Light[](Light(1,vec3(1,1,-1),vec3(-3,1,-3),80,vec3(1))
	,Light(1,normalize(vec3(-1,1,1)),vec3(3,1,3),80,vec3(1)));
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-ray-marched-sphere&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Sphere.png&#34; data-caption=&#34;Ray Marched Sphere&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Sphere.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Ray Marched Sphere
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;example--march-lava-lamp&#34;&gt;Example : March Lava Lamp&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s now build a more complex scene using only one ShapeOp chained list. The main idea here is to display a kind of lava lamp using spheres and two boxes.
This can be easily achieved by using the smooth union operator which interpolates between two SDF that are near each other.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;const int shop_nbr = 10;
ShapeOp shops[shop_nbr] = ShapeOp[](ShapeOp(Shape(1,0),3,1)
	,ShapeOp(Shape(1,1),3,2)
,ShapeOp(Shape(1,2),3,3)
,ShapeOp(Shape(1,3),3,4)
,ShapeOp(Shape(1,4),3,5)
,ShapeOp(Shape(1,5),3,6)
,ShapeOp(Shape(2,0),3,7)
,ShapeOp(Shape(2,1),1,-1)
,ShapeOp(Shape(2,2),1,-1)
,ShapeOp(Shape(0,0),0,-1));
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-lava-lamp&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Lava_Lamp.png&#34; data-caption=&#34;Lava Lamp&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Lava_Lamp.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Lava Lamp
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Lets also put a cube shaped hole inside this lamp using the substract operator.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;const int shop_nbr = 10;
	ShapeOp shops[shop_nbr] = ShapeOp[](ShapeOp(Shape(1,0),3,1)
		,ShapeOp(Shape(1,1),3,2)
	,ShapeOp(Shape(1,2),3,3)
	,ShapeOp(Shape(1,3),3,4)
	,ShapeOp(Shape(1,4),3,5)
	,ShapeOp(Shape(1,5),3,6)
	,ShapeOp(Shape(2,0),3,7)
	,ShapeOp(Shape(2,1),1,8)
	,ShapeOp(Shape(2,2),1,-1)
	,ShapeOp(Shape(0,0),0,-1));
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-lava-lamp-with-a-cube-shaped-hole&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Lava_Lamp_Cube.png&#34; data-caption=&#34;Lava Lamp with a cube shaped hole&#34;&gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Lava_Lamp_Cube.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Lava Lamp with a cube shaped hole
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Now that you can render more complex scenes and I invite you to test all kind of scene configurations by playing with different primitives and different operators.&lt;/p&gt;
&lt;h2 id=&#34;bonus-effect-ambient-occulsion&#34;&gt;Bonus Effect: Ambient Occulsion&lt;/h2&gt;
&lt;p&gt;Rendering 3D scenes using ray marching also offer other advantages. Additional rendering effect like Ambient Occlusion can be computed with a reduced cost.
Ambient Occlusion is the equivalent of ambient lighting but for shadowing, it describes which parts of the scene that are likely to not be lighted because of the geometry of its surroundings, preventing light ray to reach those areas. This effect can be simply be noticed at the edges and corners of your room where lights hardly strike those areas.
Ray Marching offers us a simple way to approximate this ambient occlusion term. The main idea is to cast an ambient occlusion ray from the intersection point $p$ in the normal direction $\vec{n}$ and march through this ray step by step with a relatively small $\delta_t$. Then at each step, we compute the scene SDF and compare its value to the distance with respect to the intersection point which is equal to $i \delta_t$, $i$ being the number of the current step. In the case where the intersection point lies on a convex surface like a sphere we expect this distance and the scene SDF to be the same. However, it is far from being true in the general case, meaning that the scene SDF can be lower than $i \delta_t$ at some steps, being nearer to other surfaces.
This simulate the fact that some areas around the intersection point can occlude incoming rays. In practice, we first initialize the occlusion factor at $1$ and compute at each step $\frac{(i \delta_t - SDF_{Scene})^2}{i}$ and subtract this amount to the occlusion factor. Notice the denominator which takes into account that the farther from the intersection point we make this evaluation the less plausible it is as we might look far from the surroundings.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;float AmbientOcclusion(vec3 point, vec3 normal, float step_dist, float step_nbr)
{
    float occlusion = 1.0f;
    while(step_nbr &amp;gt; 0.0)
    {
	occlusion -= pow(step_nbr * step_dist - (sceneSDF( point + normal * step_nbr * step_dist)).d,2) / step_nbr;
	step_nbr--;
    }

    return occlusion;
}


vec3 march(in Ray r)
{
	...
	for(int i = 0 ; i &amp;lt;= nb_refl ; i++)
	{
		ISObj io = rayMarch(curr_ray);
		if(io.t &amp;gt;= 0)
		{
			...
			accum = accum + mask * color * pow(AmbientOcclusion(hs.hit_point,hs.normal,0.015,20),40);

			...
		}
		else if(i == 0) //did not intersect anything
		{
			accum = vec3(0.3,0.2,0.8);
		}

	}
	...

	return accum;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is important to note that when computing the ambient occlusion term, the total travel distance should remain as short as possible because. Indeed, like mentioned earlier, the farther you travel through this ray, the more artifact you are likely to get because surfaces that are too far away from the intersection point might contribute to the occlusion term while not being part of its surroundings. With the parameters I used, you should have an ambient occlusion terms like depicted below. Note that I amplified its effect using a power function with a relatively high factor.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Ambient_Occlusion.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Ambient_Occlusion.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Below is what you should obtain by multiplying the ambient occlusion to the output color so that it plays a shadowing role.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Lava_Lamp_AO.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Lava_Lamp_AO.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Finally, we can also re-enable relections in the main loop and get the following result.&lt;/p&gt;





  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Final_Lava_Lamp.png&#34; &gt;


  &lt;img src=&#34;http://typhomnt.github.io/img/Images/RayMarch_Intro/Final_Lava_Lamp.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;what-is-next&#34;&gt;What is next&lt;/h2&gt;
&lt;p&gt;This tutorial is a brief introduction to Ray Marching, there are still many things to cover. At least, three important aspects are to be studied, the first one being terrain marching, the second one fractal marching and the last one being the use of impostors in real-time rendering pipelines that are rendered using raymarching. In a future update, I will probably add a section about terrain marching, stay tuned.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Open GL Tutorials: 
&lt;a href=&#34;https://learnopengl.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://learnopengl.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ray Marching Game: 
&lt;a href=&#34;https://www.youtube.com/watch?v=9U0XVdvQwAI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=9U0XVdvQwAI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cloud Raymarching in Unity: 
&lt;a href=&#34;https://www.youtube.com/watch?v=4QOcCGI6xOU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=4QOcCGI6xOU&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Converting 3D scenes to SDF with 3D textures: 
&lt;a href=&#34;https://kosmonautblog.wordpress.com/2017/05/01/signed-distance-field-rendering-journey-pt-1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kosmonautblog.wordpress.com/2017/05/01/signed-distance-field-rendering-journey-pt-1/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Inigo Quilez tutorials: 
&lt;a href=&#34;https://www.iquilezles.org/www/articles/terrainmarching/terrainmarching.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.iquilezles.org/www/articles/terrainmarching/terrainmarching.htm&lt;/a&gt; 
&lt;a href=&#34;https://www.iquilezles.org/www/articles/raymarchingdf/raymarchingdf.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.iquilezles.org/www/articles/raymarchingdf/raymarchingdf.htm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GLSL / Shadertoy: 
&lt;a href=&#34;https://www.opengl.org/documentation/glsl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.opengl.org/documentation/glsl/&lt;/a&gt; 
&lt;a href=&#34;https://www.shadertoy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.shadertoy.com/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
