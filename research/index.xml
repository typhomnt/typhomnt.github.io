<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Publications | Maxime Garcia</title>
    <link>http://typhomnt.github.io/research/</link>
      <atom:link href="http://typhomnt.github.io/research/index.xml" rel="self" type="application/rss+xml" />
    <description>Publications</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 20 Feb 2019 16:22:09 +0100</lastBuildDate>
    <image>
      <url>http://typhomnt.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_box_center_2.png</url>
      <title>Publications</title>
      <link>http://typhomnt.github.io/research/</link>
    </image>
    
    <item>
      <title>Coherent Mark-based Stylization of 3D Scenes at the Compositing Stage</title>
      <link>http://typhomnt.github.io/research/coherent_splat_stylization/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/research/coherent_splat_stylization/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/NPR/splat_teaser.jpg&#34; alt=&#34;Teaser&#34;&gt;&lt;/p&gt;
&lt;p&gt;Comptuer Graphics Forum, Eurographics 2021. Article available 
&lt;a href=&#34;https://hal.inria.fr/hal-03143244&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;authors-&#34;&gt;Authors&lt;/h2&gt;
&lt;p&gt;Maxime Garcia
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Romain Vergne
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Mohamed-Amine Farhat
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Pierre Benard
(Inria / LaBRI)&lt;/p&gt;
&lt;p&gt;Camille Nous
(Laboratoire Cogitamus)&lt;/p&gt;
&lt;p&gt;Joelle Thollot
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;h3 id=&#34;abstract-&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present a novel temporally coherent stylized rendering technique working entirely at the compositing stage. We first generate a distribution of 3D anchor points using an implicit grid based on the local object positions stored in a G-buffer, hence following object motion. We then draw splats in screen space anchored to these points so as to be motion coherent. To increase the perceived flatness of the style, we adjust the anchor points density using a fractalization mechanism. Sudden changes are prevented by controlling the anchor points opacity and introducing a new order-independent blending function. We demonstrate the versatility of our method by showing a large variety of styles thanks to the freedom offered by the splats content and their attributes that can be controlled by any G-buffer.&lt;/p&gt;
&lt;h4 id=&#34;main-video-&#34;&gt;Main Video&lt;/h4&gt;
&lt;center&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/466IoPKs0p0&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;
&lt;h4 id=&#34;supplementals-&#34;&gt;Supplementals&lt;/h4&gt;
&lt;center&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/sxU_ZEjwwjI&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;
&lt;h4 id=&#34;app-and-turorial-&#34;&gt;App and Turorial&lt;/h4&gt;
&lt;p&gt;Available soon !&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recognition of Laban Effort Qualities from Hand Motion</title>
      <link>http://typhomnt.github.io/research/recognition_laban_hand/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/research/recognition_laban_hand/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Laban_Classification/effort_cube.jpg&#34; alt=&#34;Teaser&#34;&gt;&lt;/p&gt;
&lt;p&gt;MOCO&amp;rsquo;20 - 7th International Conference on Movement and Computing, Jul 2020, Jersey City/ Virtual, United States. Article available 
&lt;a href=&#34;https://hal.inria.fr/hal-02899999&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;authors-&#34;&gt;Authors&lt;/h2&gt;
&lt;p&gt;Maxime Garcia
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Remi Ronfard
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;h3 id=&#34;abstract-&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this paper, we conduct a study for recognizing motion qualities in hand gestures using virtual reality trackers attached to the hand. From this 6D signal, we extract Euclidean, equi-affine and moving frame features and compare their effectiveness in the task of recognizing Laban Effort qualities. Our experimental results reveal that equi-affine features are highly discriminant features for this task. We also compare two classification methods on this task. In the first method, we trained separate HMM models for the 6 Laban Effort qualities (light, strong, sudden, sustained, direct, indirect). In the second method, we trained separate HMM models for the 8 Laban motion verbs (dab, glide, float, flick, thrust, press, wring, slash) and combined them to recognize individual qualities. In our experiments, the second method gives improved results. Together, those findings suggest that low-dimensional signals from VR trackers can be used to predict motion qualities with reasonable precision.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spatial Motion Doodles: Sketching Animation in VR Using Hand Gestures and Laban Motion Analysis</title>
      <link>http://typhomnt.github.io/research/spatial_motion_doodle/</link>
      <pubDate>Wed, 20 Feb 2019 16:22:09 +0100</pubDate>
      <guid>http://typhomnt.github.io/research/spatial_motion_doodle/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://typhomnt.github.io/Images/Animation_Transfer/SMD_Teaser.png&#34; alt=&#34;Teaser&#34;&gt;&lt;/p&gt;
&lt;p&gt;ACM Motion, Interaction and Games 2019 (MIG &amp;lsquo;19)&lt;/p&gt;
&lt;h2 id=&#34;authors-&#34;&gt;Authors&lt;/h2&gt;
&lt;p&gt;Maxime Garcia
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Remi Ronfard
(Inria / LJK / Universite Grenoble Alpes)&lt;/p&gt;
&lt;p&gt;Marie-Paule Cani
(Ecole Polythecnique / LIX)&lt;/p&gt;
&lt;h3 id=&#34;abstract-&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present a method for easily drafting expressive character animation by playing with instrumented rigid objects. We parse the input 6D trajectories (position and orientation over time) – called spatial motion doodles – into sequences of actions and convert them into detailed character animations using a dataset of parameterized motion clips which are automatically fitted to the doodles in terms of global trajectory and timing. Moreover, we capture the expressiveness of user-manipulation by analyzing Laban effort qualities in the input spatial motion doodles and transferring them to the synthetic motions we generate. We validate the ease of use of our system and the expressiveness of the resulting animations through a series of user studies, showing the interest of our approach for interactive digital storytelling applications dedicated to children and non-expert users, as well as for providing fast drafting tools for animators.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;&lt;iframe width=&#34;838&#34; height=&#34;472&#34; src=&#34;https://www.youtube.com/embed/0xG2dlGg_9M&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
